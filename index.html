<!DOCTYPE html>
  <!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
  <!--[if IE 7]>    <html class="no-js lt-ie9 lt-ie8" lang="en"> <![endif]-->
  <!--[if IE 8]>    <html class="no-js lt-ie9" lang="en"> <![endif]-->
  <!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <link rel="stylesheet" type="text/css" href="/blogassets/cssbin/blog-shared-min.css?1119069029">
      <!--[if lt IE 8]>
      <link rel="stylesheet" type="text/css" href="/blogassets/cssbin/blog-shared-ie-min.css?1119069029">
      <![endif]-->
      <link rel="stylesheet" type="text/css" href="/blogassets/cssbin/blog-default-min.css?1119069029">
    <link rel="stylesheet" type="text/css" href="/blogassets/cssbin/blog-awsblog-min.css?1119069029">
    <link rel="stylesheet" type="text/css" href="/blogassets/jsbin/syntaxhighlighter/styles/shCore.css?1119069029">
    <link rel="stylesheet" type="text/css" href="/blogassets/jsbin/syntaxhighlighter/styles/shThemeDefault.css?1119069029">
  <script type="text/javascript">
        var blogs = blogs || {};
        blogs.rootURL = blogs.rootURL || "/bigdata/";
      </script>
      <script type="text/javascript" src="http://g-ecx.images-amazon.com/images/G/01/javascripts/lib/jquery/jquery-1.6.2.min.js"></script>
      <script type="text/javascript" src="/blogassets/jsbin/blog-shared-ux-min.js?1119069029"></script>
      <script type="text/javascript" src="/blogassets/jsbin/blog-default-ux-min.js?1119069029"></script>
    <script type="text/javascript" src="/blogassets/jsbin/blog-awsblog-ux-min.js?1119069029"></script>
    <script type="text/javascript" src="/blogassets/jsbin/syntaxhighlighter/scripts/shCore.js?1119069029"></script>
    <script type="text/javascript" src="/blogassets/jsbin/syntaxhighlighter/scripts/shBrushBash.js?1119069029"></script>
    <script type="text/javascript" src="/blogassets/jsbin/syntaxhighlighter/scripts/shBrushCSharp.js?1119069029"></script>
    <script type="text/javascript" src="/blogassets/jsbin/syntaxhighlighter/scripts/shBrushJava.js?1119069029"></script>
    <script type="text/javascript" src="/blogassets/jsbin/syntaxhighlighter/scripts/shBrushRuby.js?1119069029"></script>
    <script type="text/javascript" src="/blogassets/jsbin/syntaxhighlighter/scripts/shBrushJScript.js?1119069029"></script>
    <script type="text/javascript" src="/blogassets/jsbin/syntaxhighlighter/scripts/shBrushCpp.js?1119069029"></script>
    <script type="text/javascript" src="/blogassets/jsbin/syntaxhighlighter/scripts/shBrushObjC.js?1119069029"></script>
    <script type="text/javascript" src="/blogassets/jsbin/syntaxhighlighter/scripts/shBrushPhp.js?1119069029"></script>
    <script type="text/javascript" src="/blogassets/jsbin/syntaxhighlighter/scripts/shBrushPlain.js?1119069029"></script>
    <script type="text/javascript" src="/blogassets/jsbin/syntaxhighlighter/scripts/shBrushPython.js?1119069029"></script>
    <script type="text/javascript" src="/blogassets/jsbin/syntaxhighlighter/scripts/shBrushVb.js?1119069029"></script>
    <script type="text/javascript" src="/blogassets/jsbin/syntaxhighlighter/scripts/shBrushXml.js?1119069029"></script>
    <script type="text/javascript" src="/blogassets/jsbin/syntaxhighlighter/scripts/shBrushSql.js?1119069029"></script>
    <script type="text/javascript">
       blogs.url.archive = function(options) { 
    	  var template = "/bigdata/" + "blog/archive/{year}/{month}/{day}";
          return blogs.url.template(template, options);
       };
    </script>
    
  <title>
      AWS Big Data Blog</title>
  
    <link href="//www.amazon.com/favicon.ico" type="image/ico" rel="icon">
    <link href="//www.amazon.com/favicon.ico" type="image/ico" rel="shortcut icon">
    <link rel="alternate" type="application/rss+xml" title="Recent Posts (RSS)" href="/bigdata/blog/feed/recentPosts.rss">
  </head>
  <body class="bigdata_awsblog-theme ">
    <div id="page">
        <div class="container">
          <header class="clearfix">
  <div id="defaultNavigation" class="navigation">
    <div>
      <hgroup>
        <h1>
          <a href="/bigdata/blog" id="nav-aws-logo" class="aws-sprite" title="Amazon Web Services"></a>
        </h1>
        <h2></h2>
        <p class="aws-common-header-message">AWS Big Data Blog</p>
      </hgroup>
    </div>
    
    <div class="aws-banner-common-background">
      <div>
        <span class="security-banner-taglines-style"> 
          Helping you collect, store, clean, process, and visualize big data.
        </span>
      </div>
    </div>
  </div>
 </header><section id="content" class="clearfix">
            <div id="main" class="span-16">
              <ul class="blog-posts navigation nav-clearfix">
          <li class="blog-post">
              <article
  class="post display published has-comments " 
  data-postid="Tx22THFQ9MI86F9">
  
  <div class="post-heading-block clearfix">

  <div class="post-author">
      
<div class="author avatar author-avatar blog-ui-avatar blog-ui-author-avatar" data-content-id="wunnavag">
  <a href="/bigdata/blog/author/Gopal+Wunnava" title="Gopal Wunnava"><img class="author-image" alt="Gopal Wunnava" src="//cdn.amazonblogs.com/bigdata_awsblog/authors/Author_pic_Gopal_resized2.fw_1435184902107.png"></a>
</div></div>
  <div class="post-heading">

    <h1 class="post-title navigation">
      <a href="/bigdata/post/Tx22THFQ9MI86F9/Applying-Machine-Learning-to-Text-Mining-with-Amazon-S3-and-RapidMiner" rel="bookmark">
        Applying Machine Learning to Text Mining with Amazon S3 and RapidMiner</a>
    </h1>

    <ul class="post-attributes navigation inline-navigation blog-ui-piped">
      <li>
        <a href="/bigdata/post/Tx22THFQ9MI86F9/Applying-Machine-Learning-to-Text-Mining-with-Amazon-S3-and-RapidMiner#" rel="bookmark">
          <time datetime="2015-06-25T14:48:21.861Z" title="2015-06-25T14:48:21.861Z" class="post-creation-date">June 25, 2015</time></a>
      </li>
      <li class="last">      
        <address class="by-line">
          <a rel="nofollow" data-contentid="wunnavag" class="author-name" itemprop="author" href="/bigdata/blog/author/Gopal+Wunnava">Gopal Wunnava</a>
</address>
      </li>    
    </ul>
    
    <ul class="category-list post-categories navigation inline-navigation blog-ui-piped">
  </ul><ul class="tag-list post-tags navigation inline-navigation blog-ui-piped">
  </ul></div>      
</div><div class="post-content-block">
    <div class="post-text"><p>
	<em>Gopal Wunnava is a Senior Consultant with AWS Professional Services</em></p>
<p>
	By some estimates, 80% of an organization's data is unstructured content. This content includes web pages, call center transcripts, surveys, feedback forms, legal documents, forums, social media, and blog articles. Therefore, organizations must analyze not just transactional information but also textual content to gain insight and boost performance. A powerful way to analyze this textual content is by using text mining.</p>
<p>
	Text mining typically applies machine learning techniques such as clustering, classification, association rules and predictive modeling. These techniques uncover meaning and relationships in the underlying content. Text mining is used in areas such as competitive intelligence, life sciences, voice of the customer, media and publishing, legal and tax, law enforcement, sentiment analysis and trend-spotting.</p>
<p>
	In this blog post, you’ll learn how to apply machine learning techniques to text mining. I’ll show you how to build a text mining application using <a href="https://rapidminer.com/" target="_blank">RapidMiner</a>, a popular open source tool for predictive analytics, and <a href="http://aws.amazon.com/s3/" target="_blank">Amazon Simple Storage Service</a> (Amazon S3), an easy-to-use storage service that lets organizations store and retrieve any amount of data from anywhere on the web.</p>
<h1>
	<strong>Why use text mining?</strong></h1>
<p>
	Text mining techniques help reveal patterns and relationships in large volumes of textual content that are not visible to the naked eye, leading to new business opportunities and improvements in processes. Using text mining techniques can save you time and resources: the process can be automated and the results from a text mining model can be consistently derived and applied to solve specific problems.</p>
<p>
	These techniques help you:</p>
<ul>
	<li>
		Extract key concepts, patterns and relationships from large volumes of textual content</li>
	<li>
		Spot trends in textual content on subjects such as travel and entertainment to understand consumer sentiment</li>
	<li>
		Summarize content from documents and gain semantic understanding of the underlying content</li>
	<li>
		Index and search text for use in predictive analytics</li>
</ul>
<p>
	As you can see, if you don’t analyze textual content in addition to transactional content, you might miss huge opportunities.&nbsp;</p>
<h1>
	<strong>Past barriers to text mining</strong></h1>
<p>
	In the past, it was often hard to extract valuable insight from large volumes of text. Doing so required complex programming and modeling tasks performed by highly skilled IT resources. In addition, the infrastructure simply couldn’t scale to handle the demands of processing large volumes of unstructured text with the speed and agility required to sustain performance and innovation cycles. Integrating tools with the underlying infrastructure was another challenge. This often resulted in data and tools being migrated from one environment to another. Moreover, business users found it hard to interpret the results. Structured data that was easy to mine and analyze became the primary source of most data analysis tasks. The result: vast pools of textual content went virtually untapped.&nbsp;</p>
<h1>
	<strong>Recent advances in text analytics</strong></h1>
<p>
	Data and cloud infrastructure has made huge advancements. This includes the tools and technology available in the machine learning and text mining space. With these advancements, speed, innovation and scalability are now realistic. There has also been a fundamental shift in how organizations use analytics: rather than react to past trends, they emphasize being proactive by predicting future trends based on current events. Thanks to cloud infrastructure services provided by AWS and tools such as <a href="https://rapidminer.com/" target="_blank">RapidMiner</a> that combine machine learning, text mining and visualization capabilities, organizations can analyze textual content quickly in a scalable and durable environment without the need for advanced programming skills.</p>
<h1>
	<strong>Text mining workflow</strong></h1>
<p>
	Most text mining follows a typical workflow:</p>
<ol>
	<li>
		Identify and retrieve documents for analysis. Apply structural, statistical and linguistic techniques (often in combination) to discern, tag and extracts elements such as entities, concepts and relationships.</li>
</ol>
<ol start="2">
	<li>
		Apply statistical pattern-matching and similarity techniques to classify documents and organize extracted features according to a specified grouping or category. The underlying unstructured content is transformed into structured data formats that are easier to analyze.&nbsp; The classification process helps discern meaning and relationships.</li>
</ol>
<ol start="3">
	<li>
		Evaluate the model for performance.</li>
</ol>
<ol start="4">
	<li>
		Present the findings to end users.</li>
</ol>
<p>
	The flow chart below illustrates this workflow.</p>
<p>
	<img alt="Typical text mining workflow" height="423" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Text_Mining_Image_1.png" width="561" /></p>
<h1>
	<strong>The role of machine learning in text mining</strong></h1>
<p>
	Text mining techniques typically establish a set of significant words and sentences based on a statistical analysis of factors such as term frequency and distribution. Words and sentences scoring highest in terms of significance typically indicate the underlying opinion, sentiment or the general subject matter.</p>
<p>
	As part of the process, modern tools typically construct a document term matrix (DTM) and use a weighting such as TF-IDF (term frequency - inverse document frequency). These tools extract and store underlying information such as standard features, keyword frequency, documents and text list features in the form of tables in a database. These tables can be queried for efficient analysis and processing. These steps are a precursor to applying machine learning techniques to the textual content.</p>
<p>
	Text analytics typically applies such machine learning techniques as clustering, classification, association rules and predictive modeling to discern meaning and relationships in the underlying content. Methods are then used to process the underlying text contained in unstructured data sources, including Natural Language Processing (NLP), parsing, tokenization (identification of distinct elements such as words or n-grams), stemming (reducing word variants to bases), term reduction (group like terms using synonyms and similarity measures) and parts of speech tagging (POS tags) which help discern facts and relationships.</p>
<p>
	Another key aspect in text analytics involves organizing and structuring the underlying textual content. Typical techniques include clustering, categorization, classification and taxonomy. Some typical classification methods used by many tools include Naive Bayes, Support Vector Machine and K-nearest neighbor.</p>
<p>
	The table below contains common text mining techniques, including machine learning, and the key considerations for each.</p>
<p>
	<img alt="Text mining techniques" height="543" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Text_Mining_Image_table1.PNG" width="625" /></p>
<p>
	Once the text is processed, grouped and analyzed using the techniques above, it is important to evaluate the results. The goal of this evaluation is to determine whether you have found the most relevant material or if you have missed any important terms. You will use measures such as precision and recall to evaluate your results.</p>
<h1>
	<strong>Sentiment analysis using AWS and RapidMiner</strong></h1>
<p>
	Now let's look at how you can use AWS and RapidMiner for sentiment analysis, a popular use case for text mining. In sentiment analysis, you identify positive and negative opinions, emotions, and evaluations, and often analyze textual content using machine learning techniques. Using AWS and RapidMiner, you can apply techniques like sentiment analysis on unstructured data stored in S3 directly, without pushing the data into another environment.</p>
<p>
	As shown below, you can use RapidMiner to create your text mining workflows integrated with S3. An object on S3 can be any kind of file or format such as a text file, a photo, or a video. This makes S3 useful for storing unstructured data required for text mining and advanced analytics.</p>
<p>
	<img alt="Using RapidMiner to create a text mining workflow integrated with S3" height="342" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Text_Mining_Image_2.png" width="611" /></p>
<p>
	Amazon S3 is integrated with other Amazon big data services such as Amazon <a href="http://aws.amazon.com/redshift/" target="_blank">Redshift</a>, Amazon <a href="http://aws.amazon.com/rds/" target="_blank">RDS</a>, Amazon <a href="http://aws.amazon.com/dynamodb/" target="_blank">DynamoDB</a>, Amazon <a href="http://aws.amazon.com/kinesis/" target="_blank">Kinesis</a> and Amazon <a href="http://aws.amazon.com/elasticmapreduce/" target="_blank">EMR</a>. This leads to interesting scenarios for developing text mining models on <a href="http://aws.amazon.com/" target="_blank">AWS</a> using RapidMiner. For example, you can use S3 to store the data ingested from these Amazon services and then use RapidMiner to quickly build a text mining model on this data. You could store output results from your model into an S3 bucket and region of your choice and share these results with a broader end user community.</p>
<p>
	The examples below use the <a href="https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection" target="_blank">SMS Spam collection</a> dataset hosted by the University of California, Irvine. The SMS spam collection is a public set of labeled messages that have been collected for mobile phone spam research. This dataset combines “spam” and non-spam messages marked as “ham”. The dataset is a tab-separated text file with one message per line, with UTF-8 encoding.</p>
<h1>
	<strong>Video demonstrations</strong></h1>
<p>
	The following demos show you how to use RapidMiner and S3 for text mining. <strong>Note: </strong>the demos do not have sound.</p>
<p>
	To get started:</p>
<ol>
	<li>
		<a href="https://rapidminer.com/signup/" target="_blank">Download</a> and install the RapidMiner software, along with the RapidMiner Text Processing Extension<strong> </strong>available from the RapidMiner Marketplace. You can install RapidMiner either on your local machine or on an <a href="http://aws.amazon.com/ec2/instance-types/" target="_blank">Amazon EC2 instance</a> of your choice when you need more capacity than your current configuration provides.</li>
</ol>
<ol start="2">
	<li>
		Configure your S3 connection on RapidMiner using your AWS credentials.To use S3, you need an <a href="http://aws.amazon.com/" target="_blank">AWS account</a>.</li>
</ol>
<ol start="3">
	<li>
		&nbsp;<a href="http://docs.aws.amazon.com/AmazonS3/latest/UG/UploadingObjectsintoAmazonS3.html" target="_blank">Upload</a> the input dataset required for this text mining case study into your S3 bucket.</li>
</ol>
<p>
	<strong>Importing and Reading Data from S3 into RapidMiner</strong></p>
<p>
	The following video shows you how to create a text mining application with S3 and RapidMiner using data you uploaded into an S3 bucket. Remember that you must import the file with UTF-8 encoding and specify tab as the delimiter in order to process the file in the right format.</p>
<p>
	<a href="https://s3.amazonaws.com/awsbigdatablog/1-AmazonS3-RapidMiner-Text-Mining-Video.mp4">Video: Importing and Reading Data from S3 into RapidMiner</a></p>
<p>
	<strong>Working with RapidMiner’s Validation Operator</strong></p>
<p>
	When running a model on unseen data, you may see lower accuracy levels than expected. This is probable, because our approach may have learned what it has seen and was never tested on unseen data. To address this, you can work with the<strong> </strong>RapidMiner Validation operator as demonstrated in the video below.</p>
<p>
	<a href="http://s3.amazonaws.com/awsbigdatablog/2-AmazonS3-RapidMiner-Text-Mining-Video.mp4" target="_blank">Video: Working with RapidMiner's Validation Operator</a></p>
<p>
	<strong>Applying Store Operators in RapidMiner</strong></p>
<p>
	To apply the learned model to new data, you must save the model as well as the word list into the RapidMiner repository. You must save the word list is because when you predict the probability of a new message being either “spam” or “ham”, you have to use the same attributes or words used in the original process. Therefore, you need the same word list and the same model, and need to process the new data in exactly the same way as you had processed the learning data. The following video demonstrates how this can be done.</p>
<p>
	<a href="https://s3.amazonaws.com/awsbigdatablog/3-AmazonS3-RapidMiner-Text-Mining-Video.mp4">Video: Applying Store Operators in RapidMiner</a></p>
<p>
	<strong>Applying Unseen Data to the RapidMiner Model</strong></p>
<p>
	The following video demonstrates how to apply your model on new unseen data using Retrieve operators in order to predict whether a new message is ham or spam.</p>
<p>
	<a href="https://s3.amazonaws.com/awsbigdatablog/4-AmazonS3-RapidMiner-Text-Mining-Video.mp4" target="_blank">Video: Applying Unseen Data to the RapidMiner Model</a></p>
<p>
	<strong>Saving Results using Write S3 Operator</strong></p>
<p>
	The following video demonstrates how the Write S3 operator can be used in RapidMiner to save output results into an S3 bucket configured with a connection as outlined previously. You can download the output results file from your specified S3 bucket into your local machine and view the results using a text editor.</p>
<p>
	<a href="https://s3.amazonaws.com/awsbigdatablog/5-AmazonS3-RapidMiner-Text-Mining-Video.mp4" target="_blank">Video: Saving Results Using Write S3 Operator</a></p>
<h1>
	<strong>Conclusion</strong></h1>
<p>
	I’ve shown you how to easily create a text mining application using RapidMiner and Amazon S3. Typically, such tasks would require complex programming knowledge along with hardware and software resources that are often difficult to provision and manage.&nbsp; Integrating RapidMiner with S3 lets you quickly and easily create text mining models while allowing you to tap into the flexible, secure, durable, and highly scalable environment provided by S3.</p>
<p>
	If you have questions or suggestions, please leave a comment below.</p>
<p>
	-----------------------------------------------------------</p>
<p>
	<em><span style="font-size:16px;"><strong>Want to dive deeper into machine learning?</strong></span></em></p>
<p>
	<strong><a href="https://blogs.aws.amazon.com/bigdata/post/Tx2OZ63RJ6Z41A0/Building-a-Numeric-Regression-Model-with-Amazon-Machine-Learning" target="_blank">Building a Numeric Regression Model with Amazon Machine Learning</a></strong></p>
<p>
	<img alt="" height="110" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Social%20media%20image%20for%20ML%20thumbnail.PNG" width="363" /></p>
<p>
	---------------------------------------------------------------</p>
<p>
	<span style="font-size:14px;"><em><strong>Love to work on open source? Check out EMR's <a href="http://www.amazon.jobs/results?jobCategoryIds[]=83&amp;jobCategoryIds[]=70&amp;businessCategoryIds[]=1&amp;locationCities[]=US%2C%20CA%2C%20Palo%20Alto&amp;locationCities[]=US%2C%20WA%2C%20Seattle&amp;searchStrings[]=EMR" target="_blank">careers page</a>.</strong></em></span></p>
<p>
	----------------------------------------------------------------</p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p></div>
  </div>

  <div class="post-footer-block">
  <div class="fl-r">
    <ul class="share-buttons navigation inline-navigation">
      <li class="quiet">
        Share</li>
      <li>
        <span class="social-media email share-post">
<a href="/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx22THFQ9MI86F9" class="share-url icon email-icon" onclick="window.open('/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx22THFQ9MI86F9', '_blank', 'location=yes,width=700,height=600');return false;" title="Share via Email"></a>
</span>
</li>
      <li>            
        <span class="social-media facebook share-post">
<a href="https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx22THFQ9MI86F9%2FApplying-Machine-Learning-to-Text-Mining-with-Amazon-S3-and-RapidMiner&t=Applying%20Machine%20Learning%20to%20Text%20Mining%20with%20Amazon%20S3%20and%20RapidMiner" class="share-url icon facebook-icon" onclick="window.open('https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx22THFQ9MI86F9%2FApplying-Machine-Learning-to-Text-Mining-with-Amazon-S3-and-RapidMiner&t=Applying%20Machine%20Learning%20to%20Text%20Mining%20with%20Amazon%20S3%20and%20RapidMiner', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Facebook"></a>
</span>
</li>
      <li>
        <span class="social-media twitter share-post">
<a href="https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx22THFQ9MI86F9%2FApplying-Machine-Learning-to-Text-Mining-with-Amazon-S3-and-RapidMiner&via=awscloud&text=Applying+Machine+Learning+to+Text+Mining+with+Amazon+S3+and+RapidMiner&count=none" class="share-url icon twitter-icon" onclick="window.open('https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx22THFQ9MI86F9%2FApplying-Machine-Learning-to-Text-Mining-with-Amazon-S3-and-RapidMiner&via=awscloud&text=Applying+Machine+Learning+to+Text+Mining+with+Amazon+S3+and+RapidMiner&count=none', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Twitter"></a>
</span></li>
      </ul>
  </div>
<div>
  <ul class="post-attributes navigation inline-navigation blog-ui-piped push-bottom-1">
    <li>
      <a href="/bigdata/post/Tx22THFQ9MI86F9/Applying-Machine-Learning-to-Text-Mining-with-Amazon-S3-and-RapidMiner#" rel="bookmark">
        <time datetime="2015-06-25T14:48:21.861Z" title="2015-06-25T14:48:21.861Z" class="post-creation-date">June 25, 2015</time></a>
    </li>
    <li>
          <a href="/bigdata/post/Tx22THFQ9MI86F9/Applying-Machine-Learning-to-Text-Mining-with-Amazon-S3-and-RapidMiner#" rel="bookmark">Permalink</a>
        </li>
    <li>
          <a href="/bigdata/post/Tx22THFQ9MI86F9/Applying-Machine-Learning-to-Text-Mining-with-Amazon-S3-and-RapidMiner#postCommentsTx22THFQ9MI86F9">Comments (<span id="commentNumber">1</span>)</a>
        </li>
    </ul>
</div></div></article>
</li>
          <li class="blog-post">
              <article
  class="post display published no-comments " 
  data-postid="Tx21LOP0UQ2ZA9N">
  
  <div class="post-heading-block clearfix">

  <div class="post-author">
      
<div class="author avatar author-avatar blog-ui-avatar blog-ui-author-avatar" data-content-id="herona">
  <a href="/bigdata/blog/author/Intent+Media" title="Intent Media"><img class="author-image" alt="Intent Media" src="//cdn.amazonblogs.com/bigdata_awsblog/authors/Resized logo-IntentMedia_1434403726740.PNG"></a>
</div></div>
  <div class="post-heading">

    <h1 class="post-title navigation">
      <a href="/bigdata/post/Tx21LOP0UQ2ZA9N/Large-Scale-Machine-Learning-with-Spark-on-Amazon-EMR" rel="bookmark">
        Large-Scale Machine Learning with Spark on Amazon EMR</a>
    </h1>

    <ul class="post-attributes navigation inline-navigation blog-ui-piped">
      <li>
        <a href="/bigdata/post/Tx21LOP0UQ2ZA9N/Large-Scale-Machine-Learning-with-Spark-on-Amazon-EMR#" rel="bookmark">
          <time datetime="2015-06-16T20:09:39.553Z" title="2015-06-16T20:09:39.553Z" class="post-creation-date">June 16, 2015</time></a>
      </li>
      <li class="last">      
        <address class="by-line">
          <a rel="nofollow" data-contentid="herona" class="author-name" itemprop="author" href="/bigdata/blog/author/Intent+Media">Intent Media</a>
</address>
      </li>    
    </ul>
    
    <ul class="category-list post-categories navigation inline-navigation blog-ui-piped">
  </ul><ul class="tag-list post-tags navigation inline-navigation blog-ui-piped">
  </ul></div>      
</div><div class="post-content-block">
    <div class="post-text"><p>
	<em>This is a guest post by Jeff Smith, Data Engineer at Intent Media</em></p>
<p>
	<a href="http://intentmedia.com/" target="_blank">Intent Media</a> operates a platform for advertising on commerce sites.&nbsp; We help online travel companies optimize revenue on their websites and apps through sophisticated data science capabilities. On the data team at Intent Media, we are responsible for processing terabytes of e-commerce data per day and using that data and machine learning techniques to power prediction services for our customers.</p>
<h1>
	<strong>Our Big Data Journey</strong></h1>
<p>
	Building large-scale machine learning models has never been simple.&nbsp; Over the history of our team, we’ve continually evolved our approach for running modeling jobs.&nbsp;</p>
<h1>
	The dawn of big data: Java and Pig on Apache Hadoop</h1>
<p>
	Our first data processing jobs were built on Hadoop MapReduce using the Java API.&nbsp; After building some basic aggregation jobs, we went on to develop a scalable, reliable <a href="https://github.com/intentmedia/admm" target="_blank">implementation of logistic regression on Hadoop</a> using this paradigm.&nbsp; While Hadoop MapReduce certainly gave us the ability to operate at the necessary scale, using the Java API resulted in verbose, difficult-to-maintain code.&nbsp; More importantly, the achievable feature development velocity using the complex Java API was not fast enough to keep up with our growing business. Our implementation of Alternating Direction Method of Multipliers (ADMM) logistic regression on Hadoop consists of several thousand lines of Java code.&nbsp; As you might imagine, it took months to develop.&nbsp; Compared with a library implementation of logistic regression that can be imported and applied in a single line, this was simply too large of a time investment.</p>
<p>
	Around the same time, we built some of our decisioning capabilities in Pig, a Hadoop-specific domain language (DSL).&nbsp; Part of our motivation for looking into Pig was to write workflows at a higher level of abstraction than the Java Hadoop API allowed. Although we had some successes with Pig, eventually we abandoned it. &nbsp;Pig was still a young application, and because it is implemented as a DSL, it led to certain inherent difficulties for our team, such as immature tooling. For example, PigUnit, the xUnit testing framework for Pig, was only released in December 2010 and took a while to mature.&nbsp; For years after its release, it was still not integrated into standard Pig distributions or published as a Maven artifact.&nbsp; Given our strong TDD culture, we really craved mature tooling for testing. Other difficulties included the impedance mismatch with our codebaase at the time, which was largely Java.</p>
<h1>
	Going functional and logical with Cascalog</h1>
<p>
	Our issues with Pig drove us to re-implement many of these capabilities in Cascalog, a Hadoop DSL written in Clojure and inspired by Datalog.&nbsp; We found that Cascalog’s more concise logic programming style can be far more declarative than low-level Java Hadoop MapReduce code.&nbsp; Moreover, by using Cascalog, we were able to write the supporting functionality of our capabilities in Clojure with less of an impedance mismatch problem than with Pig.&nbsp; As a functional language, Clojure allowed us to write more pure, testable, and composable code.</p>
<p>
	Despite these advantages, we still found Cascalog lacking as a long-term solution.&nbsp; As a macro-driven DSL, it was often difficult to reason about its behavior.&nbsp; Particular composition approaches still led to difficulties with testing isolated components, rather than whole pipelines.&nbsp; The interoperation with Clojure also proved to be less true in practice than in principle.&nbsp; Logic programming is a useful paradigm for reasoning about data transformations.&nbsp; But Cascalog’s version of logic programming simply has different semantics than Clojure’s version of functional programming.&nbsp; This mismatch made it hard to develop good patterns for combined Clojure/Cascalog applications.</p>
<h1>
	Questioning Hadoop MapReduce</h1>
<p>
	By this time, we had also realized that Hadoop MapReduce’s approach to data processing was not always a good fit for our use case.&nbsp; By writing to disk after each step, Hadoop ensures fault-tolerance but incurs a significant runtime cost.&nbsp; Because we use Amazon Elastic MapReduce (Amazon EMR), we largely did not need most of this fault-tolerance.&nbsp; Our EMR clusters were being spun up dynamically.&nbsp; The outputs were stored in S3 and an application database, so persistence using HDFS after a job was completed was not necessary.&nbsp; Also, Hadoop MapReduce’s execution model was simply not a great fit for the highly-iterative machine learning algorithms that we were trying to implement.</p>
<p>
	We realized that we wanted a data processing platform suited for iterative computations, ideally with some understanding of machine learning out of the box.&nbsp; It needed to have a high-level API that allowed us to compose our applications using functional programming idioms in a robust production language.&nbsp; There is, in fact, such a platform; it’s called Spark.</p>
<h1>
	<strong>Success with Spark</strong></h1>
<p>
	<a href="http://spark.apache.org/" target="_blank">Apache Spark&nbsp;</a> was originally developed at UC Berkeley explicitly for the use case of large-scale machine learning.&nbsp; Early in Spark’s development, the team realized that Spark could be a general data processing platform, so they carved out different pieces of functionality into separate subprojects, all relying on common facilities provided by Spark Core.&nbsp; The machine learning capabilities became a library called <a href="https://spark.apache.org/mllib/" target="_blank">MLlib</a>, and there are libraries for <a href="https://spark.apache.org/streaming/" target="_blank">streaming</a>, <a href="https://spark.apache.org/sql/" target="_blank">SQL</a>, and <a href="https://spark.apache.org/graphx/" target="_blank">graph processing</a> as well.</p>
<h1>
	Performance</h1>
<p>
	Compared to Hadoop, Spark is much better suited for building large-scale machine learning problems.&nbsp; By maintaining and reasoning about the execution’s directed acyclic graph (DAG), Spark can figure out when to cache data in memory.&nbsp; This and other features allow it to be up to 100 times faster than Hadoop for some workflows.&nbsp; In our experience, it we have seen an order of magnitude of performance improvement before any tuning.</p>
<h1>
	Loving our code again</h1>
<p>
	Beyond better performance, the developer experience when using Spark is much better than when developing against Hadoop.&nbsp; Spark’s Scala, Java, and Python APIs are famously well-conceived and provide a functional programming data model that is declarative and high-level.&nbsp; At first, we wrote our jobs against the Java API using a Clojure DSL called <a href="https://github.com/yieldbot/flambo" target="_blank">flambo</a>.&nbsp; The usability and functional style of Spark’s API kept flambo’s design very simple, allowing us to extend the library’s functionality where we needed greater access to Spark and MLlib’s capabilities.</p>
<p>
	More recently, we’ve been exploring Scala for writing some of our Spark jobs.&nbsp; By removing the intermediate layer of flambo, we get even simpler code and can adopt new Spark features as soon as they’re released.&nbsp; We expect the transition to be smooth and easy, due to the high quality and clarity of our existing Clojure codebase that we use to build Spark jobs.&nbsp; This is largely a testament to the power of Spark’s <a href="https://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds" target="_blank">resilient distributed dataset</a> abstraction and the functional programming model it allows. Scala, like Clojure, is an excellent functional programming language that allows us to write clean, testable code.&nbsp;</p>
<p>
	We just released an open source library for composing pipelines called <a href="https://github.com/intentmedia/mario" target="_blank">Mario</a>, which the example below will use. It is motivated by our experiences with Spark and our desire to compose pipelines in a functional, declarative style, similar to the way Spark programs are written.&nbsp; For more details about Mario, see the <a href="http://intentmedia.com/one-up-building-machine-learning-pipelines-with-mario/" target="_blank">launch post</a> on the Intent Media blog.</p>
<h1>
	Algorithms for free</h1>
<p>
	Another significant advantage of using Spark as a platform has been getting access to scalable library implementations of common machine learning algorithms via MLlib.&nbsp; In the past, we had to implement every machine learning algorithm that we wanted to employ because they were not readily available for running jobs at scale with Hadoop.&nbsp; With MLlib and Spark, many learning algorithms are already available, and a vibrant open source community is actively improving them and adding new algorithms.&nbsp;</p>
<h1>
	<strong>Enabling Services</strong></h1>
<p>
	Some people might be overwhelmed by the number of changes that we’ve made to our data platform in just a few years.&nbsp; We’re proud of our ability to iterate rapidly and find new solutions.&nbsp; It’s part of the fun at a fast-moving startup.&nbsp;</p>
<p>
	A big reason why we’ve been able to continually refine our approaches to machine learning at scale is that we use Amazon Web Services (AWS) to provide us with the infrastructure we need to execute on our ambitions.&nbsp; All of our applications have always been deployed on Amazon EC2.&nbsp; All of our raw data is stored on Amazon S3.&nbsp; We use Amazon DynamoDB to store the data for <a href="http://intentmedia.com/cross-browser-user-bridging-with-dynamodb/" target="_blank">our user identification system</a>.</p>
<h1>
	Iterating and shipping using Amazon EMR</h1>
<p>
	Perhaps most importantly, all of our large-scale data processing jobs are executed on EMR.&nbsp; When we started using Hadoop with EMR, we were able to focus on the higher-level problems of data processing and modeling, rather than creating and maintaining Hadoop clusters.&nbsp; This allowed us to rapidly and broadly explore all of the approaches discussed above. &nbsp;After learning that Hadoop would not be a long-term solution for our modeling workflows, we used EMR to get up and running quickly on the Spark platform.</p>
<p>
	By being able to construct on-demand clusters programmatically that auto-terminate on completion, we’ve been able to use ephemeral clusters for all our data jobs. For much of the day, we can have very few data processing clusters running at any given time.&nbsp; But periodically, we spin up many large clusters via EMR that train all of the models that we need to learn.&nbsp; This usage pattern is neither harder to implement nor more expensive than a serial execution of all of our jobs and matches our preferred workflow much better.&nbsp; For our usage pattern, this actually represents a large cost savings over a persistent cluster.</p>
<p>
	Spark on EMR can also read and write directly to S3 using EMRFS.&nbsp; This allowed us to continue to use S3 as our persistent data store as we had done in our Hadoop workflows.</p>
<p>
	Similar to the advantages we get from Spark and MLlib, there is a huge advantage for a startup like ours to pick up tooling advances made by other teams.&nbsp; This frees up time that we would have otherwise spent building this functionality in-house.&nbsp; In the case of EMR, we don’t need to worry about finding the time to implement cluster termination, Hadoop installation, cluster-level monitoring, or a cluster management interface.&nbsp; By giving us a web console that allows us to manage our clusters, EMR makes it much easier to get everyone up to speed on the status of their jobs and to show people how to debug jobs as they are developed.&nbsp; The simplicity of creating and managing clusters via the web interface allows data analysts to use Hadoop and Spark on EMR clusters for ad hoc analyses without needing deep knowledge in cluster creation or management.</p>
<h1>
	<strong>A Sample Workflow</strong></h1>
<p>
	Below is an example of a simplified version of the sort of workflow we run many times every day on EMR.&nbsp; It is a basic model learning pipeline, starting with ingesting previously learned features and ending with persisting a learned model.&nbsp;</p>
<p>
	<img alt="Machine learning workflow with Spark" height="430" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/DAG%20image%20for%20post%20v4.jpg" width="613" /></p>
<p>
	This example builds on our <a href="http://intentmedia.com/machine-learning-with-spark-mllib-on-elastic-2/" target="_blank">initial prototype workflow</a> developed last year, as well as the examples in the <a href="https://spark.apache.org/docs/latest/programming-guide.html" target="_blank">Spark programming guide</a>.&nbsp; Additionally, this example uses the <a href="https://github.com/intentmedia/mario">Mario library</a> that we developed to compose our pipelines in a type-safe and declarative style using functional idioms.</p>
<p>
	In our production pipeline, we extract features from the raw data collected by our applications and stored on S3.&nbsp; These features are semantically meaningful, derived representations of our raw data.&nbsp; They are the input to our model learning algorithm.&nbsp;&nbsp; Our production machine learning pipeline extracts hundreds of non-trivial features, but this example simply uses arbitrary identifiers to stand in for real features.</p>
<p>
	In this example, we begin with defining a function to load the feature data from S3.&nbsp; This function is used to load both the training and testing datasets.</p>
<pre>
def loadFeatures(inputPath: String) = MLUtils.loadLibSVMFile(sc, inputPath)
</pre>
<p>
	The example files use <a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html" target="_blank">LibSVM format</a>, a common format for storing features.&nbsp; MLlib comes with utilities that understand this format and can parse it into an RDD of LabeledPoints.&nbsp; It’s also worth noting that LibSVM is a sparse format.&nbsp; Having native support for a common sparse format is tremendously valuable for us.&nbsp; In previous versions of our pipeline, we wrote a lot of code that handled transformations between dense and sparse formats for different libraries.</p>
<p>
	Because the loading of training and testing data does not rely on any upstream dependencies, these two steps can be started concurrently.&nbsp; Spark is able to determine this through its understanding of the DAG of the job.&nbsp; Also, the Mario library provides similar guarantees around concurrent execution for all operations defined in the pipeline, even if those operations are not Spark operations (e.g., retrieving data from a database).</p>
<p>
	Next, you define a function to learn a model from the features in the training set.&nbsp; This is one of the key advantages we picked up in our transition to Spark and MLlib. &nbsp;All of the model learning functionality comes from the library implementation.</p>
<pre>
def learnModel(trainingData: RDD[LabeledPoint]) = new LogisticRegressionWithLBFGS()
  .setNumClasses(2)
  .run(trainingData)
</pre>
<p>
	Then you define a function to evaluate that model over a test set.&nbsp; This allows you to see how the model can be expected to perform in live usage.&nbsp; In our production workflow, it also allows us to set a threshold for classification based on arbitrary parameters that come from our business use case.</p>
<pre>
def predictAndLabel(testingData: RDD[LabeledPoint], model: LogisticRegressionModel) =
testingData.map { case LabeledPoint(label, features) =&gt;
  val prediction = model.predict(features)
  (prediction, label)
}
</pre>
<p>
	You can define a function to see some of those basic statistics around model performance using built-in functionality from MLlib:</p>
<pre>
def metrics(predictionsAndLabels: RDD[(Double, Double)]) = new MulticlassMetrics(predictionsAndLabels)
</pre>
<p>
	Then, you define functions to persist the model and the performance statistics to disk (S3):</p>
<pre>
def saveModel(outputPath: String)(model: LogisticRegressionModel) = {
  val modelPath = outputPath + &quot;/model&quot;
  model.save(sc, modelPath)
}

def saveMetrics(outputPath: String)(metrics: MulticlassMetrics) = {
  val precision = metrics.precision
  val recall = metrics.recall
  val metricsPath = outputPath + &quot;/metrics&quot;
  val metricsStringRDD = sc.parallelize(List(precision, recall))
  metricsStringRDD.saveAsTextFile(metricsPath)
}
</pre>
<p>
	Then you compose all of these steps into a single pipeline:</p>
<pre>
for {
  trainingDataStep &lt;- pipe(loadFeatures(trainingDataPath))
  testingDataStep &lt;- pipe(loadFeatures(testingDataPath))

  modelStep &lt;- pipe(learnModel, trainingDataStep)

  predictionsAndLabelsStep &lt;- pipe(predictAndLabel, testingDataStep, modelStep)

  metricsStep &lt;- pipe(metrics, predictionsAndLabelsStep)
  saveMetricsStep &lt;- pipe(saveMetrics(currentOutputPath), metricsStep)

  saveModelStep &lt;- pipe(saveModel(currentOutputPath), modelStep)

} yield modelStep.runWith(saveMetricsStep, saveModelStep)
</pre>
<p>
	Those steps that do not yield a result and simply have a side effect (writing to disk) are specified in the call to the runWith method.&nbsp; The composed pipeline yields the learned model as its return value.</p>
<p>
	In our production workflow, we load some of this persisted data into an application database for use in runtime model serving.&nbsp; Other data about the model learning pipeline’s performance is loaded to an analytics database for reporting and analytics tasks.</p>
<p>
	This job must be compiled into a JAR to be provided to your EMR cluster.&nbsp; Do this using sbt:</p>
<pre>
sbt assembly
</pre>
<p>
	After you have a JAR, you can push it to S3.&nbsp; The following AWS CLI command copies the JAR to S3:</p>
<pre>
aws s3 cp spark-emr/target/scala-2.10/spark-emr-assembly-1.0.jar s3://your-bucket-name/$USER/spark/jars/spark-emr-assembly-1.0.jar</pre>
<p>
	Then, start up an EMR cluster that executes the Spark job you just wrote using the AWS CLI:</p>
<pre>
aws emr create-cluster \
  --name &quot;exampleJob&quot; \
  --ec2-attributes KeyName=MyKeyName \
  --auto-terminate \
  --ami-version 3.8.0 \
  --instance-type m3.xlarge \
  --instance-count 3 \
  --log-uri s3://your-bucket-name/$USER/spark/`date +%Y%m%d%H%M%S`/logs \
  --applications Name=Spark,Args=[-x] \
  --steps &quot;Name=\&quot;Run Spark\&quot;,Type=Spark,Args=[--deploy-mode,cluster,--master,yarn-cluster,--conf,spark.executor.extraJavaOptions=-XX:MaxPermSize=256m,--conf,spark.driver.extraJavaOptions=-XX:MaxPermSize=512m,--class,ModelingWorkflow,s3://your-bucket-name/$USER/spark/jars/spark-emr-assembly-1.0.jar,s3://support.elasticmapreduce/bigdatademo/intentmedia/,s3://your-bucket-name/$USER/spark/output/]&quot;
</pre>
<p>
	After the job finishes, the cluster terminates automatically.&nbsp; You can adjust the size of the nodes and the size of the cluster by just changing the InstanceCount and InstanceType arguments to suit your workload.&nbsp; You can see the full code for <a href="http://github.com/awslabs/aws-big-data-blog/tree/master/aws-blog-machine-learning-with-spark/spark-emr" target="_blank">this example</a> in the AWS Big Data Blog GitHub Repo.</p>
<h1>
	<strong>Wrapping Up</strong></h1>
<p>
	This post has given you an overview of how Intent Media has evolved our data platform through a variety of different approaches.&nbsp; We’ve found great success using popular open source frameworks like Spark and MLlib to learn models at massive scale.&nbsp; The advantages of using these tools are further amplified by relying on AWS and EMR, specifically, to create and manage our clusters.&nbsp; The combination of these approaches has enabled us to move quickly and scale with our technology alongside our rapidly expanding business.</p>
<p>
	If you have questions or suggestions, please leave a comment below.</p>
<p>
	------------------------------</p>
<p>
	<em><span style="font-size:16px;"><strong>More posts about Machine Learning:</strong></span></em></p>
<p>
	<strong><a href="https://blogs.aws.amazon.com/bigdata/post/Tx2OZ63RJ6Z41A0/Building-a-Numeric-Regression-Model-with-Amazon-Machine-Learning" target="_blank">Building a Numeric Regression Model with Amazon Machine Learning</a></strong></p>
<p>
	<img alt="" height="110" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Social%20media%20image%20for%20ML%20thumbnail.PNG" width="363" /></p>
<p>
	----------------------------------</p>
<p>
	<strong><em>Love to work on open source? Check out EMR's </em></strong><a href="http://www.amazon.jobs/results?jobCategoryIds%5b%5d=83&amp;jobCategoryIds%5b%5d=70&amp;businessCategoryIds%5b%5d=1&amp;locationCities%5b%5d=US%2C%20CA%2C%20Palo%20Alto&amp;locationCities%5b%5d=US%2C%20WA%2C%20Seattle&amp;searchStrings%5b%5d=EMR" target="_blank"><strong><em>careers page</em></strong></a><strong><em>.</em></strong></p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p></div>
  </div>

  <div class="post-footer-block">
  <div class="fl-r">
    <ul class="share-buttons navigation inline-navigation">
      <li class="quiet">
        Share</li>
      <li>
        <span class="social-media email share-post">
<a href="/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx21LOP0UQ2ZA9N" class="share-url icon email-icon" onclick="window.open('/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx21LOP0UQ2ZA9N', '_blank', 'location=yes,width=700,height=600');return false;" title="Share via Email"></a>
</span>
</li>
      <li>            
        <span class="social-media facebook share-post">
<a href="https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx21LOP0UQ2ZA9N%2FLarge-Scale-Machine-Learning-with-Spark-on-Amazon-EMR&t=Large-Scale%20Machine%20Learning%20with%20Spark%20on%20Amazon%20EMR" class="share-url icon facebook-icon" onclick="window.open('https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx21LOP0UQ2ZA9N%2FLarge-Scale-Machine-Learning-with-Spark-on-Amazon-EMR&t=Large-Scale%20Machine%20Learning%20with%20Spark%20on%20Amazon%20EMR', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Facebook"></a>
</span>
</li>
      <li>
        <span class="social-media twitter share-post">
<a href="https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx21LOP0UQ2ZA9N%2FLarge-Scale-Machine-Learning-with-Spark-on-Amazon-EMR&via=awscloud&text=Large-Scale+Machine+Learning+with+Spark+on+Amazon+EMR&count=none" class="share-url icon twitter-icon" onclick="window.open('https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx21LOP0UQ2ZA9N%2FLarge-Scale-Machine-Learning-with-Spark-on-Amazon-EMR&via=awscloud&text=Large-Scale+Machine+Learning+with+Spark+on+Amazon+EMR&count=none', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Twitter"></a>
</span></li>
      </ul>
  </div>
<div>
  <ul class="post-attributes navigation inline-navigation blog-ui-piped push-bottom-1">
    <li>
      <a href="/bigdata/post/Tx21LOP0UQ2ZA9N/Large-Scale-Machine-Learning-with-Spark-on-Amazon-EMR#" rel="bookmark">
        <time datetime="2015-06-16T20:09:39.553Z" title="2015-06-16T20:09:39.553Z" class="post-creation-date">June 16, 2015</time></a>
    </li>
    <li>
          <a href="/bigdata/post/Tx21LOP0UQ2ZA9N/Large-Scale-Machine-Learning-with-Spark-on-Amazon-EMR#" rel="bookmark">Permalink</a>
        </li>
    <li>
          <a href="/bigdata/post/Tx21LOP0UQ2ZA9N/Large-Scale-Machine-Learning-with-Spark-on-Amazon-EMR#postCommentsTx21LOP0UQ2ZA9N">Comments (<span id="commentNumber">0</span>)</a>
        </li>
    </ul>
</div></div></article>
</li>
          <li class="blog-post">
              <article
  class="post display published no-comments " 
  data-postid="TxGVITXN9DT5V6">
  
  <div class="post-heading-block clearfix">

  <div class="post-author">
      
<div class="author avatar author-avatar blog-ui-avatar blog-ui-author-avatar" data-content-id="gernest">
  <a href="/bigdata/blog/author/Guy+Ernest" title="Guy Ernest"><img class="author-image" alt="Guy Ernest" src="//cdn.amazonblogs.com/bigdata_awsblog/authors/Guy_pic.fw_1430254017862.png"></a>
</div></div>
  <div class="post-heading">

    <h1 class="post-title navigation">
      <a href="/bigdata/post/TxGVITXN9DT5V6/Building-a-Binary-Classification-Model-with-Amazon-Machine-Learning-and-Amazon-R" rel="bookmark">
        Building a Binary Classification Model with Amazon Machine Learning and Amazon Redshift</a>
    </h1>

    <ul class="post-attributes navigation inline-navigation blog-ui-piped">
      <li>
        <a href="/bigdata/post/TxGVITXN9DT5V6/Building-a-Binary-Classification-Model-with-Amazon-Machine-Learning-and-Amazon-R#" rel="bookmark">
          <time datetime="2015-06-12T00:07:15.238Z" title="2015-06-12T00:07:15.238Z" class="post-creation-date">June 11, 2015</time></a>
      </li>
      <li class="last">      
        <address class="by-line">
          <a rel="nofollow" data-contentid="gernest" class="author-name" itemprop="author" href="/bigdata/blog/author/Guy+Ernest">Guy Ernest</a>
</address>
      </li>    
    </ul>
    
    <ul class="category-list post-categories navigation inline-navigation blog-ui-piped">
  </ul><ul class="tag-list post-tags navigation inline-navigation blog-ui-piped">
  </ul></div>      
</div><div class="post-content-block">
    <div class="post-text"><p>
	<em>Guy Ernest is a Solutions Architect with AWS</em></p>
<p>
	<em>This post builds on Guy's earlier posts </em><a href="https://blogs.aws.amazon.com/bigdata/post/Tx2OZ63RJ6Z41A0/Building-a-Numeric-Regression-Model-with-Amazon-Machine-Learning" target="_blank"><em>Building a Numeric Regression Model with Amazon Machine Learning</em></a><em> and </em><a href="http://blogs.aws.amazon.com/bigdata/post/Tx2LQ4WAWOP80EG/Building-a-Multi-Class-ML-Model-with-Amazon-Machine-Learning" target="_blank"><em>Building a Multi-Class ML Model with Amazon Machine Learning</em></a><em>.</em></p>
<p>
	Many decisions in life are binary, answered either Yes or No. Many business problems also have binary answers. For example: &quot;Is this transaction fraudulent?&quot;, &quot;Is this customer going to buy that product?&quot;, or &quot;Is this user going to churn?&quot; In machine learning, this is called a binary classification problem. Many business decisions can be enhanced by accurately predicting the answer to a binary question. Amazon Machine Learning (<a href="http://aws.amazon.com/machinelearning" target="_blank">Amazon ML</a>) provides a simple and low-cost option to answer some of these questions at speed and scale.</p>
<p>
	Like the previous posts (<a href="http://blogs.aws.amazon.com/bigdata/post/Tx2OZ63RJ6Z41A0/Building-a-Numeric-Regression-Model-with-Amazon-Machine-Learning" target="_blank">Numeric Regression</a> and <a href="http://blogs.aws.amazon.com/bigdata/post/Tx2LQ4WAWOP80EG/Building-a-Multi-Class-ML-Model-with-Amazon-Machine-Learning" target="_blank">Multiclass Classification</a>), this post uses a publicly available example from <a href="http://www.kaggle.com" target="_blank">Kaggle</a>. This time, you will use the <a href="http://www.kaggle.com/c/avazu-ctr-prediction" target="_blank">Click-Through Rate Prediction</a> example, which is from the online advertising field. In this example, you will predict the likelihood that a specific user will click on a specific ad.</p>
<h1>
	Preparing the data to build the machine learning model</h1>
<p>
	You'll be getting the data for building the model from the competition site, but to make it more realistic, you will use <a href="http://aws.amazon.com/redshift" target="_blank">Amazon Redshift</a> as an intermediary. In many cases, historical event data required to build a machine learning model is already stored in the data warehouse. Amazon ML integrates with Amazon Redshift to allow you to query relevant event data and perform aggregation, join, or manipulation operations to prepare the data to train the machine learning model. You will see some examples for these operations in this post.</p>
<p>
	To be able to follow through this exercise, you need an AWS account, Kaggle account (to download the data set), Amazon Redshift cluster, and SQL client. If you don't already have an Amazon Redshift cluster, you can get a two-month free trial for a dw2.large single-node cluster, which you can use for this demo.</p>
<h1>
	Setting up an Amazon Redshift cluster</h1>
<p>
	In the <a href="https://console.aws.amazon.com/redshift/home?region=us-east-1" target="_blank">AWS Management Console</a>, in the <strong>Supported Regions</strong> list, choose <strong>US East (N. Virginia)</strong>, and then Amazon Redshift in the <strong>Database</strong> section. Choose <strong>Launch Cluster</strong>.</p>
<p>
	On the <strong>Cluster Details</strong> page, provide a name for the cluster (for example, <em>ml-demo</em>) and for the database (for example, <em>dev</em>), and then provide the master user name and a password.</p>
<p>
	<img alt="Naming the cluster" height="343" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_1.png" width="601" /></p>
<p>
	On the <strong>Node Configuration</strong> page, define the layout of the cluster. For the amount of data in this example, a single dc1.large node is sufficient (and fits into the Amazon Redshift free tier).</p>
<p>
	<img alt="" height="290" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Ml_Redshift_2a.png" width="576" /></p>
<p>
	Choose <strong>Continue</strong>, and on the following page review the settings and choose <strong>Launch Cluster</strong>. After a few minutes, the cluster is available. Choose the cluster name to see its configuration.</p>
<p>
	<img alt="" height="238" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_3a.png" width="573" /></p>
<p>
	&nbsp;</p>
<p>
	For now, you need to note the <strong>Endpoint</strong> value, to be able to connect to the cluster and ingest the data downloaded from the Kaggle site.</p>
<h1>
	Downloading and storing the data</h1>
<p>
	Download the training file from the <a href="https://www.kaggle.com/c/avazu-ctr-prediction/data" target="_blank">competition site</a>, and then upload it to Amazon Simple Storage Service (<a href="http://aws.amazon.com/s3" target="_blank">Amazon S3</a>), using the <a href="http://aws.amazon.com/cli/" target="_blank">AWS CLI</a> to handle the large file upload in parts.</p>
<pre>
# Download the train data from:
http://www.kaggle.com/c/avazu-ctr-prediction/download/train.csv.gz
# upload the file to S3
aws s3 cp train.csv.gz s3:///click_thru/input/
</pre>
<p>
	You can use various SQL clients to connect to the cluster, such as <a href="http://www.sql-workbench.net" target="_blank">SQL-Workbench</a> or <a href="http://www.aginity.com/workbench/" target="_blank">Aginity Workbench</a>, or you can connect with <em>psql</em> in a terminal from a Linux-based EC2 instance.</p>
<pre>
ssh -i .pem ec2-user@ec2-.eu-west-1.compute.amazonaws.com
psql -h ml-demo..us-east-1.redshift.amazonaws.com -U  -d dev -p 5439
psql -h ml-demo.&lt;CLUSTER_ID&gt;.us-east-1.redshift.amazonaws.com -U &lt;USER_NAME&gt; -d dev -p 5439
</pre>
<p>
	From your SQL client, create a table to store the events from the competition site. Be sure to use the right data type for each column.</p>
<pre>
CREATE TABLE click_train (
  id varchar(25) not null,
  click boolean,
  -- the format is YYMMDDHH but defined it as string
  hour char(8),
  C1 varchar(20),
  banner_pos smallint,
  site_id varchar(10),
  site_domain varchar(10),
  site_category varchar(10),
  app_id varchar(10),
  app_domain varchar(10),
  app_category varchar(10),
  device_id varchar(10),
  device_ip varchar(10),
  device_model varchar(10),
  device_type integer,
  device_conn_type integer,
  C14 integer,
  C15 integer,
  C16 integer,
  C17 integer,
  C18 integer,
  C19 integer,
  C20 integer,
  C21 integer
);
</pre>
<p>
	In the SQL client, use the <a href="http://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html">COPY</a> command to copy the events into the cluster:</p>
<pre>
COPY click_train FROM 's3:///input/click_thru/train.csv.gz'
credentials 'aws_access_key_id=;aws_secret_access_key=' 
GZIP
DELIMITER ','
IGNOREHEADER 1;
</pre>
<p>
	If everything went okay, you should see that you have more than 40 million records, by using the following SELECT query:</p>
<pre>
dev=# SELECT count(*) FROM click_train;
  count
----------
 40428967
(1 row)
</pre>
<h1>
	Building a machine learning model from data in Amazon Redshift</h1>
<p>
	In the previous blog posts, you built machine learning models from data files in S3. Data files could also originate from SQL dumps from a database. Because using SQL dumps is common, Amazon ML integrates directly with two popular database sources, Amazon Relational Database Service (<a href="http://aws.amazon.com/rds" target="_blank">Amazon RDS</a>) and Amazon Redshift. This integration makes it easier to train machine learning models directly on &quot;live&quot; data by speeding up the process of data ingestion.</p>
<p>
	To build an ML model from data in Amazon Redshift, allow Amazon ML to connect to Amazon Redshift, run the <strong>UNLOAD</strong> command of the relevant query to Amazon S3, and then start the training phase.</p>
<p>
	In the <a href="https://console.aws.amazon.com/iam/home?region=us-east-1#roles" target="_blank">IAM console</a>, create a new role called <strong>AML-Redshift</strong> and choose <strong>Continue</strong>.</p>
<p>
	<img alt="Create a new role" height="120" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_4.png" width="593" /></p>
<p>
	<br />
	On the <strong>Select Role Type</strong> page, choose the default role type for <strong>Amazon Machine Learning Role for Redshift Data Source</strong>.</p>
<p>
	<img alt="" height="177" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_5.png" width="570" /></p>
<p>
	On the <strong>Attach Policy</strong> page, choose the single policy in the list and choose <strong>Continue</strong>.</p>
<p>
	<img alt="" height="125" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_6.png" width="532" /></p>
<p>
	Finally, review the settings for the new role, copy the <strong>Role ARN</strong> value for the next step, and choose <strong>Create</strong>.</p>
<p>
	<img alt="" height="188" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_7.png" width="533" /></p>
<p>
	In the <a href="https://console.aws.amazon.com/machinelearning/home?region=us-east-1#/">Amazon Machine Learning console</a>, choose <strong>Create new… Datasource and ML model</strong>.</p>
<p>
	<img alt="" height="330" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_8.png" width="571" /></p>
<p>
	On the <strong>Data Input</strong> page, choose <strong>Redshift</strong>, and fill in the information, including the ARN of the role that you just created, cluster name, database name, user name, and your password. You also need to specify the SELECT query to use (included below) and the name of the S3 bucket and folder to be used as the staging location.</p>
<p>
	<img alt="" height="497" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_9.png" width="572" /></p>
<p>
	<br />
	&nbsp;In the SQL query, you need to get the binary target 'click' as an integer value (0 or 1), instead as <em>false</em> or <em>true</em>, by converting it to int. We also recommend shuffling the records for the training to remove order influence, using ORDER BY RANDOM().</p>
<pre>
SELECT
 id,
-- target field as 0/1 instead of f/t
 click::int,
 hour,      
 c1,           
 banner_pos,   
 site_id,      
 site_domain,      
 site_category,    
 app_id,           
 app_domain,       
 app_category,     
 device_id,        
 device_ip,        
 device_model,     
 device_type,      
 device_conn_type, 
 c14, c15, c16, c17,
 c18, c19, c20, c21
 FROM click_train
 -- Shuffle the records
 ORDER BY RANDOM();
</pre>
<p>
	In the Amazon ML wizard, on the <strong>Schema</strong> page, you can see the definition of the schema that Amazon identifies automatically from the data. At this stage, it's best to review the proposed values for each one of the attributes, and change the numeric values that represent category ID to 'Categorical' instead.</p>
<p>
	<img alt="" height="406" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_10.png" width="608" /></p>
<p>
	On the <strong>Target</strong> page, choose the <em>click</em> field as the target.</p>
<p>
	<img alt="" height="464" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_11.png" width="593" /></p>
<p>
	Continue with the wizard to define the row ID (<em>id</em> field). When you reach the <strong>Review</strong> page, choose the default settings to create the machine learning model. By default, Amazon ML splits the data so that 70% is used for model training and 30% is used for model evaluation.</p>
<p>
	<img alt="" height="503" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_12.png" width="581" /></p>
<p>
	Building the data source, ML model, and evaluation can take some time because there are so many records to process. You can monitor progress in the Amazon ML dashboard.</p>
<p>
	<img alt="" height="196" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_13.png" width="623" /></p>
<p>
	&nbsp;</p>
<p>
	On the dashboard, you can see that the original datasource that you created is already &quot;In progress,&quot; splitting the datasource to 70% for training and 30% for evaluation is also &quot;In progress,&quot; and ML model creation and evaluation are 'Pending', waiting on the completion of datasource creation. After the process is done, check the model evaluation.</p>
<h1>
	Evaluating the accuracy of the machine learning model</h1>
<p>
	In the previous two blog posts, you saw how Amazon ML provides a prediction accuracy metric (a single number) and a graph that reports the accuracy of a model. For the numeric regression example, we reviewed the root-mean-square-error (RMSE) metric and the error distribution accuracy graph, and for the multiclass classification example, we reviewed the F1 score and the confusion matrix.</p>
<p>
	In this <a href="http://docs.aws.amazon.com/machine-learning/latest/mlconcepts/mlconcepts.html#binary-classification" target="_blank">Binary Classification</a> case, the prediction accuracy metric is called AUC (Area-Under-the-Curve). You can read more about the meaning of this overall score in <a href="http://docs.aws.amazon.com/machine-learning/latest/dg/evaluating_models.html#binary-model-insights" target="_blank">Amazon ML documentation</a>. In this case, the score is 0.74:</p>
<p>
	<img alt="" height="343" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_14.png" width="588" /></p>
<p>
	For more insight, see the <a href="http://docs.aws.amazon.com/machine-learning/latest/dg/evaluating_models.html#using-the-performance-visualization" target="_blank">performance visualization</a>, where the effect of choosing the cut-off score can be better understood. The prediction score for each record is a numeric value between 0 and 1. The closer to 1, the more likely it should be set to <em>Yes</em>, and vice versa for <em>No</em>. With the combination of the cut-off score, the record can fall into one out of four cases:</p>
<ul>
	<li>
		true-positive (TP) – correctly classified as <em>Yes</em></li>
	<li>
		true-negative (TN) – correctly classified as <em>No</em></li>
	<li>
		false-positive (FP) – <strong>wrongly</strong> classified as <em>Yes</em></li>
	<li>
		false-negative (FN) – <strong>wrongly</strong> classified as <em>No</em></li>
</ul>
<p>
	<img alt="" height="310" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_15.png" width="620" /></p>
<p>
	If you set the cut-off score closer to 1, fewer records will be classified as false-positives, but at the same time more records will be classified as false-negatives. Where you set the cut-off value is a business decision. If each false-positive costs a penalty ($1 to present an ad, for example), you might set the value higher, but if each false-negative causes you to miss a big sale ($1,000 commission on a luxury car purchase, for example), you might set the value lower.</p>
<p>
	You can slide the cut-off value on the graph left or right. Sliding it to the left decreases the value, as well as the number of false-positive mistakes, but increases the number of false-negative mistakes. Increasing the value of the cut-off has the opposite effect. You can also use the four slides under <strong>Advance metrics</strong> on the bottom of the graph to control different aspects of the cut-off value. In line with the &quot;No free lunch&quot; theorem, modifying one slide modifies the values of the other slides.</p>
<ul>
	<li>
		<a href="http://docs.aws.amazon.com/machine-learning/latest/dg/evaluating_models.html#accuracy" target="_blank"><strong>Accuracy</strong></a> – The ratio of all correct classifications (<em>yes</em> and <em>no</em>) out of all predictions. This is a balanced view of both types of mistakes.</li>
	<li>
		<a href="http://docs.aws.amazon.com/machine-learning/latest/dg/evaluating_models.html#false-positive-rate" target="_blank"><strong>False Positive Rate</strong></a> – The ratio of actual negative that are predicted as positive out of all negative cases.</li>
	<li>
		<a href="http://docs.aws.amazon.com/machine-learning/latest/dg/evaluating_models.html#precision" target="_blank"><strong>Precision</strong></a> – The ratio of correct positive classifications (<em>yes</em>) out of all positive predictions. This views the problem of saying <em>yes</em> too often (a waste of a costly bid or annoying users with irrelevant pop ups). In other words, how <strong>precise</strong> are you when you decide to send something to a person or how precisely do you spend your marketing budget? For more information and illustrations (such as the one below), see the <a href="http://en.wikipedia.org/wiki/Precision_and_recall" target="_blank">Wikipedia entry on Precision and recall</a>.</li>
</ul>
<p>
	<img alt="" height="204" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_16.png" width="393" /></p>
<ul>
	<li>
		<a href="http://docs.aws.amazon.com/machine-learning/latest/dg/evaluating_models.html#recall" target="_blank"><strong>Recall</strong></a> – The ratio of correct positive classifications (<em>yes</em>) out of all positive records. This views the problem of saying <em>no</em> too often (loss of sale opportunities). In other words, how many of the people that you want to <strong>recall</strong> that you actually get. Recall of 0.06, in the example above, means that only 6% of the people that you want to see the ad (as they would click it) will see it.</li>
</ul>
<p>
	For example, set the <strong>Recall</strong> slide to 0.5 to see what will happen if you want to make sure that you get to at least 50% of your target users with each ad.</p>
<p>
	<img alt="" height="274" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Redshift_17.png" width="615" /></p>
<p>
	As you can see, the accuracy didn't drop dramatically (from 0.83 to 0.74), but the precision decreased significantly (from 0.6 to 0.33), which means that only 1 out of 3 will click the ad compared to 2 out of 3 in the previous settings. These changes are caused by changing the cut-off value, with no changes or improvements to the model itself.</p>
<p>
	You can improve a model by creating new datasources from Amazon Redshift that include additional relevant information, such as the customer's week day and time based on the IP address (which is missing from this data set, but is usually available in real-life data), or whether it is morning, day, evening, or night. Here are some examples of manipulations you can play with when you use SELECT queries on the data from your Amazon Redshift datasource:</p>
<pre>
SELECT
    id,
    click::int,
    -- Calculating the date of the week from the Hour string
    date_part(dow, TO_DATE (hour, 'YYMMDDHH')) as dow,
    -- Creating bins of the hours of the day based on common behaviour
    case
        when RIGHT(Hour,2) &gt;= '00' and RIGHT (Hour,2) &lt;= '05' then 'Night'
        when RIGHT(Hour,2) &gt;= '06' and RIGHT (Hour,2) &lt;= '11' then 'Morning'
        when RIGHT(Hour,2) &gt;= '12' and RIGHT (Hour,2) &lt;= '17' then 'Afternoon'
        when RIGHT(Hour,2) &gt;= '18' and RIGHT (Hour,2) &lt;= '23' then 'Evening'
        else 'Unknown'
    end
        as day_period
...
</pre>
<p>
	Other types of information can include data about your users that can be deduced from clickstream analysis, such as gender or age, and other analytical queries that use <em>JOIN</em> statements on data from other tables in the Amazon Redshift data warehouse.</p>
<h1>
	Summary</h1>
<p>
	In this post, you read about when and how to use the binary classification type of machine learning model offered by Amazon ML. You learned how to use Amazon Redshift as the datasource for training data, select the data, cast the target data type to int to trigger binary classification, and use the RANDOM function to shuffle the data.</p>
<p>
	You also read about how to evaluate a binary classification model, including reviewing accuracy, precision, and recall metrics. This knowledge could help you build, evaluate, and modify binary classification models to solve your business problems.</p>
<p>
	If you have questions or suggestions, please leave a comment below.</p>
<p>
	----------------------------------------------------------------</p>
<p>
	<strong><em>Love to work on open source? Check out our </em></strong><a href="http://www.amazon.jobs/results?jobCategoryIds%5b%5d=83&amp;jobCategoryIds%5b%5d=70&amp;businessCategoryIds%5b%5d=1&amp;locationCities%5b%5d=US%2C%20CA%2C%20Palo%20Alto&amp;locationCities%5b%5d=US%2C%20WA%2C%20Seattle&amp;searchStrings%5b%5d=EMR" target="_blank"><strong><em>careers page</em></strong></a><strong><em>.</em></strong></p>
<p>
	----------------------------------------------------------------</p>
<p>
	<em><span style="font-size:16px;"><strong>Do more with Amazon Machine Learning:</strong></span></em></p>
<p>
	<strong><a href="https://blogs.aws.amazon.com/bigdata/post/Tx2OZ63RJ6Z41A0/Building-a-Numeric-Regression-Model-with-Amazon-Machine-Learning" target="_blank">Building a Numeric Regression Model with Amazon Machine Learning</a></strong></p>
<p>
	<img alt="" height="110" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Social%20media%20image%20for%20ML%20thumbnail.PNG" width="363" /></p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p></div>
  </div>

  <div class="post-footer-block">
  <div class="fl-r">
    <ul class="share-buttons navigation inline-navigation">
      <li class="quiet">
        Share</li>
      <li>
        <span class="social-media email share-post">
<a href="/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=TxGVITXN9DT5V6" class="share-url icon email-icon" onclick="window.open('/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=TxGVITXN9DT5V6', '_blank', 'location=yes,width=700,height=600');return false;" title="Share via Email"></a>
</span>
</li>
      <li>            
        <span class="social-media facebook share-post">
<a href="https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTxGVITXN9DT5V6%2FBuilding-a-Binary-Classification-Model-with-Amazon-Machine-Learning-and-Amazon-R&t=Building%20a%20Binary%20Classification%20Model%20with%20Amazon%20Machine%20Learning%20and%20Amazon%20Redshift" class="share-url icon facebook-icon" onclick="window.open('https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTxGVITXN9DT5V6%2FBuilding-a-Binary-Classification-Model-with-Amazon-Machine-Learning-and-Amazon-R&t=Building%20a%20Binary%20Classification%20Model%20with%20Amazon%20Machine%20Learning%20and%20Amazon%20Redshift', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Facebook"></a>
</span>
</li>
      <li>
        <span class="social-media twitter share-post">
<a href="https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTxGVITXN9DT5V6%2FBuilding-a-Binary-Classification-Model-with-Amazon-Machine-Learning-and-Amazon-R&via=awscloud&text=Building+a+Binary+Classification+Model+with+Amazon+Machine+Learning+and+Amazon+Redshift&count=none" class="share-url icon twitter-icon" onclick="window.open('https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTxGVITXN9DT5V6%2FBuilding-a-Binary-Classification-Model-with-Amazon-Machine-Learning-and-Amazon-R&via=awscloud&text=Building+a+Binary+Classification+Model+with+Amazon+Machine+Learning+and+Amazon+Redshift&count=none', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Twitter"></a>
</span></li>
      </ul>
  </div>
<div>
  <ul class="post-attributes navigation inline-navigation blog-ui-piped push-bottom-1">
    <li>
      <a href="/bigdata/post/TxGVITXN9DT5V6/Building-a-Binary-Classification-Model-with-Amazon-Machine-Learning-and-Amazon-R#" rel="bookmark">
        <time datetime="2015-06-12T00:07:15.238Z" title="2015-06-12T00:07:15.238Z" class="post-creation-date">June 11, 2015</time></a>
    </li>
    <li>
          <a href="/bigdata/post/TxGVITXN9DT5V6/Building-a-Binary-Classification-Model-with-Amazon-Machine-Learning-and-Amazon-R#" rel="bookmark">Permalink</a>
        </li>
    <li>
          <a href="/bigdata/post/TxGVITXN9DT5V6/Building-a-Binary-Classification-Model-with-Amazon-Machine-Learning-and-Amazon-R#postCommentsTxGVITXN9DT5V6">Comments (<span id="commentNumber">0</span>)</a>
        </li>
    </ul>
</div></div></article>
</li>
          <li class="blog-post">
              <article
  class="post display published has-comments " 
  data-postid="Tx11LQAXCMNYZOA">
  
  <div class="post-heading-block clearfix">

  <div class="post-author">
      
<div class="author avatar author-avatar blog-ui-avatar blog-ui-author-avatar" data-content-id="myan">
  <a href="/bigdata/blog/author/Matt+Yanchyshyn" title="Matt Yanchyshyn"><img class="author-image" alt="Matt Yanchyshyn" src="//cdn.amazonblogs.com/bigdata_awsblog/authors/Matt resized.fw_1408114260981.png"></a>
</div></div>
  <div class="post-heading">

    <h1 class="post-title navigation">
      <a href="/bigdata/post/Tx11LQAXCMNYZOA/Test-drive-two-big-data-scenarios-from-the-Building-a-Big-Data-Platform-on-AWS-b" rel="bookmark">
        Test drive two big data scenarios from the 'Building a Big Data Platform on AWS' bootcamp</a>
    </h1>

    <ul class="post-attributes navigation inline-navigation blog-ui-piped">
      <li>
        <a href="/bigdata/post/Tx11LQAXCMNYZOA/Test-drive-two-big-data-scenarios-from-the-Building-a-Big-Data-Platform-on-AWS-b#" rel="bookmark">
          <time datetime="2015-06-04T20:19:57.836Z" title="2015-06-04T20:19:57.836Z" class="post-creation-date">June 4, 2015</time></a>
      </li>
      <li class="last">      
        <address class="by-line">
          <a rel="nofollow" data-contentid="myan" class="author-name" itemprop="author" href="/bigdata/blog/author/Matt+Yanchyshyn">Matt Yanchyshyn</a>
</address>
      </li>    
    </ul>
    
    <ul class="category-list post-categories navigation inline-navigation blog-ui-piped">
  </ul><ul class="tag-list post-tags navigation inline-navigation blog-ui-piped">
  </ul></div>      
</div><div class="post-content-block">
    <div class="post-text"><p>
	<em>Matt Yanchyshyn is a Sr. Manager for AWS Solutions Architecture</em></p>
<p>
	AWS offers a number of events during the year such as our annual <a href="https://reinvent.awsevents.com/" target="_blank">AWS re:Invent conference</a>, the <a href="https://aws.amazon.com/summits/" target="_blank">AWS Summit series</a>, the AWS Pop-up Loft, and a variety of roadshows such as the upcoming <a href="https://aws.amazon.com/events/big-data-solutions-day/" target="_blank">AWS Big Data Solutions Days</a>. All of these provide opportunities for AWS customers to attend talks focused on big data and participate in hands-on learning with AWS trainers, Solutions Architects and&nbsp;product teams. &nbsp;At our annual AWS re:Invent conference last year in Las Vegas, for example, there were more than twenty sessions in the big data track, including full-day bootcamp and hands-on labs.&nbsp;&nbsp;&nbsp;</p>
<p>
	<img alt="AWS Summit Audience" height="328" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Resized%20audience%20pic.PNG" width="607" /></p>
<p>
	At the AWS Pop-up Loft and at a few of this year's AWS Summits we have been giving a full-day bootcamp called &quot;Building a Big Data Platform on AWS”. (Formerly&nbsp;called “Store, Manage and Analyze&nbsp;Big Data in the Cloud”)&nbsp;This bootcamp provides a broad, hands-on introduction to the collection, storage, and analysis of data using AWS services and 3rd-party software. During the day, we dive deep into Amazon S3, Amazon EMR, Amazon Redshift and Amazon Kinesis, and show you how to combine them to build a comprehensive platform for both batch and streaming data analysis. Hands-on exercises help you learn to work with these services and test best practices for designing data analytics environments. &nbsp;This course is highly technical and is geared towards data analysts, DBAs, developers and solution architects.</p>
<p>
	<span style="font-size:16px;"><a href="http://aws.amazon.com/summits/chicago/" target="_blank"><strong>Register for the Chicago Summit </strong></a></span><strong>(July 1)</strong></p>
<p>
	<span style="font-size:16px;"><a href="http://aws.amazon.com/summits/new-york/" target="_top"><strong>Register for the New York Summit </strong></a></span><strong>(July 9)</strong></p>
<h1>
	<strong>What is covered in the 'Building a Big Data Platform on AWS' bootcamp?</strong></h1>
<p>
	Building a Big Data Platform on AWS covers the following:</p>
<ul>
	<li>
		Efficient storage and handling of large datasets with Amazon Simple Storage Service (S3)&nbsp;</li>
	<li>
		Batch data processing with Apache Hadoop and Hive running on EMR</li>
	<li>
		Interactive querying for large datasets with Presto and Spark running on EMR</li>
	<li>
		Streaming data collection and real-time analysis with Amazon Kinesis and Spark Streaming</li>
	<li>
		Best practices for deploying, data modeling, data distribution and querying with Amazon Redshift&nbsp;</li>
</ul>
<p>
	The hands-on exercises start simple. For example, you'll learn how to efficiently copy and aggregate data for batch processing with EMR and&nbsp; S3. They gradually get more complex as the day progresses, including advanced Amazon Redshift query and table optimization and code for real-time stream processing at scale. &nbsp;</p>
<p>
	We can’t possibly cover all AWS big data services in a day, so we are hard at work on new, complementary bootcamps that focus on Amazon DynamoDB, Amazon Elastic Countainer Service/Docker, Amazon Machine Learning, Amazon ElasticSearch (SOLR/CloudSearch) and more.</p>
<h1>
	<strong>Sample exercises from the bootcamp</strong></h1>
<p>
	There are currently seven hands-on exercises in the Building a Big Data Platform on AWS bootcamp.&nbsp; Below are two partial examples.&nbsp; The first shows you how to process data in a Kinesis stream with the Spark Streaming framework running on EMR.&nbsp; In the second, we explore query optimization with Amazon Redshift.</p>
<h2>
	Processing data in your Amazon Kinesis Stream with Spark</h2>
<p>
	Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput and fault-tolerant processing of data streams.&nbsp;When Spark Streaming receives data it divides it into batches. It then uses the Spark engine for processing and result generation. You can run your Spark Streaming cluster in several modes: local, standalone, or with cluster management frameworks such as Mesos and YARN.&nbsp; In the AWS Building a Big Data Platform on AWS bootcamp, we use EMR clusters running Hadoop 2 and YARN to run Spark Streaming.&nbsp; This approach allows us to experiment with multiple applications on a single cluster, including Hive, Presto, Spark and more.&nbsp;</p>
<p>
	This exercise shows you how to use Spark Streaming to extract information from incoming clickstream data.&nbsp; Specifically, we extract “HTTP referrer” (what website a visitor to your website was on before landing on yours).&nbsp;</p>
<p>
	We assume that you are using an Amazon EC2 instance with the AWS CLI installed and with an associated role granting your instance access to EMR, EC2 and Amazon Kinesis.&nbsp; We also default to the us-west-2 AWS Region:</p>
<ol>
	<li>
		Start a three-node EMR cluster with Hive and Spark installed.&nbsp; Replace YOUR-KEYPAIR with the AWS key pair (PEM file) that you will use to SSH into the EMR cluster:</li>
</ol>
<p style="margin-left: 40px;">
	aws emr create-default-roles --region us-west-2<br />
	aws emr create-cluster --no-termination-protected \<br />
	--no-enable-debugging --no-auto-terminate --use-default-role \<br />
	--name=bootcamp --ami-version 3.6 \<br />
	--ec2-attributes KeyName=YOUR-KEY \<br />
	--instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m3.xlarge InstanceGroupType=CORE,InstanceCount=2,InstanceType=m3.xlarge \<br />
	--applications Name=Hive \<br />
	--bootstrap-actions Path=s3://support.elasticmapreduce/spark/install-spark,Args=-v1.3.0.d \<br />
	--region us-west-2</p>
<ol start="2">
	<li>
		Create an Amazon Kinesis stream to hold the inbound streaming data with two shards:</li>
</ol>
<p style="margin-left: 40px;">
	aws kinesis create-stream --stream-name bootcamp --shard-count 2 \<br />
	--region us-west-2</p>
<ol start="3">
	<li>
		Download the sample application from GitHub and set your region:</li>
</ol>
<p style="margin-left: 40px;">
	wget https://github.com/awslabs/amazon-kinesis-data-visualization-sample/releases/download/v1.1.2/amazon-kinesis-data-visualization-sample-1.1.2-assembly.zip<br />
	<br />
	unzip amazon-kinesis-data-visualization-sample-1.1.2-assembly.zip \<br />
	-d kinesis-data-vis-sample-app</p>
<ol start="4">
	<li>
		Start an application that sends randomly generated records to the Amazon Kinesis stream that we created above.&nbsp; Leave it running while you complete the following steps.</li>
</ol>
<p style="margin-left: 40px;">
	java -cp &quot;kinesis-data-vis-sample-app/lib/*&quot;&nbsp; com.amazonaws.services.kinesis.samples.datavis.HttpReferrerStreamWriter 5 bootcamp us-west-2</p>
<ol start="5">
	<li>
		Check that the IAM Role assigned to the instances in your EMR cluster have permission to read and write Amazon Kinesis Streams:
		<ol style="list-style-type:lower-alpha;">
			<li>
				Go to <a href="https://console.aws.amazon.com/iam/home?region=us-west-2#roles/EMR_EC2_DefaultRole">https://console.aws.amazon.com/iam/home?region=us-west-2#roles/EMR_EC2_DefaultRole</a></li>
			<li>
				Under Permissions, click on “Attach Policy.”</li>
			<li>
				&nbsp;Search for “kinesis” and tick the box beside “AmazonKinesisFullAccess” then click the “Attach Policy” button.</li>
		</ol>
	</li>
</ol>
<ol start="6">
	<li>
		Create a temporary S3 bucket with a unique name of your choice:</li>
</ol>
<p style="margin-left: 40px;">
	aws s3 mb s3://<strong><em>YOUR-S3-BUCKET</em></strong> --region us-west-2</p>
<ol start="7">
	<li>
		&nbsp;SSH into your EMR cluster and start the spark-shell:</li>
</ol>
<p style="margin-left: 40px;">
	spark/bin/spark-shell \<br />
	--jars spark/lib/spark-streaming-kinesis-asl_2.10-1.3.0.jar</p>
<ol start="8">
	<li>
		Setup the Spark Shell for your streaming application.&nbsp;&nbsp;Replace&nbsp;<strong>YOUR-S3-BUCKET</strong>&nbsp;with the bucket name that you just created.</li>
</ol>
<p style="margin-left: 40px;">
	/* import dependencies */<br />
	import org.apache.spark.SparkContext<br />
	import org.apache.spark.storage.StorageLevel<br />
	import org.apache.spark.streaming.Seconds<br />
	import org.apache.spark.streaming.StreamingContext<br />
	import org.apache.spark.streaming.StreamingContext.toPairDStreamFunctions<br />
	import org.apache.spark.streaming.kinesis.KinesisUtils<br />
	import com.amazonaws.auth.DefaultAWSCredentialsProviderChain<br />
	import com.amazonaws.services.kinesis.AmazonKinesisClient<br />
	import com.amazonaws.services.kinesis.clientlibrary.lib.worker._<br />
	import java.util.Date<br />
	import org.apache.hadoop.io.compress._<br />
	import scala.util.parsing.json.JSON<br />
	import org.apache.log4j.Logger<br />
	import org.apache.log4j.Level</p>
<p style="margin-left: 40px;">
	/* disable logging */<br />
	Logger.getLogger(&quot;org&quot;).setLevel(Level.OFF)<br />
	Logger.getLogger(&quot;akka&quot;).setLevel(Level.OFF)</p>
<p style="margin-left: 40px;">
	/* Set up the variables as needed */<br />
	val streamName = &quot;bootcamp&quot;<br />
	val endpointUrl = &quot;https://kinesis.us-west-2.amazonaws.com&quot;<br />
	val outputDir = &quot;s3://<strong><em>YOUR-S3-BUCKET</em></strong>/raw-stream&quot;<br />
	val outputBatchInterval = Seconds(60)<br />
	val inputBatchInterval = Seconds(2)</p>
<p style="margin-left: 40px;">
	/* Reconfigure the spark-shell */<br />
	val sparkConf = sc.getConf<br />
	sparkConf.setAppName(&quot;KinesisReferrerCount&quot;)<br />
	sparkConf.remove(&quot;spark.driver.extraClassPath&quot;)<br />
	sparkConf.remove(&quot;spark.executor.extraClassPath&quot;)</p>
<p style="margin-left: 40px;">
	sc.stop()<br />
	val sc = new SparkContext(sparkConf)</p>
<ol start="9">
	<li>
		Create a worker for each shard in your Amazon Kinesis stream. Each worker will create a Spark Streaming Discretized Stream (DStream; a continuous sequent of <a href="https://spark.apache.org/docs/1.3.1/api/java/org/apache/spark/rdd/RDD.html">RDDs</a>) which we will eventually merge into a single DStream.</li>
</ol>
<p style="margin-left: 40px;">
	/* Setup the KinesisClient */<br />
	val kinesisClient = new AmazonKinesisClient(new DefaultAWSCredentialsProviderChain())<br />
	kinesisClient.setEndpoint(endpointUrl)</p>
<p style="margin-left: 40px;">
	/* Determine the number of shards from the stream */<br />
	val numShards = kinesisClient.describeStream(streamName).getStreamDescription().getShards().size()</p>
<p style="margin-left: 40px;">
	/* Create a streaming context and then create one worker per shard */<br />
	val ssc = new StreamingContext(sc, inputBatchInterval)<br />
	val kinesisStreams = (0 until numShards).map { i =&gt;<br />
	&nbsp; KinesisUtils.createStream(ssc, streamName, endpointUrl,<br />
	&nbsp;&nbsp;&nbsp; inputBatchInterval,InitialPositionInStream.LATEST,<br />
	&nbsp;&nbsp;&nbsp; StorageLevel.MEMORY_ONLY)<br />
	}</p>
<p style="margin-left: 40px;">
	/* Merge the worker Dstreams and translate the byteArray to string */<br />
	val unionStreams = ssc.union(kinesisStreams)<br />
	val accessLogs = unionStreams.flatMap(byteArray =&gt; new String(byteArray).split(&quot; &quot;))</p>
<ol start="10">
	<li>
		With the records now available in the accessLogs Dstream, we can now translate the string into a JSON object and extract the HTTP referrer field. We will group the dataset by referrer, using the reduceByKey operation on every RDD in the DStream:</li>
</ol>
<p style="margin-left: 40px;">
	val jsonFields = accessLogs.map(JSON.parseFull(_)).map(_.get.asInstanceOf[scala.collection.immutable.Map[String,Any]]) val referrerCount = jsonFields.map(data =&gt; data(&quot;referrer&quot;).toString).map(word =&gt; ( word.split('.')(1), 1)).reduceByKey(_ + _) referrerCount.print()</p>
<ol start="11">
	<li>
		We can also derive a second DStream from the accessLogs Dstream by applying the windowing function over a sliding interval of outputBatchInterval.&nbsp; We will store the resulting output in S3:</li>
</ol>
<p style="margin-left: 40px;">
	val batchLogs = accessLogs.window(outputBatchInterval,outputBatchInterval)<br />
	batchLogs.foreachRDD( (rdd,time) =&gt; {<br />
	&nbsp; if (rdd.count &gt; 0) {<br />
	&nbsp;&nbsp;&nbsp; val outPartitionFolder = new java.text.SimpleDateFormat(&quot;yyyy/MM/dd/HH/mm&quot;).format(new Date(time.milliseconds))<br />
	&nbsp;&nbsp;&nbsp; rdd.coalesce(1).saveAsTextFile(&quot;%s/%s&quot;.format(outputDir, outPartitionFolder)+&quot;/logs_&quot;+time.milliseconds.toString,classOf[GzipCodec])<br />
	}})</p>
<ol start="12">
	<li>
		Now that setup is complete, start the Spark Streaming context and&nbsp;leave the application running for 2-3 minutes to see the results.&nbsp; When you’re done, hit Control-C to quit:</li>
</ol>
<p style="margin-left: 40px;">
	ssc.start()<br />
	ssc.awaitTermination()</p>
<ol start="13">
	<li>
		After terminating the above program, check your S3 bucket. You should see compressed files in your S3 bucket under&nbsp;<strong>“raw-stream/year/month/day/hour/minute/logs_timestamp.”</strong></li>
</ol>
<h2>
	Using the Explain Plan to debug long-running queries in Amazon Redshift</h2>
<p>
	The Explain Plan shows the logical steps that Amazon Redshift will perform for the query. The corresponding EXPLAIN command does not actually run the query; the output contains only the plan that Amazon Redshift will execute if the query is run under current operating conditions. If you change the schema of a table in some way or if you change the data in the table and run ANALYZE again to update the statistical metadata, the explain plan might be different.</p>
<p>
	Reading the explain plan from the bottom up, you can see a breakdown of logical operations needed to perform the query as well as an indication of their relative cost and the amount of data that needs to be processed. By analyzing the plan, you can often identify opportunities to improve query performance.</p>
<p>
	To illustrate this, let’s dig into a straightforward example of how to optimize joins:</p>
<ol>
	<li>
		Start a single-node Amazon Redshift cluster:</li>
</ol>
<p style="margin-left: 40px;">
	aws redshift create-cluster --db-name bootcamp --node-type dw2.large \<br />
	--region us-west-2 --cluster-type single-node \<br />
	--master-username master --master-user-password Redshift123 \<br />
	--cluster-identifier bootcamp --port 8192</p>
<ol start="2">
	<li>
		Connect to your Amazon Redshift cluster.&nbsp; (Instructions here: <a href="https://docs.aws.amazon.com/redshift/latest/gsg/rs-gsg-connect-to-cluster.html">https://docs.aws.amazon.com/redshift/latest/gsg/rs-gsg-connect-to-cluster.html</a>)</li>
</ol>
<ol start="3">
	<li>
		Create tables to hold our sample data:</li>
</ol>
<p style="margin-left: 40px;">
	CREATE TABLE cloudfront ( logtime TIMESTAMP, edge VARCHAR(40), bytes INT, client_ip VARCHAR(50), http_method VARCHAR(50), cfhost VARCHAR(512),&nbsp; uri VARCHAR(2048), file_name VARCHAR(1024), content_type VARCHAR(256), user_id INT, order_id INT, status INT, referer VARCHAR(4096), referer_host VARCHAR(256), referer_protocol VARCHAR(16)) DISTKEY(user_id) SORTKEY(logtime);</p>
<p style="margin-left: 40px;">
	CREATE TABLE user_profile ( user_id INT, age INT, gender char, city VARCHAR(256), country VARCHAR(256));</p>
<p style="margin-left: 40px;">
	CREATE TABLE games ( game_id INT, gamename VARCHAR(256), price DECIMAL);</p>
<p style="margin-left: 40px;">
	CREATE TABLE order_line ( customer_id INT, order_id INT, description VARCHAR(256), product_id INT, unitprice DECIMAL, quantity INT, extended_price DECIMAL, line_tax DECIMAL);</p>
<ol start="4">
	<li>
		Load sample data into the tables, replacing YOUR-ACCESS-KEY and YOUR-SECRET-KEY with AWS IAM credentials from your AWS account corresponding to an AWS IAM user that has access to S3:</li>
</ol>
<p style="margin-left: 40px;">
	COPY cloudfront FROM 's3://aws-bigdata-bootcamp/data/logs-processed/' CREDENTIALS 'aws_access_key_id=YOUR-ACCESS-KEY;aws_secret_access_key=YOUR-SECRET-KEY' DELIMITER '\t' IGNOREHEADER 0 MAXERROR 0 GZIP;</p>
<p style="margin-left: 40px;">
	COPY user_profile FROM 's3://aws-bigdata-bootcamp/data/users.lzo' CREDENTIALS 'aws_access_key_id=YOUR-ACCESS-KEY;aws_secret_access_key=YOUR-SECRET-KEY' DELIMITER ',' IGNOREHEADER 0 MAXERROR 0 LZOP;</p>
<p style="margin-left: 40px;">
	COPY order_line FROM 's3://aws-bigdata-bootcamp/data/orders/' CREDENTIALS 'aws_access_key_id=YOUR-ACCESS-KEY;aws_secret_access_key=YOUR-SECRET-KEY' DELIMITER '|' IGNOREHEADER 0 MAXERROR 0 GZIP;</p>
<p style="margin-left: 40px;">
	COPY games FROM 's3://aws-bigdata-bootcamp/data/gameList.csv' CREDENTIALS 'aws_access_key_id=YOUR-ACCESS-KEY;aws_secret_access_key=YOUR-SECRET-KEY' DELIMITER ',' IGNOREHEADER 0 MAXERROR 0;</p>
<ol start="5">
	<li>
		Use the EXPLAIN command below to determine how an inefficient query can be rewritten to perform better. &nbsp;&nbsp;The result includes a warning about an inefficient Nested Loop, which has a very high row count and very high relative cost. Nested Loop is the least-optimal join in Amazon Redshift and should be avoided in most cases. It is used mainly for cross-joins (Cartesian products) and some inequality joins. A by-product of the Nested Loop in the query above is the very high number of rows that must be evaluated before this query can aggregate the results. This is because the query is causing Amazon Redshift do a nested loop that scans over 4 million rows, 670 million times!</li>
</ol>
<p style="margin-left: 40px;">
	EXPLAIN SELECT u.gender, g.gamename, count(*) AS purchases<br />
	FROM user_profile AS u, games AS g, order_line AS ol<br />
	GROUP BY u.gender, g.gamename<br />
	ORDER BY purchases DESC;</p>
<ol start="6">
	<li>
		Re-write the query, using JOINs. The WHERE or INNER JOIN clauses allow Amazon Redshift to perform Hash Joins and Hashes instead of Nested Loops. In the Explain Plan you will see a sharp decline in the number of rows that will need to be accessed as compared to the Nested Loop example above. You will also see a “Hash” operator. This creates the hash table for the inner table in the join by reading the outer table, hashing the joining column, and finding matches in the inner hash table. Hash Joins are typically faster than a Nested Loop join. They are used for inner joins and left and right outer joins. This operator will generally be used when you are joining tables if your joining columns for both tables are not both distribution keys and sort keys</li>
</ol>
<p style="margin-left: 40px;">
	EXPLAIN SELECT u.gender, g.gamename, count(*) AS purchases<br />
	FROM user_profile AS u<br />
	<strong>JOIN order_line AS ol ON ol.customer_id = u.user_id<br />
	JOIN games AS g ON g.game_id = ol.product_id</strong><br />
	GROUP BY u.gender, g.gamename<br />
	ORDER BY purchases DESC;</p>
<ol start="7">
	<li>
		Next, run the new query without EXPLAIN to see if it improves performance. Unlike the never-ending Nested Loop query in the first example, this second query should finish in &lt;5 seconds:</li>
</ol>
<p style="margin-left: 40px;">
	SELECT u.gender, g.gamename, count(*) AS purchases<br />
	FROM user_profile AS u<br />
	JOIN order_line AS ol ON ol.customer_id = u.user_id<br />
	JOIN games AS g ON g.game_id = ol.product_id<br />
	GROUP BY u.gender, g.gamename<br />
	ORDER BY purchases DESC;</p>
<ol start="8">
	<li>
		Run the same query one more time. It should perform even faster the second time.&nbsp; Why? Amazon Redshift’s execution engine assembles a sequence of steps, segments, and streams to execute the query plan supplied by the optimizer. It then generates and compiles C++ code, and sends the compiled code segments to the compute nodes. The compiled code executes much faster because it eliminates the overhead of using an interpreter, but there is always some overhead cost the first time the code is generated and compiled, even for the cheapest query plans. As a result, the performance of a query the first time you run it can be misleading. The compiled code is cached and shared across sessions in a cluster, so subsequent executions of the same query will run faster, even with different query parameters and in different sessions, because they can skip the initial generation and compilation steps.</li>
</ol>
<h1>
	<strong>Conclusion</strong></h1>
<p>
	Hopefully the description and examples above gave you a good taste of the Building a Big Data Platform on AWS.&nbsp; We encourage you to sign up for this&nbsp; bootcamp and others at the AWS Summits, AWS Pop-up Loft and AWS re:Invent.&nbsp; We are also always looking for ideas for new full-day training sessions, so please leave any ideas in the comments.</p>
<p>
	<a href="http://aws.amazon.com/summits/chicago/" target="_blank"><strong>Register for the Chicago Summit </strong></a>(July 1)</p>
<p>
	<a href="http://aws.amazon.com/summits/new-york/" target="_top"><strong>Register for the New York Summit</strong></a> (July 9)</p>
<p>
	&nbsp;</p></div>
  </div>

  <div class="post-footer-block">
  <div class="fl-r">
    <ul class="share-buttons navigation inline-navigation">
      <li class="quiet">
        Share</li>
      <li>
        <span class="social-media email share-post">
<a href="/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx11LQAXCMNYZOA" class="share-url icon email-icon" onclick="window.open('/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx11LQAXCMNYZOA', '_blank', 'location=yes,width=700,height=600');return false;" title="Share via Email"></a>
</span>
</li>
      <li>            
        <span class="social-media facebook share-post">
<a href="https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx11LQAXCMNYZOA%2FTest-drive-two-big-data-scenarios-from-the-Building-a-Big-Data-Platform-on-AWS-b&t=Test%20drive%20two%20big%20data%20scenarios%20from%20the%20%27Building%20a%20Big%20Data%20Platform%20on%20AWS%27%20bootcamp" class="share-url icon facebook-icon" onclick="window.open('https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx11LQAXCMNYZOA%2FTest-drive-two-big-data-scenarios-from-the-Building-a-Big-Data-Platform-on-AWS-b&t=Test%20drive%20two%20big%20data%20scenarios%20from%20the%20%27Building%20a%20Big%20Data%20Platform%20on%20AWS%27%20bootcamp', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Facebook"></a>
</span>
</li>
      <li>
        <span class="social-media twitter share-post">
<a href="https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx11LQAXCMNYZOA%2FTest-drive-two-big-data-scenarios-from-the-Building-a-Big-Data-Platform-on-AWS-b&via=awscloud&text=Test+drive+two+big+data+scenarios+from+the+%27Building+a+Big+Data+Platform+on+AWS%27+bootcamp&count=none" class="share-url icon twitter-icon" onclick="window.open('https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx11LQAXCMNYZOA%2FTest-drive-two-big-data-scenarios-from-the-Building-a-Big-Data-Platform-on-AWS-b&via=awscloud&text=Test+drive+two+big+data+scenarios+from+the+%27Building+a+Big+Data+Platform+on+AWS%27+bootcamp&count=none', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Twitter"></a>
</span></li>
      </ul>
  </div>
<div>
  <ul class="post-attributes navigation inline-navigation blog-ui-piped push-bottom-1">
    <li>
      <a href="/bigdata/post/Tx11LQAXCMNYZOA/Test-drive-two-big-data-scenarios-from-the-Building-a-Big-Data-Platform-on-AWS-b#" rel="bookmark">
        <time datetime="2015-06-04T20:19:57.836Z" title="2015-06-04T20:19:57.836Z" class="post-creation-date">June 4, 2015</time></a>
    </li>
    <li>
          <a href="/bigdata/post/Tx11LQAXCMNYZOA/Test-drive-two-big-data-scenarios-from-the-Building-a-Big-Data-Platform-on-AWS-b#" rel="bookmark">Permalink</a>
        </li>
    <li>
          <a href="/bigdata/post/Tx11LQAXCMNYZOA/Test-drive-two-big-data-scenarios-from-the-Building-a-Big-Data-Platform-on-AWS-b#postCommentsTx11LQAXCMNYZOA">Comments (<span id="commentNumber">3</span>)</a>
        </li>
    </ul>
</div></div></article>
</li>
          <li class="blog-post">
              <article
  class="post display published no-comments " 
  data-postid="TxC0CXZ3RPPK7O">
  
  <div class="post-heading-block clearfix">

  <div class="post-author">
      
<div class="author avatar author-avatar blog-ui-avatar blog-ui-author-avatar" data-content-id="damle">
  <a href="/bigdata/blog/author/Hernan+Vivani" title="Hernan Vivani"><img class="author-image" alt="Hernan Vivani" src="//cdn.amazonblogs.com/bigdata_awsblog/authors/Hernan photo-resize.fw_1422661501922.png"></a>
</div></div>
  <div class="post-heading">

    <h1 class="post-title navigation">
      <a href="/bigdata/post/TxC0CXZ3RPPK7O/Indexing-Common-Crawl-Metadata-on-Amazon-EMR-Using-Cascading-and-Elasticsearch" rel="bookmark">
        Indexing Common Crawl Metadata on Amazon EMR Using Cascading and Elasticsearch</a>
    </h1>

    <ul class="post-attributes navigation inline-navigation blog-ui-piped">
      <li>
        <a href="/bigdata/post/TxC0CXZ3RPPK7O/Indexing-Common-Crawl-Metadata-on-Amazon-EMR-Using-Cascading-and-Elasticsearch#" rel="bookmark">
          <time datetime="2015-05-28T15:53:48.533Z" title="2015-05-28T15:53:48.533Z" class="post-creation-date">May 28, 2015</time></a>
      </li>
      <li class="last">      
        <address class="by-line">
          <a rel="nofollow" data-contentid="damle" class="author-name" itemprop="author" href="/bigdata/blog/author/Hernan+Vivani">Hernan Vivani</a>
</address>
      </li>    
    </ul>
    
    <ul class="category-list post-categories navigation inline-navigation blog-ui-piped">
  </ul><ul class="tag-list post-tags navigation inline-navigation blog-ui-piped">
  </ul></div>      
</div><div class="post-content-block">
    <div class="post-text"><p>
	<em>Hernan Vivani <em>is a Big Data Support Engineer for Amazon Web Services</em></em></p>
<p>
	A previous post showed you how to <a href="http://blogs.aws.amazon.com/bigdata/post/Tx1E8WC98K4TB7T/Getting-Started-with-Elasticsearch-and-Kibana-on-Amazon-EMR" target="_blank">get started with Elasticsearch and Kibana on Amazon EMR</a>. In that post, we installed Elasticsearch and Kibana on an <a href="https://aws.amazon.com/emr" target="_blank">Amazon EMR</a> cluster using bootstrap actions.</p>
<p>
	This post shows you how to build a simple application with Cascading for reading <a href="http://commoncrawl.org/the-data/" target="_blank">Common Crawl</a> metadata, index the metadata on Elasticsearch, and use Kibana to query the indexed content.</p>
<h1>
	What is Common Crawl?</h1>
<p>
	Common Crawl is an open-source repository of web crawl data. This data set is freely available on Amazon S3 under the Common Crawl terms of use. The data is stored in several data formats. In this example, you work with the <a href="http://commoncrawl.org/the-data/get-started/" target="_blank">WAT response format</a> that contains the metadata for the crawled HTML information. This allows you to build an Elasticsearch index, which can be used to extract useful information about tons of sites on the Internet.</p>
<h1>
	What is Cascading?</h1>
<p>
	<a href="http://www.cascading.org/projects/cascading/" target="_blank">Cascading</a> is an application development platform for building data applications on Apache Hadoop. In this post, you use it to build a simple application that indexes JSON files in Elasticsearch, without the need to think in terms of MapReduce methods.</p>
<h1>
	Launching an EMR cluster with Elasticsearch, Maven, and Kibana</h1>
<p>
	As in the previous post, you launch a cluster with Elasticsearch and Kibana installed. You also install Maven to compile the application and run a script to resolve some library dependencies between Elasticsearch and Cascading. All the bootstrap actions are public, so you can download the code to verify the installation steps at any time.</p>
<p>
	To launch the cluster, use the <a href="https://aws.amazon.com/cli" target="_blank">AWS CLI</a> and run the following command:</p>
<pre>
aws emr create-cluster --name Elasticsearch --ami-version 3.3.2 \
--instance-type=m1.medium --instance-count 3 \
--ec2-attributes KeyName=your-key \
--log-uri s3://your-bucket/logs/ \
--bootstrap-action Name=&quot;Setup Jars&quot;,Path= s3://support.elasticmapreduce/bootstrap-actions/other/cascading-elasticsearch-jar-classpath.sh \
Name=&quot;Install Maven&quot;,Path=s3://support.elasticmapreduce/bootstrap-actions/other/maven-install.sh \
Name=&quot;Install Elasticsearch&quot;,Path=s3://support.elasticmapreduce/bootstrap-actions/other/elasticsearch_install.rb \
Name=&quot;Install Kibana&quot;,Path=s3://support.elasticmapreduce/bootstrap-actions/other/kibananginx_install.rb \
--no-auto-terminate
</pre>
<h1>
	Compiling Cascading Source Code with Maven</h1>
<p>
	After you have the cluster up and running, you can <a href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-connect-master-node-ssh.html" target="_blank">connect using SSH into the master node</a> to compile and run the application. Your Cascading application applies a filter before you start the indexing process, to remove the WARC envelope and obtain plain JSON output. For more information about the code, see the <a href="https://github.com/awslabs/aws-big-data-blog/tree/master/aws-blog-elasticsearch-cascading-commoncrawl/commoncrawl.cascading.elasticsearch" target="_blank">Github repository</a>.</p>
<p>
	<strong>Install git:</strong></p>
<pre>
$ sudo yum install git
</pre>
<p>
	<strong>Clone the repository:</strong></p>
<pre>
$ git clone https://github.com/awslabs/aws-big-data-blog.git
</pre>
<p>
	<strong>Compile the code:</strong></p>
<pre>
$ cd aws-big-data-blog/aws-blog-elasticsearch-cascading-commoncrawl/commoncrawl.cascading.elasticsearch
$ mvn clean &amp;&amp; mvn assembly:assembly -Dmaven.test.skip=true  -Ddescriptor=./src/main/assembly/job.xml -e
</pre>
<p>
	Compiled application is placed in the following directory: aws-big-data-blog/aws-blog-elasticsearch-cascading-commoncrawl/commoncrawl.cascading.elasticsearch/target</p>
<p>
	Listing the directory should show the packaged application, as shown in the following graphic:</p>
<p>
	<img alt="" height="165" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/elasticsearch-cascading-compiled-code-new.png" width="899" /></p>
<h1>
	Indexing Common Crawl Metadata on Elasticsearch</h1>
<p>
	Using the application you just compiled, you can index a single Common Crawl file or a complete directory, by modifying the parameter. The following commands show you how to index a file or directory.</p>
<p>
	<strong>Index a single file:</strong></p>
<pre>
hadoop jar /home/hadoop/aws-big-data-blog/aws-blog-elasticsearch-cascading-commoncrawl/commoncrawl.cascading.elasticsearch/target/commoncrawl.cascading.elasticsearch-0.0.1-SNAPSHOT-job.jar com.amazonaws.bigdatablog.indexcommoncrawl.Main s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2014-52/segments/1419447563504.69/wat/CC-MAIN-20141224185923-00099-ip-10-231-17-201.ec2.internal.warc.wat.gz
</pre>
<p>
	<strong>Index a complete directory:</strong></p>
<pre>
hadoop jar /home/hadoop/aws-big-data-blog/aws-blog-elasticsearch-cascading-commoncrawl/commoncrawl.cascading.elasticsearch/target/commoncrawl.cascading.elasticsearch-0.0.1-SNAPSHOT-job.jar com.amazonaws.bigdatablog.indexcommoncrawl.Main s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2014-52/segments/1419447563504.69/wat/
</pre>
<p>
	Running the command to index a single file produces the following output:</p>
<p>
	<img alt="Running the command to index a single file produces this output" height="838" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Elasticsearch2_Image%202a.png" width="1087" /></p>
<p>
	The application writes each JSON entry directly into Elasticsearch using the Cascading and Hadoop connectors.</p>
<h1>
	Checking Indexes and Mappings</h1>
<p>
	The index on Elasticsearch is created automatically, using the default configuration. Now, run a couple of commands on the console to check the index and mappings.</p>
<p>
	<strong>List all indexes:</strong></p>
<pre>
$ curl 'localhost:9200/_cat/indices?v'</pre>
<p>
	<img alt="Listing indexes" height="49" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Elasticsearch2_Image%203a.png" width="643" /></p>
<p>
	<strong>View the mappings:</strong></p>
<pre>
curl -XGET 'http://localhost:9200/_all/_mapping' | python -m json.tool |more</pre>
<p>
	If you look at the mapping output, you’ll see that it follows the structure showed on the Common Crawl WAT metadata description: <a href="http://commoncrawl.org/the-data/get-started/" target="_blank">http://commoncrawl.org/the-data/get-started/</a>.</p>
<p>
	<img alt="Viewing mappings" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Elasticsearch2_Image%204.png" style="width: 620px; height: 831px;" /></p>
<p>
	This mapping is shown in the Kibana menu and allows you to navigate the different metadata entries.</p>
<h1>
	Querying Indexed Content</h1>
<p>
	Because the Kibana bootstrap action configures the cluster to use port 80, you can point the browser to the master node public DNS address to access the Kibana console. On the Kibana console, click <strong>Sample Dashboard</strong> to start exploring the content indexed earlier in this post.</p>
<p>
	<img alt="" height="318" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Elasticsearch2_Image%205a.png" width="643" /></p>
<p>
	A sample dasbhard appears with some basic information extracted:</p>
<p>
	<img alt="Querying Indexed Content" height="473" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Elasticsearch2_Image%206.png" width="573" /></p>
<p>
	You can search Head.Metas headers for all the occurrences of “hello”; in the search box, type “HTML-Metadata.Head.Metas AND keywords AND hello”.</p>
<p>
	<img alt="Searching for headers" height="407" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Elasticsearch2_Image%207a.png" width="586" /></p>
<p>
	That search returns all the records that contain ‘keywords’ and ‘hello’ on the “Metadata.Head.Metas” header. The result looks like the following:</p>
<p>
	<img alt="Search results" height="58" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Elasticsearch2_Image%208a.png" width="576" /></p>
<p>
	Another useful way to find information is by using the mapping index. You can click “Envelope.Payload-Metadata.HTTP-Response-Metadata.Headers.Server” to see a ranking of the different server technologies of all the indexed sites:</p>
<p>
	<img alt="using the mapping index" height="319" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Elasticsearch2_Image%2010.png" width="622" /></p>
<p>
	Click the magnifier icon to find all the details on the selected entry.</p>
<p>
	Or you can get the top ten technologies used in the indexed web application by clicking “Envelope.Payload-Metadata.HTTP-Response-Metadata.Headers.X-Powered-By”. The following graphic shows an example:</p>
<p>
	<img alt="Getting the top ten technologies used in the indexed web application" height="399" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Elasticsearch2_Image%2011.png" width="619" /></p>
<h1>
	Conclusion</h1>
<p>
	This post has shown how EMR lets you build and compile a simple Cascading application and use it to index Common Crawl metadata on an Elasticsearch cluster.</p>
<p>
	Cascading provided a simple application layer on top of Hadoop to parallelize the process and fetch the data directly from the S3 repository location, while Kibana provided a presentation interface that allowed you to research the indexed data in many ways.</p>
<p>
	If you have questions or suggestions, please leave a comment below.</p>
<p>
	------------------------------------</p>
<p>
	<span style="font-size:14px;"><em><strong>Do more with EMR:</strong></em></span></p>
<p>
	<strong><a href="http://blogs.aws.amazon.com/bigdata/post/TxX4BY5T1PQ7BQ/Using-IPython-Notebook-to-Analyze-Data-with-Amazon-EMR" target="_blank">Using IPython Notebook to Analyze Data with EMR</a></strong></p>
<p>
	<img alt="" height="128" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Social%20media%20image.PNG" width="214" /></p>
<p>
	---------------------------------------------------------------</p>
<p>
	<span style="font-size:14px;"><em><strong>Love to work on open source? Check out EMR's <a href="http://www.amazon.jobs/results?jobCategoryIds[]=83&amp;jobCategoryIds[]=70&amp;businessCategoryIds[]=1&amp;locationCities[]=US%2C%20CA%2C%20Palo%20Alto&amp;locationCities[]=US%2C%20WA%2C%20Seattle&amp;searchStrings[]=EMR">careers page</a>.</strong></em></span></p>
<p>
	----------------------------------------------------------------</p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p></div>
  </div>

  <div class="post-footer-block">
  <div class="fl-r">
    <ul class="share-buttons navigation inline-navigation">
      <li class="quiet">
        Share</li>
      <li>
        <span class="social-media email share-post">
<a href="/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=TxC0CXZ3RPPK7O" class="share-url icon email-icon" onclick="window.open('/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=TxC0CXZ3RPPK7O', '_blank', 'location=yes,width=700,height=600');return false;" title="Share via Email"></a>
</span>
</li>
      <li>            
        <span class="social-media facebook share-post">
<a href="https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTxC0CXZ3RPPK7O%2FIndexing-Common-Crawl-Metadata-on-Amazon-EMR-Using-Cascading-and-Elasticsearch&t=Indexing%20Common%20Crawl%20Metadata%20on%20Amazon%20EMR%20Using%20Cascading%20and%20Elasticsearch" class="share-url icon facebook-icon" onclick="window.open('https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTxC0CXZ3RPPK7O%2FIndexing-Common-Crawl-Metadata-on-Amazon-EMR-Using-Cascading-and-Elasticsearch&t=Indexing%20Common%20Crawl%20Metadata%20on%20Amazon%20EMR%20Using%20Cascading%20and%20Elasticsearch', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Facebook"></a>
</span>
</li>
      <li>
        <span class="social-media twitter share-post">
<a href="https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTxC0CXZ3RPPK7O%2FIndexing-Common-Crawl-Metadata-on-Amazon-EMR-Using-Cascading-and-Elasticsearch&via=awscloud&text=Indexing+Common+Crawl+Metadata+on+Amazon+EMR+Using+Cascading+and+Elasticsearch&count=none" class="share-url icon twitter-icon" onclick="window.open('https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTxC0CXZ3RPPK7O%2FIndexing-Common-Crawl-Metadata-on-Amazon-EMR-Using-Cascading-and-Elasticsearch&via=awscloud&text=Indexing+Common+Crawl+Metadata+on+Amazon+EMR+Using+Cascading+and+Elasticsearch&count=none', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Twitter"></a>
</span></li>
      </ul>
  </div>
<div>
  <ul class="post-attributes navigation inline-navigation blog-ui-piped push-bottom-1">
    <li>
      <a href="/bigdata/post/TxC0CXZ3RPPK7O/Indexing-Common-Crawl-Metadata-on-Amazon-EMR-Using-Cascading-and-Elasticsearch#" rel="bookmark">
        <time datetime="2015-05-28T15:53:48.533Z" title="2015-05-28T15:53:48.533Z" class="post-creation-date">May 28, 2015</time></a>
    </li>
    <li>
          <a href="/bigdata/post/TxC0CXZ3RPPK7O/Indexing-Common-Crawl-Metadata-on-Amazon-EMR-Using-Cascading-and-Elasticsearch#" rel="bookmark">Permalink</a>
        </li>
    <li>
          <a href="/bigdata/post/TxC0CXZ3RPPK7O/Indexing-Common-Crawl-Metadata-on-Amazon-EMR-Using-Cascading-and-Elasticsearch#postCommentsTxC0CXZ3RPPK7O">Comments (<span id="commentNumber">0</span>)</a>
        </li>
    </ul>
</div></div></article>
</li>
          <li class="blog-post">
              <article
  class="post display published has-comments " 
  data-postid="Tx2LQ4WAWOP80EG">
  
  <div class="post-heading-block clearfix">

  <div class="post-author">
      
<div class="author avatar author-avatar blog-ui-avatar blog-ui-author-avatar" data-content-id="gernest">
  <a href="/bigdata/blog/author/Guy+Ernest" title="Guy Ernest"><img class="author-image" alt="Guy Ernest" src="//cdn.amazonblogs.com/bigdata_awsblog/authors/Guy_pic.fw_1430254017862.png"></a>
</div></div>
  <div class="post-heading">

    <h1 class="post-title navigation">
      <a href="/bigdata/post/Tx2LQ4WAWOP80EG/Building-a-Multi-Class-ML-Model-with-Amazon-Machine-Learning" rel="bookmark">
        Building a Multi-Class ML Model with Amazon Machine Learning</a>
    </h1>

    <ul class="post-attributes navigation inline-navigation blog-ui-piped">
      <li>
        <a href="/bigdata/post/Tx2LQ4WAWOP80EG/Building-a-Multi-Class-ML-Model-with-Amazon-Machine-Learning#" rel="bookmark">
          <time datetime="2015-05-21T15:36:46.913Z" title="2015-05-21T15:36:46.913Z" class="post-creation-date">May 21, 2015</time></a>
      </li>
      <li class="last">      
        <address class="by-line">
          <a rel="nofollow" data-contentid="gernest" class="author-name" itemprop="author" href="/bigdata/blog/author/Guy+Ernest">Guy Ernest</a>
</address>
      </li>    
    </ul>
    
    <ul class="category-list post-categories navigation inline-navigation blog-ui-piped">
  </ul><ul class="tag-list post-tags navigation inline-navigation blog-ui-piped">
  </ul></div>      
</div><div class="post-content-block">
    <div class="post-text"><p>
	<em>Guy Ernest is a Solutions Architect with AWS</em></p>
<p>
	<em>This post builds on our earlier post <a href="https://blogs.aws.amazon.com/bigdata/post/Tx2OZ63RJ6Z41A0/Building-a-Numeric-Regression-Model-with-Amazon-Machine-Learning" target="_blank">Building a Numeric Regression Model with Amazon Machine Learning</a>.</em></p>
<p>
	We often need to assign an object (product, article, or customer) to its class (product category, article topic or type, or customer segment). For example, <em>which category of products is most interesting to this customer?</em> Because of the massive scale of some businesses and the short lifespan of articles or customer visits, it's essential to be able to assign an object to its class at scale and speed to ensure successful business transactions.</p>
<p>
	This blog post shows how to build a <a href="http://docs.aws.amazon.com/machine-learning/latest/dg/training_models.html#multiclass-classification-model" target="_blank">multiclass classification</a> model that:</p>
<ul>
	<li>
		Helps automate the process of predicting object assignment to one of more than two classes, at scale and speed</li>
	<li>
		Can be used in a simple and scalable way to accommodate classes and objects that constantly evolve</li>
	<li>
		Requires minimal help from machine learning experts</li>
	<li>
		Can be extended to many aspects of your business</li>
</ul>
<p>
	In this post, you learn how to address multiclassification problems by using cartographic information to predict the type of forest cover that will occur on a land segment, from among six types. Similar multiclassification machine learning (ML) problems could include determining recommendations such as which product in an e-commerce store or on a video steaming service is most relevant for a visiting user.</p>
<p>
	As in my previous blog post on <a href="http://blogs.aws.amazon.com/bigdata/post/Tx2OZ63RJ6Z41A0/Building-a-Numeric-Regression-Model-with-Amazon-Machine-Learning" target="_blank">numerical regression</a>, I show how to build a multiclassification model based on a data set that is publicly available on <a href="http://www.kaggle.com" target="_blank">Kaggle</a>. Kaggle is a community site on which companies and researchers post their data, which data scientists then use to compete to solve data science problems. While building the model, you should think about how to use Amazon Machine Learning (<a href="http://aws.amazon.com/machine-learning/" target="_blank">Amazon ML</a>) to solve similar problems in your domain.</p>
<h2>
	Preparing the data to build the ML model</h2>
<p>
	The most important part of building a successful ML model is finding the most relevant data to feed it. The rule of thumb is GIGO, or <em>Garbage In, Garbage Out</em> (or <em>Gold In, Gold Out</em>, based on your perspective). Domain knowledge helps to identify what might be relevant. It's also important to have access to good data for training the model and the prediction process.</p>
<p>
	In this example, you might consider variables such as the elevation of the area, slope, and soil type as good predictors for the type of trees you would find in an area. Other parameters could include the distance to a water source or to a road. The organizers of the forest cover type prediction <a href="http://www.kaggle.com/c/forest-cover-type-prediction" target="_blank">competition</a> on the Kaggle site prepared this data:</p>
<pre>
Elevation - Elevation in meters
Aspect - Aspect in degrees azimuth
Slope - Slope in degrees
Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features
Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features
Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway
Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice
Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice
Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice
Horizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points
Wilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation
Soil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation
Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation
</pre>
<p>
	You probably wouldn't modify the data attributes for the first training of the model, but you should shuffle the rows to remove any artificial order that might come from the source of the data because this can cause bias in the model training.</p>
<p>
	For this example, you need to download the data from the Kaggle competition site after you sign in to the site. If you want to follow the commands in this post, you need a terminal window on a Linux or MacOS machine to run bash commands. For Windows users, you can spawn an <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html" target="_blank">EC2 instance</a> and run all commands from this instance.</p>
<p>
	Run the following commands:</p>
<pre>
# Download the training data from Kaggle: http://www.kaggle.com/c/forest-cover-type-prediction/download/train.csv.zip
unzip train.csv.zip
# shuffle the lines except for the first header line
tail -n+2 forest_cover_train.csv | gshuf -o forest_cover_train_shuffle.csv
# Add the header line from the original file as the first line of the shuffled file
head -1 forest_cover_train.csv | cat - forest_cover_train_shuffle.csv &gt; temp &amp;&amp; mv temp forest_cover_train_shuffle.csv
</pre>
<p>
	Then upload the shuffled file to Amazon S3, using the <a href="http://aws.amazon.com/cli/" target="_blank">AWS CLI</a>, to create the data source:</p>
<pre>
aws s3 cp forest_cover_train_shuffle.csv s3://&lt;BUCKET_NAME&gt;/ML/input/ForestCover/ --region us-east-1
</pre>
<p>
	Create a new data source and ML model using the <a href="https://console.aws.amazon.com/machinelearning/home?region=us-east-1" target="_blank">AWS Management Console</a>.</p>
<p>
	<img alt="Creating a new data source and machine learning model" height="266" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_1a_ML2.png" width="583" /></p>
<p>
	Continue to follow the wizard, making minimal changes to ensure that binary variables are identified as binary and not as numeric (mainly the soil type variables).</p>
<p>
	<img alt="Follow the machine learning wizard" height="363" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_2a_ML2.png" width="595" /></p>
<p>
	You can filter the long list of 56 variables by the <strong>Name</strong> prefix and see that you have 40 binary variables for soil type. This representation of the soil type variable, with 40 different Boolean variables instead of a single variable with 40 possible values, is common when preparing data for a specific ML algorithm. It might not be the simplest way of representing the data, but continue with the data as it is for now.</p>
<p>
	The next step is to choose the prediction target, which is a special attribute in the training data that contains the information that Amazon ML attempts to predict. Choose <strong>Cover_Type</strong> as the target.</p>
<p>
	<img alt="Choosing the prediction target" height="313" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_3a_ML2.png" width="621" /></p>
<p>
	<br />
	Because <strong>Cover_Type</strong> is a categorical variable, Amazon ML automatically identifies the model as a multiclass classification type.</p>
<p>
	Filter to find the <strong>Id</strong> variable in the data source and choose it as the row identifier.</p>
<p>
	<img alt="Choosing the row identifier" height="284" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_4a_ML2.png" width="639" /></p>
<p>
	Now you can launch the training process, which can take a few minutes to finish. The time it takes depends on the size of the training data: the number of attributes and rows.</p>
<h2>
	Evaluating model accuracy</h2>
<p>
	By default, Amazon ML splits the data so that 70% is used for training the model and 30% is used to evaluate the accuracy of the model. You can use your own split of training vs. evaluation data, but it is important that you never use training data to evaluate accuracy because the evaluation should estimate model accuracy from new data only.</p>
<p>
	Let's see how Amazon ML makes it easy to understand the model that it created.</p>
<p>
	<img alt="Seeing how Amazon Machine Learning makes it easy to understand the model" height="304" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_5_ML2.png" width="595" /></p>
<p>
	For <a href="http://docs.aws.amazon.com/machine-learning/latest/mlconcepts/mlconcepts.html#multiclass-classification" target="_blank">multiclass models</a>, the service uses a score called the F1-measure, which is a statistical measure of the precision and recall of all of the classes in the model. The F1 score ranges between 0 and 1 and the higher the score, the better the overall accuracy of the model. This ML model received a score of 0.69, which is much better than the random baseline score of 0.03.</p>
<p>
	But it is relatively hard to understand how the model is behaving for each of the classes. To see this, look at the confusion matrix by choosing <strong>Explore model performance</strong>.</p>
<p>
	<img alt="Confusion matrix" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_6_ML2.png" style="width: 539px; height: 371px;" /></p>
<p>
	The confusion matrix gives some insights about the performance of the model as a way to visualize the accuracy of multiclass classification predictive models. The confusion matrix illustrates in a table the number or percentage of correct and incorrect predictions for each class by comparing an observation’s predicted class and its true class. For more information about evaluating multiclass models, see <a href="http://docs.aws.amazon.com/machine-learning/latest/dg/evaluating_models.html#multiclass-model-insights" target="_blank">Multiclass Model Insights</a>.</p>
<p>
	The best possible confusion matrix shows all blue diagonally across the matrix, which means that all of the predictions were correct. But such an accuracy rate is very rare in machine learning. In this model, you can see that class 7 is very good with correct classification of 576 out of 629 cases of this class in the evaluation data set. This is more than 91% accurate. You can also see on the right side of the matrix that the F1 score for this class is very high, with a score of 0.89. Class 4 also has a high F1 score.</p>
<p>
	However, some cells have shades of red, which means that the model confused them with another class. For example, class 6 got a relatively low F1 score of 0.55, with an orange cell in the confusion matrix for class 3. This means that our model is confusing these two classes, and tends to predict class 3, when the correct answer should be class 6.</p>
<p>
	To understand why the ML model confuses the two, go back to the problem domain, which is classes of trees. Here are the types of trees that you are trying to predict:</p>
<pre>
1 - Spruce/Fir
2 - Lodgepole Pine
3 - Ponderosa Pine
4 - Cottonwood/Willow
5 - Aspen
6 - Douglas-fir
7 - Krummholz
</pre>
<p>
	Class 6 is &quot;Douglas-fir&quot; and class 3 is &quot;Ponderosa Pine.&quot; These trees are very close botanically. You can see the similarity between the two species by comparing the images Google presents for each.</p>
<p>
	<img alt="Comparing Google images of the species" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_7_ML2.png" style="width: 563px; height: 290px;" /><br />
	&nbsp;<br />
	When you have classes that are similar, you can expect to find human errors in the classifications used for training. Even small errors in classification can reduce the model's ability to distinguish accurately between classes. To mitigate this type of problem, you can ask the domain expert to evaluate how important it is to distinguish between very close classes. Maybe you can combine them into a single class (for example, Northwest Coast Pine), recheck the manual classifications you used for the problematic classes, or think about collecting data that distinguishes between the two classes. For example, needle length could help differentiate tree classes.</p>
<h2>
	How can you improve ML model accuracy?</h2>
<p>
	At this stage, you need to decide if you want to improve the ML model or use it as it is. As you may remember, the data set included 40 Boolean variables for each of the soil types, which is a way to improve the performance of some machine learning algorithms. With Amazon ML, you don't really need to use this kind of trick. Instead, it's better to focus on adding knowledge to the model.</p>
<p>
	For example, you can replace the long list of binary numbers with the textual description of soil type and allow Amazon ML to run textual analysis on these descriptions. Review the descriptions of the soil types as they appear on the competition site.</p>
<pre>
1 Cathedral family - Rock outcrop complex, extremely stony.
2 Vanet - Ratake families complex, very stony.
3 Haploborolis - Rock outcrop complex, rubbly.
4 Ratake family - Rock outcrop complex, rubbly.
5 Vanet family - Rock outcrop complex complex, rubbly.
6 Vanet - Wetmore families - Rock outcrop complex, stony.
7 Gothic family.
8 Supervisor - Limber families complex.
9 Troutville family, very stony.
10 Bullwark - Catamount families - Rock outcrop complex, rubbly.
11 Bullwark - Catamount families - Rock land complex, rubbly.
12 Legault family - Rock land complex, stony.
13 Catamount family - Rock land - Bullwark family complex, rubbly.
</pre>
<p>
	Notice that terms like <em>rubbly</em> and <em>stony</em> are repeated across the descriptions. Also notice that there are different levels for the term <em>stony</em>, such as <em>extremely stony</em>, <em>very stony</em>, or simply <em>stony</em>. Because these terms add knowledge to the system, you can assume that they will improve the ML model. Amazon ML can pick up patterns in textual descriptions; for example, soil types that include the word <em>rock</em> may be similar.</p>
<p>
	Go back to the data files you downloaded from the competition site and modify them to include the textual description for each row. You can use a long but simple AWK script to add the description to each line of the training data (and later, to the evaluation data).</p>
<pre>
BEGIN { FS = &quot;,&quot; } ;
NR == 1 {
    for (i = 1; i &lt;= NF; i++) headers[i] = $i;
    next
}
{
    # Choose the last bit flag
    description = &quot;&quot;
    if ($1 ==1) description = &quot;\&quot;1 Cathedral family - Rock outcrop complex, extremely stony.\&quot;&quot;
    else if ($2 ==1) description = &quot;\&quot;2 Vanet - Ratake families complex, very stony.\&quot;&quot;
    else if ($3 ==1) description = &quot;\&quot;3 Haploborolis - Rock outcrop complex, rubbly.\&quot;&quot;
    else if ($4 ==1) description = &quot;\&quot;4 Ratake family - Rock outcrop complex, rubbly.\&quot;&quot;
    else if ($5 ==1) description = &quot;\&quot;5 Vanet family - Rock outcrop complex complex, rubbly.\&quot;&quot;
    else if ($6 ==1) description = &quot;\&quot;6 Vanet - Wetmore families - Rock outcrop complex, stony.\&quot;&quot;
    else if ($7 ==1) description = &quot;\&quot;7 Gothic family.\&quot;&quot;
    else if ($8 ==1) description = &quot;\&quot;8 Supervisor - Limber families complex.\&quot;&quot;
    else if ($9 ==1) description = &quot;\&quot;9 Troutville family, very stony.\&quot;&quot;
    ...
    else if ($38 ==1) description = &quot;\&quot;38 Leighcan - Moran families - Cryaquolls complex, extremely stony.\&quot;&quot;
    else if ($39 ==1) description = &quot;\&quot;39 Moran family - Cryorthents - Leighcan family complex, extremely stony.\&quot;&quot;
    else if ($40 ==1) description = &quot;\&quot;40 Moran family - Cryorthents - Rock land complex, extremely stony.\&quot;&quot;
    else description = &quot;&quot;
    printf &quot;%s&quot;, description
    printf &quot;\n&quot;
}
</pre>
<p>
	Apply it to the training data file and paste the description file into the original file, using the following commands in your terminal:</p>
<pre>
# extract the cover_type field and match it to its textual description
cut -d&quot;,&quot; -f16- forest_cover_train_shuffle.csv | awk -f soil_type_translation.awk &gt; train_soil_description.csv
# add header line to the new created csv file
echo 'Description' | cat - train_soil_description.csv &gt; temp &amp;&amp; mv temp
train_soil_description.csv
# append the textual description as the last field of training data file
paste -d, forest_cover_train_shuffle.csv train_soil_description.csv &gt; forest_cover_train_with_description.csv
</pre>
<p>
	Then remove the 40 Boolean fields from the data file before uploading it to Amazon S3.</p>
<pre>
# remove fields 15 to 56 (soil type bits)
cut -d, -f1-15,56- forest_cover_train_with_description.csv &gt;
forest_cover_train_with_description_no_soil.csv
# copy the new training file to S3
 aws s3 cp forest_cover_train_with_description.csv s3:///ML/input/ForestCover/ --region us-east-1
</pre>
<h2>
	Defining a custom recipe for the ML model</h2>
<p>
	So far, you've used the default values for most of the wizards, but for this model you can use a simple custom recipe. Recipes are preformatted instructions for common transformations. You can use the insight that we got from reviewing the textual description of soil types, that having a single term like <em>stony</em> is not enough, and instruct the service to look at two words, such as <em>very stony</em> or <em>extremely stony</em>, together. This is called <a href="http://docs.aws.amazon.com/machine-learning/latest/dg/transforming_data.html#n-gram-transformation" target="_blank">n-gram transformation</a> of length 2.</p>
<p>
	After you upload the new training file to Amazon S3, create the data source as you did before, with the new textual description marked as a textual attribute, not as a category. Now you can use the option to provide a custom recipe.</p>
<p>
	<img alt="Providing a custom recipe" height="254" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_8_ML2.png" width="570" /></p>
<p>
	Add the following line to the default recipe:</p>
<pre>
&quot;ngram(lowercase(Description),2)&quot;
</pre>
<p>
	This line tells the service to apply lowercase to the <strong>Textual Description</strong> field and create an n-gram of length 2 for it.</p>
<p>
	<img alt="Providing a custom recipe" height="351" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_9_ML2.png" width="575" /></p>
<p>
	The rest of the process is similar to the evaluation you performed earlier. When you review the results of the evaluation for the newly created model, you can see that the overall F1 score, 0.71, is slightly better.</p>
<p>
	<img alt="Reviewing results for the new model" height="154" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_10_ML2.png" width="595" /></p>
<p>
	The confusion matrix and the F1 score for each class are also different, suggesting further trials for adding meaningful attributes to the dataset.</p>
<p>
	Also note the artificial distribution of the <strong>Cover_Type</strong> attribute in the training data.</p>
<p>
	<img alt="Artificial distribution of Cover_Type in the training data" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_11_ML2.png" style="width: 440px; height: 253px;" /></p>
<p>
	The administrators of the Kaggle competition ensured that an equal number of samples for each forest cover type were provided, which is not realistic. In most real-life cases, some classes are more common than the others, and it is best to keep this proportion in the training data. Having an equal number of samples for each cover type also adds to the complexity of comparing different ML models because each default 70/30 split creates a random skew of the data. In real data, the inherit skew will most likely be preserved.</p>
<h2>
	What did you learn?</h2>
<p>
	In this exercise, you learned the importance of getting the right data into the simplest form. Examples of simplest form can include downloading data from a competition site and using it as it is, using the textual description of the soil type without the long binary flags array of the different soil types, or not forcing the data to have the same number of observations for each type of class (if this is not the way the classes are distributed in the real world).</p>
<p>
	For fast and efficient ML model creation with Amazon ML, being a domain expert is more important than being a machine-learning expert. The ability to build a data source and ML model quickly and to evaluate performance, both overall and for each class, allows you to streamline the ML model.</p>
<p>
	You can consult machine-learning experts to discuss the meaning of F1 scores and the confusion matrix and to get advice on shuffling the training data and using the n-gram function on the textual description. Aside from that, you can do everything yourself.</p>
<p>
	You also learned to use the evaluation summary to compare a number of different models based on overall F1 scores. You dove deeper into the confusion matrix to see how well each model is able to identify a specific class and to identify common mistakes. You used this insight to tune the model to fit the business requirements better. You might not win the Kaggle machine learning competition, but you can improve business performance with streamlined Amazon ML model creation and usage.</p>
<p>
	If you have questions or suggestions, please leave a comment below.</p>
<p>
	---------------------------------------------------------</p>
<p>
	<em><span style="font-size:16px;"><strong>Do more with Amazon Machine Learning:</strong></span></em></p>
<p>
	<strong><a href="https://blogs.aws.amazon.com/bigdata/post/Tx2OZ63RJ6Z41A0/Building-a-Numeric-Regression-Model-with-Amazon-Machine-Learning" target="_blank">Building a Numeric Regression Model with Amazon Machine Learning</a></strong></p>
<p>
	<img alt="" height="110" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Social%20media%20image%20for%20ML%20thumbnail.PNG" width="363" /></p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p></div>
  </div>

  <div class="post-footer-block">
  <div class="fl-r">
    <ul class="share-buttons navigation inline-navigation">
      <li class="quiet">
        Share</li>
      <li>
        <span class="social-media email share-post">
<a href="/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx2LQ4WAWOP80EG" class="share-url icon email-icon" onclick="window.open('/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx2LQ4WAWOP80EG', '_blank', 'location=yes,width=700,height=600');return false;" title="Share via Email"></a>
</span>
</li>
      <li>            
        <span class="social-media facebook share-post">
<a href="https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx2LQ4WAWOP80EG%2FBuilding-a-Multi-Class-ML-Model-with-Amazon-Machine-Learning&t=Building%20a%20Multi-Class%20ML%20Model%20with%20Amazon%20Machine%20Learning" class="share-url icon facebook-icon" onclick="window.open('https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx2LQ4WAWOP80EG%2FBuilding-a-Multi-Class-ML-Model-with-Amazon-Machine-Learning&t=Building%20a%20Multi-Class%20ML%20Model%20with%20Amazon%20Machine%20Learning', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Facebook"></a>
</span>
</li>
      <li>
        <span class="social-media twitter share-post">
<a href="https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx2LQ4WAWOP80EG%2FBuilding-a-Multi-Class-ML-Model-with-Amazon-Machine-Learning&via=awscloud&text=Building+a+Multi-Class+ML+Model+with+Amazon+Machine+Learning&count=none" class="share-url icon twitter-icon" onclick="window.open('https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx2LQ4WAWOP80EG%2FBuilding-a-Multi-Class-ML-Model-with-Amazon-Machine-Learning&via=awscloud&text=Building+a+Multi-Class+ML+Model+with+Amazon+Machine+Learning&count=none', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Twitter"></a>
</span></li>
      </ul>
  </div>
<div>
  <ul class="post-attributes navigation inline-navigation blog-ui-piped push-bottom-1">
    <li>
      <a href="/bigdata/post/Tx2LQ4WAWOP80EG/Building-a-Multi-Class-ML-Model-with-Amazon-Machine-Learning#" rel="bookmark">
        <time datetime="2015-05-21T15:36:46.913Z" title="2015-05-21T15:36:46.913Z" class="post-creation-date">May 21, 2015</time></a>
    </li>
    <li>
          <a href="/bigdata/post/Tx2LQ4WAWOP80EG/Building-a-Multi-Class-ML-Model-with-Amazon-Machine-Learning#" rel="bookmark">Permalink</a>
        </li>
    <li>
          <a href="/bigdata/post/Tx2LQ4WAWOP80EG/Building-a-Multi-Class-ML-Model-with-Amazon-Machine-Learning#postCommentsTx2LQ4WAWOP80EG">Comments (<span id="commentNumber">2</span>)</a>
        </li>
    </ul>
</div></div></article>
</li>
          <li class="blog-post">
              <article
  class="post display published no-comments " 
  data-postid="Tx1WZP38ERPGK5K">
  
  <div class="post-heading-block clearfix">

  <div class="post-author">
      
<div class="author avatar author-avatar blog-ui-avatar blog-ui-author-avatar" data-content-id="neelamd">
  <a href="/bigdata/blog/author/Chris+Keyser" title="Chris Keyser"><img class="author-image" alt="Chris Keyser" src="//cdn.amazonblogs.com/bigdata_awsblog/authors/Author pic resized.fw_1429806863446.png"></a>
</div></div>
  <div class="post-heading">

    <h1 class="post-title navigation">
      <a href="/bigdata/post/Tx1WZP38ERPGK5K/Optimizing-for-Star-Schemas-and-Interleaved-Sorting-on-Amazon-Redshift" rel="bookmark">
        Optimizing for Star Schemas and Interleaved Sorting on Amazon Redshift</a>
    </h1>

    <ul class="post-attributes navigation inline-navigation blog-ui-piped">
      <li>
        <a href="/bigdata/post/Tx1WZP38ERPGK5K/Optimizing-for-Star-Schemas-and-Interleaved-Sorting-on-Amazon-Redshift#" rel="bookmark">
          <time datetime="2015-05-11T22:09:18.151Z" title="2015-05-11T22:09:18.151Z" class="post-creation-date">May 11, 2015</time></a>
      </li>
      <li class="last">      
        <address class="by-line">
          <a rel="nofollow" data-contentid="neelamd" class="author-name" itemprop="author" href="/bigdata/blog/author/Chris+Keyser">Chris Keyser</a>
</address>
      </li>    
    </ul>
    
    <ul class="category-list post-categories navigation inline-navigation blog-ui-piped">
  </ul><ul class="tag-list post-tags navigation inline-navigation blog-ui-piped">
  </ul></div>      
</div><div class="post-content-block">
    <div class="post-text"><p>
	<em>Chris Keyser is a Solutions Architect for AWS</em></p>
<p>
	Many organizations implement star and snowflake schema data warehouse designs and many BI tools are optimized to work with dimensions, facts, and measure groups. Customers have moved data warehouses of all types to Amazon Redshift with great success.</p>
<p>
	The Amazon Redshift team has released support for interleaved sort keys, another powerful option in its arsenal for speeding up queries. Interleaved sort keys can enhance performance on Amazon Redshift data warehouses, especially with large tables. Amazon Redshift performs well on common data models like a star or snowflake schema, third normal form (3NF), and denormalized tables.</p>
<p>
	Last year AWS published an article titled &quot;Optimizing for Star Schemas on Amazon Redshift&quot; that described design considerations when using star schemas.This post replaces that article and shows how interleaved sort keys may affect design decisions when implementing star schemas on Amazon Redshift. You will see many links to the <a href="http://docs.aws.amazon.com/redshift/latest/dg/welcome.html" target="_blank">Amazon Redshift Database Developer Guide</a>, which is the authoritative technical reference and has a deeper explanation of the features and decisions highlighted in this post.</p>
<p>
	Star and snowflake schemas organize around a central fact table that contains measurements for a specific event, such as a sold item. The fact table has foreign key relationships to one or more dimension tables that contain descriptive attribute information for the sold item, such as customer or product. Snowflake schemas extend the star concept by further normalizing the dimensions into multiple tables. For example, a product dimension may have the brand in a separate table. Often, a fact table can grow quite large and will benefit from an interleaved sort key. For more information about these schema types, see <a href="http://en.wikipedia.org/wiki/Star_schema" target="_blank">star schema</a> and <a href="http://en.wikipedia.org/wiki/Snowflake_schema" target="_blank">snowflake schema</a>.</p>
<p>
	Most customers experience significantly better performance when migrating their existing data models largely unchanged to Amazon Redshift due to columnar compression, flexible data distribution, and configurable sort keys. While Amazon Redshift automatically detects star schema data structures and has built-in optimizations for efficiently querying this data, you can further optimize your data model to improve query performance. The principal areas under your control are:</p>
<ul>
	<li>
		Defining how Amazon Redshift distributes data for your tables, which affects how much data moves between nodes when queried, and how load is distributed between nodes.</li>
	<li>
		Defining the type of sort key and the columns in the sort key, which determines the ordering of data stored on disk and can speed up filtering, aggregation, and joins.</li>
	<li>
		Defining the compression of data in your table, which affects the amount of disk I/O performed.</li>
	<li>
		Defining foreign key and primary key constraints, which act as hints for the query optimizer.</li>
	<li>
		Running maintenance operations to ensure optimal performance, which affects the statistics used by the query optimizer and the ordering of new data stored on disk.</li>
	<li>
		Using workload management to separate long running queries from short running queries.</li>
	<li>
		Managing the use of cursors for large result sets.</li>
</ul>
<p>
	Each of these areas can affect the overall performance of your solution.</p>
<h1>
	Data Distribution</h1>
<p>
	Massive parallel processing (MPP) data warehouses like Amazon Redshift scale horizontally by adding compute nodes to increase compute, memory, and storage capacity. The cluster spreads data across all of the compute nodes, and the distribution style determines the method that Amazon Redshift uses to distribute the data. When a user executes SQL queries, the cluster spreads the execution across all compute nodes. The query optimizer will, where possible, optimize for operating on data local to a compute node, and minimize the amount of data that passes over the network between compute nodes.</p>
<p>
	Your choice of data distribution style and distribution key affects the amount of query computation that can occur on data local to a compute node to avoid redistributing intermediate results. When you create a table on Amazon Redshift, you specify a distribution style of EVEN, ALL, or KEY. An EVEN style spreads data across evenly all nodes in your cluster and is the default option. For a distribution style of ALL, the data for the table is put on each node, which has load performance and storage implications although it ensures that the table data will always be local on every compute node. If you specify a distribution style of KEY, then the rows with the same value for the designated DISTKEY column are placed at the same node.</p>
<p>
	<strong>Choosing a Distribution Style</strong></p>
<p>
	Using distribution keys is a good way to optimize the performance of Amazon Redshift when you use a star schema. With an EVEN distribution, data is spread equally across all nodes in the cluster to ensure balanced processing. In many cases, simply distributing data equally using EVEN does not optimize performance as the data rows on a node for the table do not have any affinity with each other. Take an example of a fact table for <em>ORDERS</em> where a distribution style of EVEN is chosen. In this case, the orders for a specific customer are potentially spread across many compute nodes in the cluster. However, if the table had a distribution style of KEY and a DISTKEY of <em>customer_id</em> was chosen, then all of the orders for a particular customer would be stored on the same compute node. Using a distribution style of EVEN can lead to more cross-node traffic.</p>
<p>
	A good selection for a distribution key distributes data relatively evenly across nodes while collocating related data on a compute node used in joins or aggregates. When you perform a join on a column that is a distribution key for both tables, Amazon Redshift is able to run the join locally on each node with no inter-node data movement; this is because rows with the same distribution key value reside on the same node for both tables in the join. Similarly, aggregating on a distribution key performs better because the data for the aggregate column value is local to the compute node.</p>
<p>
	In a typical star schema, the fact table has foreign key relationships with multiple dimension tables, so you need to choose one of the dimensions. You would choose the foreign key for the largest frequently joined dimension as a distribution key in the fact table and the primary key in the dimension table. Make sure that the distribution keys chosen result in relatively even distribution for both tables, and if the distribution is skewed, use a different dimension. Then analyze the remaining dimensions to determine if a distribution style of ALL, KEY, or EVEN is appropriate.</p>
<p>
	For slowly changing dimensions of reasonable size, DISTSTYLE ALL is a good choice for the dimension (reasonable size in this case means up to a few million rows, and that the number of rows in the dimension table is fewer than the filtered fact table for a typical join). If you have very frequent updates to a dimension table, then DISTSTYLE ALL may not be appropriate. In this case, it is better to use a distribution style of KEY and distribute on a column that distributes data relatively evenly rather than using DISTSTYLE EVEN. Joins will be more efficient when the joined tables have a distribution style of KEY even if the join column does not use the distribution key.</p>
<p>
	<strong>Key Distribution Style and Skew</strong></p>
<p>
	Skew is a critical factor related to a distribution style of KEY. Skew measures the ratio between the fewest and greatest number of rows on a compute node in the cluster. A high skew indicates that you have many more rows on one (or more) of the compute nodes compared to the other nodes. Skew results in performance hotspots and negates the benefits of distributing data for node-local joins. Check your assumptions with skew; sometimes you think a column provides good distribution but in reality it does not. A common occurrence is when you don’t realize that a column is nullable, resulting in rows with null placed on one compute node. If no column provides relatively even data distribution using a KEY distribution style, then choose a style of EVEN. For examples of checking skew, see the tuning reference below.</p>
<p>
	<img alt="Checking skew" height="406" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_StarSchema.png" width="628" /></p>
<p>
	If you need to use a style of EVEN for some of your tables, then try to form your queries to join these tables as late as possible. This results in a smaller data set, when you apply the join to the evenly distributed table, and improves performance.</p>
<p>
	For detailed guidance on choosing a distribution style, see <a href="http://docs.aws.amazon.com/redshift/latest/dg/t_Distributing_data.html">Choosing a Data Distribution Style</a>, especially the subsection <a href="http://docs.aws.amazon.com/redshift/latest/dg/t_designating_distribution_styles.html" target="_blank">Designating Distribution Styles</a>, as well as the topics <a href="http://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html" target="_blank">Choose the Best Distribution Style</a> and the <a href="http://docs.aws.amazon.com/redshift/latest/dg/tutorial-tuning-tables.html" target="_blank">Tuning Table Design tutorial</a>. More specific guidance on choosing distribution styles and keys can be found in the tutorial section <a href="http://docs.aws.amazon.com/redshift/latest/dg/tutorial-tuning-tables-distribution.html" target="_blank">Step 4: Select Distribution Styles</a>.</p>
<h1>
	Data Ordering with Sort Keys</h1>
<p>
	A sort key determines the order of data stored on disk for your table. Amazon Redshift improves query performance when the sort key is used in the <em>where </em>clause (often called a filter predicate) by checking the min and max value in a block and skipping blocks of data that do not fall into the range defined by the predicate. Tables on Amazon Redshift can have only one sort key defined, with the option of multiple columns in the sort key. Amazon Redshift now offers two types of sort keys: compound and interleaved.</p>
<p>
	A compound sort key specifies precedence among the sort key columns. It sorts first by the first key, distinguishes ties using the second sort key, and so on. A compound sort key can include up to 400 columns, which are collectively used to filter data at query time. A compound sort key is useful when a <em>prefix</em> of the sort key columns is used to filter data in the query. If the leading key in a compound sort key is relatively unique (has high cardinality), then adding additional columns to your sort key has little impact on query performance while adding maintenance costs. A good rule of thumb is to keep the number of columns to six or fewer when using a compound sort key. There are valid situations for using more than six columns, although those situations are typically more advanced tuning options. For compound sort keys, Amazon Redshift also makes operations like <em>group by </em>or <em>order by</em> on the sort column more efficient.</p>
<p>
	Interleaved sort keys, on the other hand, treat each column with equal importance. Use interleaved sort keys when you do not know exactly which combinations of columns will be used in your queries. An interleaved sort key can contain up to 8 columns and improves query performance when any subset of the keys (in any order) are used in a filter predicate. Queries perform faster as more sort key columns are specified in the filter predicate. For example, assuming you have a sort key of 4 columns, then using 3 of the columns in your filter predicate will perform better than using 2 of the columns. In addition, tables using interleaved sort keys perform better with fewer sort keys defined. Make sure that you do not have extraneous columns when you create your sort keys.</p>
<p>
	To illustrate the uses of the sort key types, let's say a product table uses the <em>type</em>, <em>color</em>, and <em>size</em> columns as the compound sort key (in that order), i.e., COMPOUND sortkey(type, color, size). If you filter on <em>type</em>=’shirt’ and <em>color</em> = 'red' and <em>size</em> = 'xl', then Amazon Redshift is able to take full advantage of the compound sort key. If you filter on <em>type</em>=’shirt’ and <em>color</em> = 'red', the compound sort key still makes the query more efficient as <em>type</em> and <em>color</em> are a leading part of the sort key (i.e., a prefix of the sort key columns). If you filter for <em>color</em>=’red’ and <em>size</em>='xl', then the compound sort key is limited to no impact (it may still boost performance if the leading key is low cardinality). However, if the type, color, and size columns are specified as an interleaved sort key, then the query on color and size is faster. The use of any of the interleaved sort key columns in the filter predicate improves performance, and performance gains increase for more selective queries that use multiple columns.</p>
<p>
	<strong>Choosing between an Interleaved or Compound Sort Key</strong></p>
<p>
	There are several tradeoffs between compound and interleaved sort keys. Interleaved sort keys are slightly more costly to maintain today than compound sort keys. Compound sort keys can also improve the performance of joins, group by and order by statements, while interleaved sort keys do not. As a result, if you always query your table with a prefix of the sort keys in filter predicates, then compound sort keys will give you the best performance with lower maintenance costs. On the other hand, a good use case for an interleaved sort key is a large fact table (&gt; 100M+ rows) where typical data query patterns use different subsets of the columns in filter predicates. Further, the benefits of the interleaved sort key increase with the table size.</p>
<p>
	Another common use case where compound sort keys are a better choice than interleaved is when you are frequently appending data in order, as in a time series table. In this case, the data in the unsorted region remains naturally sorted without the need for a vacuum operation. Interleaved sorts do not preserve this natural order, and so perform worse.</p>
<p>
	In many cases, you can take advantage of both approaches. Often, data changes frequently for recent content in your fact tables, while the bulk of historical data remains unaltered. In this situation, consider maintaining two tables: a table with an interleaved sort key for the historical fact data, and a second table containing recent data with a compound sort key, with a union view over the two tables. You will need to migrate data periodically from the recent table to the historical table.</p>
<p>
	For more information about defining sort keys, see <a href="http://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data.html" target="_blank">Choosing Sort Keys</a> and the <a href="http://docs.aws.amazon.com/redshift/latest/dg/tutorial-tuning-tables.html" target="_blank">Tuning Table Design tutorial</a>.</p>
<h1>
	Data Compression</h1>
<p>
	Amazon Redshift optimizes the amount and speed of data I/O by using compression and purpose-built hardware. Amazon Redshift analyzes the first 100,000 rows of data to determine the compression settings for each column when you copy data into an empty table. Because Amazon Redshift uses a columnar design, it can choose optimal compression algorithms for each column individually based upon the data content in that column, typically yielding significantly better compression than a row-based database.</p>
<p>
	Normally, you should rely upon the Amazon Redshift logic to choose the optimal compression type, although you can also choose to override these settings. Compression helps your queries perform faster and minimizes the amount of physical storage your data consumes. If you are not using a sort key, then use automatic compression. If you are using a sort key, then not compressing the sort key may improve performance, especially if the sort key is small (for example, integer values). Calculating compression is relatively expensive. When reloading a table or moving the data to another table, consider truncating the table or using create table like syntax to avoid the automatic compression calculation.</p>
<p>
	You should periodically check compression settings if you are loading new data, because the optimal compression scheme may change as the data changes. You should use the recommendations generated by Amazon Redshift. A helpful utility for optimizing compression on your tables is the Amazon Redshift Columnar Encoding utility (<a href="https://github.com/awslabs/amazon-redshift-utils" target="_blank">https://github.com/awslabs/amazon-redshift-utils</a>). With this utility, you can automatically generate a script that will apply the correct column encoding to a table, based upon data in the existing table. You can then use this to update your compression settings by creating a new table and either inserting from the existing table, or re-loading using a COPY operation. For more information about controlling compression options, see <a href="http://docs.aws.amazon.com/redshift/latest/dg/t_Compressing_data_on_disk.html" target="_blank">Choosing a Column Compression Type</a> and the <a href="http://docs.aws.amazon.com/redshift/latest/dg/tutorial-tuning-tables.html" target="_blank">Tuning Table Design tutorial</a>.</p>
<h1>
	Primary and Foreign Key Constraints</h1>
<p>
	It is a best practice to declare primary and foreign key relationships between your dimension tables and fact tables for your star schema. Amazon Redshift uses this information to optimize the query by eliminating redundant joins and establishing join order. Generally, the query optimizer detects redundant joins without constraints defined if you keep statistics up to date by running the ANALYZE command as described later in this post. Often, BI applications benefit from defining constraints as well.</p>
<p>
	To avoid unexpected query results, you should ensure that the data being loaded does not violate foreign key constraints because Amazon Redshift does not enforce them. You also need to ensure that primary key uniqueness is maintained by enforcing no duplicate inserts. For example, if you load the same file twice with the <em>copy</em> command, Amazon Redshift will not enforce primary keys and will duplicate the rows in the table. This duplication violates the primary key constraint. For more information, see <a href="http://docs.aws.amazon.com/redshift/latest/dg/t_Defining_constraints.html" target="_blank">Defining Constraints</a>.</p>
<h1>
	Maintenance Operations</h1>
<p>
	To maintain peak performance you must perform regular maintenance operations on a daily or weekly basis. A system view, <a href="http://docs.aws.amazon.com/redshift/latest/dg/r_SVV_TABLE_INFO.html" target="_blank">svv_table_info</a>, provides a lot of useful information on the performance health of your tables, including areas like table skew, percent unsorted, the quality of the current table statistics, and sort key information. Another great resource to check out for maintenance scripts and other useful tuning views and tools is <a href="https://github.com/awslabs/amazon-redshift-utils" target="_blank">https://github.com/awslabs/amazon-redshift-utils</a>.</p>
<p>
	Up-to-date statistics are an essential component of efficient query performance, whether using a star schema or other data model design. To maintain peak performance, execute maintenance operations when you add or refresh data and before you tune the query performance. Use the ANALYZE command to update the statistical metadata that the query planner uses to build and choose optimal plans. Alternately, you can update statistics while loading data with the COPY command by setting the STATUPDATE option to ON. If you are loading data relatively frequently and are concerned about the maintenance overhead of a full analysis, then consider only analyzing distribution and sort keys on each COPY operation, and analyze the full table at a lower frequency, such as daily or weekly.</p>
<p>
	The VACUUM command is used to re-sort data added to non-empty tables, and to recover space freed when you delete or update a significant number of rows. Performance degrades as the unsorted region grows and data is deleted or updated. One exception is when you are using a compound sort key and table data loads are in-order append only data, in which case the table remains sorted on the disk and performance does not degrade due to sort order. The COPY operation sorts data when loading an empty table, eliminating the need to run a vacuum on initial loads. If you run both VACUUM and ANALYZE, run VACUUM first because it affects the statistics generated by ANALYZE. Vacuuming is a relatively expensive operation and should be run during periods of low load. You can allocate additional memory to vacuuming in order to speed up the process.</p>
<p>
	Monitoring for long running queries and poorly performing queries is another recommended practice. You can monitor queries using the Amazon Redshift console, and periodically check a system table, <a href="http://docs.aws.amazon.com/redshift/latest/dg/r_STL_ALERT_EVENT_LOG.html" target="_blank">stl_alert_event_log</a>, which flags potential performance concerns. You can see the details of a specific query execution in the console under query execution details (see <a href="http://docs.aws.amazon.com/redshift/latest/mgmt/analyzing-query-execution.html" target="_blank">Analyzing Query Execution</a>). The visualization helps you isolate which parts of a poorly performing query are taking the most time. Another good check is for queries that are getting queued, using <a href="http://docs.aws.amazon.com/redshift/latest/dg/r_STL_WLM_QUERY.html" target="_blank">stl_wlm_query</a>. Long queuing times indicate that slow running queries in your cluster are backing up other queries, either due to an unexpected poorly performing query that requires optimization, or a need to separate big queries and small queries that are expected in the cluster, using workload management.</p>
<p>
	For more information, see <a href="http://docs.aws.amazon.com/redshift/latest/dg/t_Reclaiming_storage_space202.html" target="_blank">Vacuuming Tables</a>, <a href="http://docs.aws.amazon.com/redshift/latest/dg/t_Analyzing_tables.html" target="_blank">Analyzing Tables</a>, and <a href="http://docs.aws.amazon.com/redshift/latest/dg/performing-a-deep-copy.html" target="_blank">Performing a Deep Copy</a>.</p>
<h1>
	Workload Management</h1>
<p>
	Although there is nothing specific to star schemas related to workload management, it’s worth mentioning when discussing performance considerations. Amazon Redshift provides workload management that lets you segment longer running, more resource-intensive queries from shorter running queries. You can separate longer running queries, like those associated with batch operations or report generation, from shorter running queries, like those associated with dashboards or data exploration. This helps keep applications that require responsive queries from being backed up by long-running queries. You can also allocate more memory to queues handling more intensive queries, to boost their performance. For more information about workload management, see <a href="http://docs.aws.amazon.com/redshift/latest/dg/tutorial-configuring-workload-management.html" target="_blank">Tutorial: Configuring Workload Management (WLM) Queues to Improve Query Processing</a> and <a href="http://docs.aws.amazon.com/redshift/latest/dg/cm-c-implementing-workload-management.html" target="_blank">Implementing Workload Management</a>.</p>
<h1>
	Cursors and Drivers</h1>
<p>
	Cursors are another case where there are no specific concerns related to star schemas, but they are worth a mention. When queries are issued, the driver retrieves the results into memory, and then returns the result to the application. When large results are returned, this can adversely affect the client application by consuming too much memory. When cursors are used, results are retrieved a chunk at a time and consumed by the client application, reducing the impact on memory. However, using cursors consumes resources on the leader node of the cluster, and therefore Amazon Redshift limits the total size of results for cursors (result set sizes are still very large, and vary depending upon the cluster node type used). You can determine the number of concurrent cursors used by adjusting the maximum size of a result set. By decreasing the maximum size, you increase cursor concurrency. For more information about the impact of using cursors, and configuring <code>max_cursor_result_set_size</code> see <a href="http://docs.aws.amazon.com/redshift/latest/dg/declare.html" target="_blank">Fetch</a>.</p>
<p>
	AWS recently launched free <a href="http://docs.aws.amazon.com/redshift/latest/mgmt/configure-odbc-connection.html" target="_blank">ODBC</a> and <a href="http://docs.aws.amazon.com/redshift/latest/mgmt/configure-jdbc-connection.html" target="_blank">JDBC</a> drivers optimized for use with Amazon Redshift. These drivers improve performance over the use of open source drivers, and are more memory efficient. The drivers have an additional mode (odbc: single row mode, jdbc: blocking rows mode), which retrieves results one row or a few rows at time without requiring cursors. When loading large data sets, you can now use either cursors or one of the new modes. Both approaches limit the impact on client memory by not retrieving the full data set into memory. The use of cursors has certain restrictions with concurrency and maximum result size that depends upon the cluster type. For more information, see <a href="http://docs.aws.amazon.com/redshift/latest/dg/declare.html" target="_blank">DECLARE</a>. The use of single row or blocking row mode is slower than cursors for large data sets because results are pulled back a single or few rows at time, while avoiding the concurrency and size restrictions that cursors incur.</p>
<h1>
	Conclusion</h1>
<p>
	Star and snowflake schemas run well on Amazon Redshift, and the addition of interleaved sort keys further enhances performance by reducing I/O for a wider range of filter predicates on a table when needed. Customers have had great results running many different data models on Amazon Redshift. If you are using Amazon Redshift, then interleaved sort keys can help your queries run even faster. If you haven't tried Amazon Redshift yet, it’s easy and you can try it for free for 60 days. <a href="http://aws.amazon.com/redshift/free-trial/" target="_blank">Make sure to check it out</a>.</p>
<p>
	If you have questions or suggestions, please leave a comment below.</p>
<p>
	<em><span style="font-size:16px;"><strong>Do more with Amazon Redshift</strong></span></em><strong>:</strong></p>
<p>
	<strong><a href="http://blogs.aws.amazon.com/bigdata/post/Tx2ANLN1PGELDJU/Best-Practices-for-Micro-Batch-Loading-on-Amazon-Redshift" target="_blank">Best Practices for Micro-Batch Loading on Amazon Redshift</a></strong></p>
<p>
	<img alt="Micro-batch loading on Amazon Redshift" height="215" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Social%20media%20-%20Microbatch.PNG" width="354" /></p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p></div>
  </div>

  <div class="post-footer-block">
  <div class="fl-r">
    <ul class="share-buttons navigation inline-navigation">
      <li class="quiet">
        Share</li>
      <li>
        <span class="social-media email share-post">
<a href="/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx1WZP38ERPGK5K" class="share-url icon email-icon" onclick="window.open('/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx1WZP38ERPGK5K', '_blank', 'location=yes,width=700,height=600');return false;" title="Share via Email"></a>
</span>
</li>
      <li>            
        <span class="social-media facebook share-post">
<a href="https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx1WZP38ERPGK5K%2FOptimizing-for-Star-Schemas-and-Interleaved-Sorting-on-Amazon-Redshift&t=Optimizing%20for%20Star%20Schemas%20and%20Interleaved%20Sorting%20on%20Amazon%20Redshift" class="share-url icon facebook-icon" onclick="window.open('https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx1WZP38ERPGK5K%2FOptimizing-for-Star-Schemas-and-Interleaved-Sorting-on-Amazon-Redshift&t=Optimizing%20for%20Star%20Schemas%20and%20Interleaved%20Sorting%20on%20Amazon%20Redshift', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Facebook"></a>
</span>
</li>
      <li>
        <span class="social-media twitter share-post">
<a href="https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx1WZP38ERPGK5K%2FOptimizing-for-Star-Schemas-and-Interleaved-Sorting-on-Amazon-Redshift&via=awscloud&text=Optimizing+for+Star+Schemas+and+Interleaved+Sorting+on+Amazon+Redshift&count=none" class="share-url icon twitter-icon" onclick="window.open('https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx1WZP38ERPGK5K%2FOptimizing-for-Star-Schemas-and-Interleaved-Sorting-on-Amazon-Redshift&via=awscloud&text=Optimizing+for+Star+Schemas+and+Interleaved+Sorting+on+Amazon+Redshift&count=none', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Twitter"></a>
</span></li>
      </ul>
  </div>
<div>
  <ul class="post-attributes navigation inline-navigation blog-ui-piped push-bottom-1">
    <li>
      <a href="/bigdata/post/Tx1WZP38ERPGK5K/Optimizing-for-Star-Schemas-and-Interleaved-Sorting-on-Amazon-Redshift#" rel="bookmark">
        <time datetime="2015-05-11T22:09:18.151Z" title="2015-05-11T22:09:18.151Z" class="post-creation-date">May 11, 2015</time></a>
    </li>
    <li>
          <a href="/bigdata/post/Tx1WZP38ERPGK5K/Optimizing-for-Star-Schemas-and-Interleaved-Sorting-on-Amazon-Redshift#" rel="bookmark">Permalink</a>
        </li>
    <li>
          <a href="/bigdata/post/Tx1WZP38ERPGK5K/Optimizing-for-Star-Schemas-and-Interleaved-Sorting-on-Amazon-Redshift#postCommentsTx1WZP38ERPGK5K">Comments (<span id="commentNumber">0</span>)</a>
        </li>
    </ul>
</div></div></article>
</li>
          <li class="blog-post">
              <article
  class="post display published no-comments " 
  data-postid="Tx2CD670NRJ8XXK">
  
  <div class="post-heading-block clearfix">

  <div class="post-author">
      
<div class="author avatar author-avatar blog-ui-avatar blog-ui-author-avatar" data-content-id="ljjoseph">
  <a href="/bigdata/blog/author/Leena+Joseph" title="Leena Joseph"><img class="author-image" alt="Leena Joseph" src="//cdn.amazonblogs.com/bigdata_awsblog/authors/Author pic resized_Leena.fw_1430942323527.png"></a>
</div></div>
  <div class="post-heading">

    <h1 class="post-title navigation">
      <a href="/bigdata/post/Tx2CD670NRJ8XXK/Using-AWS-Data-Pipeline-s-Parameterized-Templates-to-Build-Your-Own-Library-of-E" rel="bookmark">
        Using AWS Data Pipeline's Parameterized Templates to Build Your Own Library of ETL Use-case Definitions</a>
    </h1>

    <ul class="post-attributes navigation inline-navigation blog-ui-piped">
      <li>
        <a href="/bigdata/post/Tx2CD670NRJ8XXK/Using-AWS-Data-Pipeline-s-Parameterized-Templates-to-Build-Your-Own-Library-of-E#" rel="bookmark">
          <time datetime="2015-05-06T20:09:29.035Z" title="2015-05-06T20:09:29.035Z" class="post-creation-date">May 6, 2015</time></a>
      </li>
      <li class="last">      
        <address class="by-line">
          <a rel="nofollow" data-contentid="ljjoseph" class="author-name" itemprop="author" href="/bigdata/blog/author/Leena+Joseph">Leena Joseph</a>
</address>
      </li>    
    </ul>
    
    <ul class="category-list post-categories navigation inline-navigation blog-ui-piped">
  </ul><ul class="tag-list post-tags navigation inline-navigation blog-ui-piped">
  </ul></div>      
</div><div class="post-content-block">
    <div class="post-text"><p>
	<em>Leena Joseph is an SDE for AWS Data Pipeline</em></p>
<p>
	In an earlier post, we introduced you to <a href="http://blogs.aws.amazon.com/bigdata/post/Tx1PU7JM7I34L81/-span-class-matches-ETL-span-Processing-Using-AWS-Data-Pipeline-and-Amazon-Elast" target="_blank">ETL processing using AWS Data Pipeline and Amazon EMR</a>. This post shows how to build ETL workflow templates with <a href="http://aws.amazon.com/datapipeline" target="_blank">AWS Data Pipeline</a>, and build a library of recipes to implement common use cases. This is an introduction to parameterized templates, which serve as proven recipes and can be shared as a library of reusable pipeline definitions with other parts of the organization/company or contributed to the larger community.</p>
<p>
	Data Pipeline allows you to define complex workflows for data movement and transformation. You can easily create complex data processing workloads, manage inter-task dependencies, and configure retries of transient failures and failure notification for individual tasks.</p>
<h1>
	<strong>Creating Pipelines Using Built-in Templates</strong></h1>
<p>
	Data Pipeline supports a library of parameterized templates for common ETL use cases. The Create Pipeline page on the <a href="https://console.aws.amazon.com/datapipeline/home?" target="_blank">AWS Management Console</a> provides an option to create your pipeline using templates.</p>
<p>
	<img alt="Create Pipeline page on AWS Management Console" height="325" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_1-Data_Pipeline_Templates.PNG" width="563" /></p>
<p>
	Select the template, fill out the parameters, and the pipeline is ready to launch. In a few clicks, you can create and launch pipelines for complex ETL use cases. Currently, this library has parameterized templates for common use cases related to Amazon DynamoDB, Amazon Redshift, Amazon RDS, and Amazon EMR. For more information, see the related blog post <a href="https://aws.amazon.com/blogs/aws/data-pipeline-params-in-templates/" target="_blank">AWS Data Pipeline Update – Parameterized Templates</a> on the AWS blog.</p>
<h1>
	Authoring New Templates</h1>
<p>
	Data Pipeline also supports the use of custom templates. You can author a template for your ETL use case and use it for launching pipelines.</p>
<p>
	The JSON-formatted template consists of three sections: “objects”, “parameters,” and “values.” The objects section consists of a list of pipeline objects that define your pipeline workflow, the parameters section defines the attributes of each parameter, and the values section defines values for parameters.</p>
<pre>
{
  &quot;objects&quot;: [],
  &quot;parameters&quot;: [],
  “values”:{}
}
</pre>
<p>
	Parameters are referenced within a pipeline object using a parameter ID, such as “#{&lt;myparameterId&gt;}”. The parameter ID always starts with the prefix <em>my</em>. Other attributes within the parameter object are available to help provide more context to the parameter, such as “description”, “type”, “optional”, “watermark”, “default”, “isArray”, etc. For more details, see <a href="http://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html#add-pipeline-variables" target="_blank">Template Parameter Objects</a>.</p>
<p>
	The values section in the template is a list of parameter values that are used by the pipeline at runtime, after the pipeline is activated. Values must be present either in the values section or as the <strong>default</strong> field in the parameters section. Values can also be applied externally to the pipeline definition as arguments to the PutPipelineDefinition or ActivatePipeline API actions (see AWS documentation to learn more about <a href="http://docs.aws.amazon.com/datapipeline/latest/APIReference/Welcome.html" target="_blank">Data Pipeline API</a>). Values for the parameter object <strong>default</strong> field are taken as the default if no values are present in the values section. Actions return errors for empty or null values in the parameters or values sections.</p>
<p>
	Let's look at how to build a template for copying DynamoDB tables from one AWS region to another using EMR.</p>
<p>
	<img alt="Copying DynamoDB tables across regions using EMR" height="300" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Social%20and%20Image%202%20recaptured.PNG" width="630" /></p>
<p>
	DynamoDB tables can be copied from one region to another using Hive DynamoDBStorageHandler on an EMR cluster. To learn how to export and import DynamoDB using EMR, see <a href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EMR_Hive_Commands.html" target="_blank">Hive Command Examples for Exporting, Importing, and Querying Data in DynamoDB</a>.</p>
<p>
	The pipeline objects and dataflow for this use case do not change for different users. However, the source and destination tables, region, and read and write throughput can vary, and are therefore parameterized. You can also parameterize other object fields, such as pipelineLogUri and EMR cluster fields.</p>
<p>
	The template given is the JSON template for copying a DynamoDB table from one AWS region to another. The template definition is also available in Amazon S3 (<a href="https://s3.amazonaws.com/datapipeline-us-east-1/templates/DynamoDB+Templates/DynamoDB+cross-regional+table+copy.json" target="_blank" title="https://s3.amazonaws.com/datapipeline-us-east-1/templates/DynamoDB+Templates/DynamoDB+cross-regional+table+copy.json">https://s3.amazonaws.com/datapipeline-us-east-1/templates/DynamoDB+Templates/DynamoDB+cross-regional+table+copy.json</a>).</p>
<p>
	The pipeline objects in the definition below define a workflow with DynamoDB table fields parameterized. The parameters section defines the attributes for the parameterized field values. The values section in the template are optional but, if provided, will be the default values for the parameters.</p>
<pre class="brush: java">
{
  &quot;objects&quot;: [
    {
      &quot;id&quot;: &quot;Default&quot;,
      &quot;scheduleType&quot;: &quot;cron&quot;,
      &quot;failureAndRerunMode&quot;: &quot;CASCADE&quot;,
      &quot;schedule&quot;: {
        &quot;ref&quot;: &quot;DefaultSchedule&quot;
      },
      &quot;name&quot;: &quot;Default&quot;,
      &quot;pipelineLogUri&quot;: &quot;#{myPipelineLogUri}&quot;,
      &quot;role&quot;: &quot;DataPipelineDefaultRole&quot;,
      &quot;resourceRole&quot;: &quot;DataPipelineDefaultResourceRole&quot;
    },
    {
      &quot;region&quot;: &quot;#{mySourceTableRegion}&quot;,
      &quot;id&quot;: &quot;EmrClusterForCopy&quot;,
      &quot;terminateAfter&quot;: &quot;2 Hours&quot;,
      &quot;amiVersion&quot;: &quot;3.3.2&quot;,
      &quot;masterInstanceType&quot;: &quot;m1.medium&quot;,
      &quot;coreInstanceType&quot;: &quot;m1.medium&quot;,
      &quot;coreInstanceCount&quot;: &quot;1&quot;,
      &quot;name&quot;: &quot;EmrClusterForCopy&quot;,
      &quot;type&quot;: &quot;EmrCluster&quot;
    },
    {
      &quot;region&quot;: &quot;#{mySourceTableRegion}&quot;,
      &quot;id&quot;: &quot;DDBSourceTable&quot;,
      &quot;tableName&quot;: &quot;#{mySourceTableName}&quot;,
      &quot;name&quot;: &quot;DDBSourceTable&quot;,
      &quot;dataFormat&quot;: {
        &quot;ref&quot;: &quot;DDBExportFormat&quot;
      },
      &quot;type&quot;: &quot;DynamoDBDataNode&quot;,
      &quot;readThroughputPercent&quot;: &quot;#{mySourceReadThroughputRatio}&quot;
    },
    {
      &quot;id&quot;: &quot;DefaultSchedule&quot;,
      &quot;startDateTime&quot;: &quot;2015-04-07T21:55:00&quot;,
      &quot;occurrences&quot;: &quot;1&quot;,
      &quot;name&quot;: &quot;RunOnce&quot;,
      &quot;type&quot;: &quot;Schedule&quot;,
      &quot;period&quot;: &quot;1 days&quot;
    },
    {
      &quot;resizeClusterBeforeRunning&quot;: &quot;true&quot;,
      &quot;id&quot;: &quot;TableCopyActivity&quot;,
      &quot;input&quot;: {
        &quot;ref&quot;: &quot;DDBSourceTable&quot;
      },
      &quot;name&quot;: &quot;TableCopyActivity&quot;,
      &quot;runsOn&quot;: {
        &quot;ref&quot;: &quot;EmrClusterForCopy&quot;
      },
      &quot;type&quot;: &quot;HiveCopyActivity&quot;,
      &quot;output&quot;: {
        &quot;ref&quot;: &quot;DDBDestinationTable&quot;
      }
    },
    {
      &quot;id&quot;: &quot;DDBExportFormat&quot;,
      &quot;name&quot;: &quot;DDBExportFormat&quot;,
      &quot;type&quot;: &quot;DynamoDBExportDataFormat&quot;
    },
    {
      &quot;region&quot;: &quot;#{myDestTableRegion}&quot;,
      &quot;id&quot;: &quot;DDBDestinationTable&quot;,
      &quot;writeThroughputPercent&quot;: &quot;#{myDestWriteThroughputRatio}&quot;,
      &quot;tableName&quot;: &quot;#{myDestTableName}&quot;,
      &quot;name&quot;: &quot;DDBDestinationTable&quot;,
      &quot;dataFormat&quot;: {
        &quot;ref&quot;: &quot;DDBExportFormat&quot;
      },
      &quot;type&quot;: &quot;DynamoDBDataNode&quot;
    }
  ],
  &quot;parameters&quot;: [
    {
      &quot;id&quot;: &quot;myDestWriteThroughputRatio&quot;,
      &quot;watermark&quot;: &quot;Enter value between 0.1-1.0&quot;,
      &quot;default&quot;: &quot;0.2&quot;,
      &quot;description&quot;: &quot;Destination table write throughput ratio&quot;,
      &quot;type&quot;: &quot;Double&quot;
    },
    {
      &quot;id&quot;: &quot;mySourceTableName&quot;,
      &quot;description&quot;: &quot;Source table name&quot;,
      &quot;type&quot;: &quot;String&quot;
    },
    {
      &quot;id&quot;: &quot;mySourceTableRegion&quot;,
      &quot;watermark&quot;: &quot;us-east-1&quot;,
      &quot;description&quot;: &quot;Source table region&quot;,
      &quot;optional&quot;: &quot;true&quot;,
      &quot;type&quot;: &quot;String&quot;
    },
    {
      &quot;id&quot;: &quot;myDestTableRegion&quot;,
      &quot;watermark&quot;: &quot;us-east-1&quot;,
      &quot;description&quot;: &quot;Destination table region&quot;,
      &quot;optional&quot;: &quot;true&quot;,
      &quot;type&quot;: &quot;String&quot;
    },
    {
      &quot;id&quot;: &quot;myDestTableName&quot;,
      &quot;description&quot;: &quot;Destination table name&quot;,
      &quot;type&quot;: &quot;String&quot;
    },
    {
      &quot;id&quot;: &quot;mySourceReadThroughputRatio&quot;,
      &quot;watermark&quot;: &quot;Enter value between 0.1-1.0&quot;,
      &quot;default&quot;: &quot;0.2&quot;,
      &quot;description&quot;: &quot;Source table read throughput ratio&quot;,
      &quot;type&quot;: &quot;Double&quot;
    },
   {
      &quot;id&quot;: &quot;myPipelineLogUri &quot;,
      &quot;description&quot;: &quot;Pipeline log uri&quot;,
      &quot;type&quot;: &quot; AWS::S3::ObjectKey&quot;
    }
  ],
  &quot;values&quot;: {
    &quot;myDestWriteThroughputRatio&quot;: &quot;0.2&quot;,
    &quot;myDestTableName&quot;: &quot;DestinationTable&quot;,
    &quot;myDestTableRegion&quot;: &quot;us-west-2&quot;,
    &quot;mySourceTableName&quot;: &quot;SourceTableName&quot;,
    &quot;mySourceTableRegion&quot;: &quot;us-east-1&quot;,
    &quot;mySourceReadThroughputRatio&quot;: &quot;0.2&quot;,
   “myPipelineLogUri”:”s3://&lt;&gt;/”
  }
}
</pre>
<h1>
	<strong>Importing a Template from the Console</strong></h1>
<p>
	AWS Data Pipeline template support helps separate the user authoring the template from those who ultimately use the template. You can create templates for common use cases and share them within your organization or community, using Amazon S3 or your own local file system.</p>
<p>
	<img alt="Importing a template from the console" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_3-Data_Pipeline_Templates-b.PNG" style="width: 380px; height: 77px;" /></p>
<p>
	After the template is defined, it can be used to create pipelines. The Create Pipeline page on the <a href="https://console.aws.amazon.com/datapipeline/home?" target="_blank">AWS Management Console</a> provides an option to import definitions from S3 or your local file system.</p>
<p>
	After you choose and import the template, parameters in the template are rendered as a form. This allows you to launch pipelines in a few clicks.</p>
<p>
	<img alt="Launching a pipeline " height="177" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_4-Data_Pipeline_Templates.PNG" width="579" /></p>
<p>
	You can further customize the pipeline workflow by navigating to the Architect page. If you would like to change parameter values after the pipeline has been activated, change them on the Architect page and activate the pipeline again. You must activate the pipeline again after the changes have been saved because the Data Pipeline console updates the saved pipeline definitions of active pipelines.</p>
<p>
	<strong>Note: </strong>Currently, the console enables importing of S3 templates in the user’s account and from disk. To import an S3 template shared across accounts, first download the file locally and import it using the <strong>Load local file</strong> option on the console.</p>
<h1>
	<strong>Importing Templates from the AWS CLI</strong></h1>
<p>
	Templates can also be used with the <a href="http://docs.aws.amazon.com/cli/latest/reference/datapipeline/index.html#cli-aws-datapipeline" target="_blank">AWS CLI</a> and <a href="http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html" target="_blank">SDKs</a>. A JSON file imported using the CLI can have all three sections in a single JSON file, or given as separate arguments to a CLI command. You can also overwrite parameter values when activating a pipeline without having to edit the pipeline definition again.</p>
<p>
	<strong>PutPipelineDefinition with parameters using the AWS CLI</strong></p>
<p>
	Parameter values can be provided as arguments with the put-pipeline-definition command. For a list type parameter values, use the same key name and specify each value as a key-value pair (for example, arrayValue=value1 arrayValue=value2). In JSON list types, parameter values are expressed as an array of values (for example, “arrayValue” : [“value1”, “value2”]).</p>
<pre>
$aws datapipeline put-pipeline-definition --pipeline-definition file://./DynamoDBCrossRegionCopyTemplate.json --parameter-values myDestWriteThroughputRatio=0.2&nbsp; myDestTableName=DestinationTable&nbsp; myDestTableRegion=us-west-2&nbsp; mySourceTableName=SourceTableName&nbsp; mySourceTableRegion=us-east-1&nbsp; mySourceReadThroughputRatio=0.2 --pipeline-id df-002163815JNZCB8XOUQS</pre>
<p>
	OR</p>
<pre>
$aws datapipeline put-pipeline-definition --pipeline-definition file://./DynamoDBCrossRegionCopyTemplate.json --parameter-values-uri&nbsp; <a>file://./DynamoDBCrossRegionCopyTemplateParameters.json</a></pre>
<p>
	Below is the DynamoDBCrossRegionCopyTemplateParameters.json file content:</p>
<pre>
{
  &quot;values&quot;: {
    &quot;myDestWriteThroughputRatio&quot;: &quot;0.2&quot;,
    &quot;myDestTableName&quot;: &quot;DestinationTable&quot;,
    &quot;myDestTableRegion&quot;: &quot;us-west-2&quot;,
    &quot;mySourceTableName&quot;: &quot;SourceTableName&quot;,
    &quot;mySourceTableRegion&quot;: &quot;us-east-1&quot;,
    &quot;mySourceReadThroughputRatio&quot;: &quot;0.2&quot;
  }
}
</pre>
<p>
	<strong>ActivatePipeline with parameters using the AWS CLI</strong></p>
<p>
	You can overwrite parameter values with the activate-pipeline command. Parameter values provided during activation are applied to executions triggered after the activation of the pipeline.</p>
<pre>
$aws datapipeline activate-pipeline --pipeline-definition file://./DynamoDBCrossRegionCopyTemplate.json --parameter-values myDestWriteThroughputRatio=0.2&nbsp; myDestTableName=DestinationTable&nbsp; myDestTableRegion=us-west-2&nbsp; mySourceTableName=SourceTableName&nbsp; mySourceTableRegion=us-east-1&nbsp; mySourceReadThroughputRatio=0.2 --pipeline-id df-002163815JNZCB8XOUQS</pre>
<p>
	OR</p>
<pre>
$aws datapipeline activate-pipeline --pipeline-definition file://./DynamoDBCrossRegionCopyTemplate.json --parameter-values-uri&nbsp; <a>file://./DynamoDBCrossRegionCopyTemplatePrameters.json</a></pre>
<h1>
	<strong>Conclusion</strong></h1>
<p>
	AWS Data Pipeline parameterized templates make it easy to implement complex workflows, and to author complex pipelines and share them within your organization or community.</p>
<p>
	<strong>Resources</strong>:</p>
<p>
	<a href="http://docs.aws.amazon.com/cli/latest/reference/datapipeline/index.html#cli-aws-datapipeline" target="_blank">AWS CLI reference (AWS Data Pipeline)</a></p>
<p>
	<a href="http://docs.aws.amazon.com/datapipeline/latest/APIReference/Welcome.html" target="_blank">AWS Data Pipeline API reference</a></p>
<p>
	<a href="http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html" target="_blank">&nbsp;AWS SDK for Java API Reference</a></p>
<p>
	If you have questions or suggestions, please comment below.</p>
<p>
	&nbsp;</p>
<p>
	<em><span style="font-size:16px;"><strong>Do more with Data Pipeline:</strong></span></em></p>
<p>
	<strong><a href="http://blogs.aws.amazon.com/bigdata/post/Tx1PU7JM7I34L81/-span-class-matches-ETL-span-Processing-Using-AWS-Data-Pipeline-and-Amazon-Elast" target="_blank">ETL Processing Using AWS Data Pipeline and Amazon EMR</a></strong></p>
<p>
	<img alt="Data Pipeline workflow" height="77" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Workflow%20image%20resize.PNG" width="266" /></p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p></div>
  </div>

  <div class="post-footer-block">
  <div class="fl-r">
    <ul class="share-buttons navigation inline-navigation">
      <li class="quiet">
        Share</li>
      <li>
        <span class="social-media email share-post">
<a href="/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx2CD670NRJ8XXK" class="share-url icon email-icon" onclick="window.open('/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx2CD670NRJ8XXK', '_blank', 'location=yes,width=700,height=600');return false;" title="Share via Email"></a>
</span>
</li>
      <li>            
        <span class="social-media facebook share-post">
<a href="https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx2CD670NRJ8XXK%2FUsing-AWS-Data-Pipeline-s-Parameterized-Templates-to-Build-Your-Own-Library-of-E&t=Using%20AWS%20Data%20Pipeline%27s%20Parameterized%20Templates%20to%20Build%20Your%20Own%20Library%20of%20ETL%20Use-case%20Definitions" class="share-url icon facebook-icon" onclick="window.open('https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx2CD670NRJ8XXK%2FUsing-AWS-Data-Pipeline-s-Parameterized-Templates-to-Build-Your-Own-Library-of-E&t=Using%20AWS%20Data%20Pipeline%27s%20Parameterized%20Templates%20to%20Build%20Your%20Own%20Library%20of%20ETL%20Use-case%20Definitions', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Facebook"></a>
</span>
</li>
      <li>
        <span class="social-media twitter share-post">
<a href="https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx2CD670NRJ8XXK%2FUsing-AWS-Data-Pipeline-s-Parameterized-Templates-to-Build-Your-Own-Library-of-E&via=awscloud&text=Using+AWS+Data+Pipeline%27s+Parameterized+Templates+to+Build+Your+Own+Library+of+ETL+Use-case+Definitions&count=none" class="share-url icon twitter-icon" onclick="window.open('https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx2CD670NRJ8XXK%2FUsing-AWS-Data-Pipeline-s-Parameterized-Templates-to-Build-Your-Own-Library-of-E&via=awscloud&text=Using+AWS+Data+Pipeline%27s+Parameterized+Templates+to+Build+Your+Own+Library+of+ETL+Use-case+Definitions&count=none', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Twitter"></a>
</span></li>
      </ul>
  </div>
<div>
  <ul class="post-attributes navigation inline-navigation blog-ui-piped push-bottom-1">
    <li>
      <a href="/bigdata/post/Tx2CD670NRJ8XXK/Using-AWS-Data-Pipeline-s-Parameterized-Templates-to-Build-Your-Own-Library-of-E#" rel="bookmark">
        <time datetime="2015-05-06T20:09:29.035Z" title="2015-05-06T20:09:29.035Z" class="post-creation-date">May 6, 2015</time></a>
    </li>
    <li>
          <a href="/bigdata/post/Tx2CD670NRJ8XXK/Using-AWS-Data-Pipeline-s-Parameterized-Templates-to-Build-Your-Own-Library-of-E#" rel="bookmark">Permalink</a>
        </li>
    <li>
          <a href="/bigdata/post/Tx2CD670NRJ8XXK/Using-AWS-Data-Pipeline-s-Parameterized-Templates-to-Build-Your-Own-Library-of-E#postCommentsTx2CD670NRJ8XXK">Comments (<span id="commentNumber">0</span>)</a>
        </li>
    </ul>
</div></div></article>
</li>
          <li class="blog-post">
              <article
  class="post display published has-comments " 
  data-postid="Tx2OZ63RJ6Z41A0">
  
  <div class="post-heading-block clearfix">

  <div class="post-author">
      
<div class="author avatar author-avatar blog-ui-avatar blog-ui-author-avatar" data-content-id="gernest">
  <a href="/bigdata/blog/author/Guy+Ernest" title="Guy Ernest"><img class="author-image" alt="Guy Ernest" src="//cdn.amazonblogs.com/bigdata_awsblog/authors/Guy_pic.fw_1430254017862.png"></a>
</div></div>
  <div class="post-heading">

    <h1 class="post-title navigation">
      <a href="/bigdata/post/Tx2OZ63RJ6Z41A0/Building-a-Numeric-Regression-Model-with-Amazon-Machine-Learning" rel="bookmark">
        Building a Numeric Regression Model with Amazon Machine Learning</a>
    </h1>

    <ul class="post-attributes navigation inline-navigation blog-ui-piped">
      <li>
        <a href="/bigdata/post/Tx2OZ63RJ6Z41A0/Building-a-Numeric-Regression-Model-with-Amazon-Machine-Learning#" rel="bookmark">
          <time datetime="2015-04-28T21:18:53.810Z" title="2015-04-28T21:18:53.810Z" class="post-creation-date">April 28, 2015</time></a>
      </li>
      <li class="last">      
        <address class="by-line">
          <a rel="nofollow" data-contentid="gernest" class="author-name" itemprop="author" href="/bigdata/blog/author/Guy+Ernest">Guy Ernest</a>
</address>
      </li>    
    </ul>
    
    <ul class="category-list post-categories navigation inline-navigation blog-ui-piped">
  </ul><ul class="tag-list post-tags navigation inline-navigation blog-ui-piped">
  </ul></div>      
</div><div class="post-content-block">
    <div class="post-text"><p>
	<em>Guy Ernest is a Solutions Architect with AWS</em></p>
<p>
	We need to predict future values in our businesses. These predictions are important for better planning of resource allocation and making other business decisions. Often, we settle for a simplified heuristic of average values from the past and some change assumption because more accurate alternatives are too complex or expensive. The new <a href="https://aws.amazon.com/machine-learning" target="_blank">Amazon Machine Learning</a> (Amazon ML) service changes this equation by providing a simple and inexpensive way of building and using models such as numeric regression.</p>
<p>
	This post uses the example of a bike share program where you need to know how many bikes are required at each hour of each day in a specific city. In this scenario, you need a machine learning model that predicts a number based on a set of features or predictors. You will build a regression model based on a data set that is publicly available in <a href="http://www.kaggle.com" target="_blank">Kaggle</a>, a large community site of data scientists that compete against each other to solve data science problems. By building the model, you will explore a few concepts around the successful application of machine learning to solve similar problems in your domain.</p>
<h2>
	What is the difference between analytics and machine learning?</h2>
<p>
	The bike share example demonstrates the limits of analytics systems when it comes to making accurate predictions. One of the Kaggle participants created the following web page to analyze the provided data. If you choose the <strong>Plots</strong> tab, you can see a visualization of data that was created using R, popular free analytics software, and Shiny, a popular web interface for R: <a href="https://mlespiau.shinyapps.io/devdataprod-016/" target="_blank">View Bike Sharing Demand</a>.</p>
<p>
	<img alt="" height="375" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Image_1.png" width="581" /></p>
<p>
	This visualization shows that the demand for bikes is different on weekdays versus weekends, and peaks at 8 AM and 5 PM. You can also dive deeper and compare registered versus occasional (casual) users. The data visualizations below show that casual users are more likely to rent their bikes during the weekend and on Monday, and that registered users are more likely to rent their bikes during weekdays.</p>
<p>
	<img alt="" height="277" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Image_2.png" width="567" /><br />
	&nbsp;</p>
<p>
	<img alt="" height="271" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Image_3.png" width="567" /></p>
<p>
	It may appear that you can predict the usage of the bike share service based on the data visualizations above; however, there are other factors that influence usage such as season, weather, holidays, and more. It becomes more complex to visualize these additional factors, which is why you may turn to machine learning.</p>
<h2>
	Preparing the data to build the machine learning model</h2>
<p>
	The most important part of building a successful machine learning model is to find the most relevant data to feed it. The rule to remember is &quot;Garbage-In-Garbage-Out&quot; (or &quot;Gold-In-Gold-Out&quot;, depending on your perspective). Domain knowledge is helpful in identifying what might be a relevant factor (relevant = impactful, easily available for all records, and available for predicting future results).</p>
<p>
	In the above example, you saw the importance of the weekday versus weekend distinction, so you can use these as Boolean variables in the data set. You also know that the weather has a major effect on bike usage, as people rent fewer bikes when it is raining or too cold. It is easy to get historical weather information and accurate weather prediction for the coming days. The organizers of the <a href="http://www.kaggle.com/c/bike-sharing-demand" target="_blank">competition</a> on the Kaggle site prepared the data as follows:</p>
<pre>
datetime - hourly date + timestamp&nbsp;
season -&nbsp; 1 = spring, 2 = summer, 3 = fall, 4 = winter
holiday - whether the day is considered a holiday
workingday - whether the day is neither a weekend nor holiday
weather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy
2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light
Rain + Scattered clouds
4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
temp - temperature in Celsius
atemp - &quot;feels like&quot; temperature in Celsius
humidity - relative humidity
windspeed - wind speed
casual - number of non-registered user rentals initiated
registered - number of registered user rentals initiated
count - number of total rentals
</pre>
<p>
	You have three numbers to predict at the end of the list above: the number of rentals by casual users, number of rentals by registered users, and the total number of rentals. Because the total count is the sum of the first two numbers, and you saw above that casual users behave differently than registered users, you will build two different models to predict each user type separately.</p>
<p>
	You can use various tools to work with the columns of the CSV file, for example Microsoft Excel or RStudio IDE (http://www.rstudio.com), which is popular among data scientists. In this post, you will use simple shell commands such as <em>cut</em>, <em>sed</em>, and <em>awk</em> to manipulate the data.</p>
<p>
	The first manipulation to perform is to shuffle the lines of the training data to remove any order in the data that might bias the machine learning model.</p>
<pre>
# shuffle the lines except for the first header line
tail -n+2 train.csv | gshuf -o BikeShareTrainData.csv
# Add the header line from the original file as the first line of the
shuffled file
head -1 train.csv | cat - BikeShareTrainData.csv &gt; temp &amp;&amp; mv temp BikeShareTrainData.csv</pre>
<p>
	To create a training data file for the prediction of casual user rentals, you need to trim the last two fields (<em>registered</em> and <em>count</em>) from the original training data file. Use <em>cut</em> with the ',' delimiter to store the first 10 fields (up to the <em>casual</em> counter) in a new casual training data file:</p>
<pre>
cat BikeShareTrainData.csv | cut -d',' -f1-10 &gt; BikeShareCasualTrainData.csv</pre>
<p>
	Repeat this for the registered users, by skipping the 10th field (<em>casual</em>) and keeping the 11th (<em>registered</em>):</p>
<pre>
cat BikeShareTrainData.csv | cut -d',' -f1-9,11 &gt; BikeShareRegisteredTrainData.csv
</pre>
<p>
	To train the model, you need to copy the files to Amazon S3. Create a bucket in the same AWS region where the machine learning models will run and copy the files into the bucket using the <a href="http://aws.amazon.com/cli/">AWS CLI</a>.</p>
<pre>
aws s3 cp BikeShareCasualTrainData.csv s3://&lt;BUCKET_NAME&gt;/ML/input/ --region us-east-1
aws s3 cp BikeShareRegisteredTrainData.csv s3://&lt;BUCKET_NAME&gt;/ML/input/ --region us-east-1
</pre>
<p>
	Make sure to remove data that you are not going to have later in prediction time (the test data in this case). For example, if you didn't remove the fields for <em>casual</em> and <em>registered</em> from the training data and you then try to learn to predict the <em>count</em> variable, the model has a very easy task; it will simply sum the two variables and ignore the weather and other features.</p>
<p>
	<img alt="" height="385" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Image_4.png" width="568" /></p>
<p>
	<br />
	&nbsp;In the Amazon ML console, create a datasource by pointing to the training data file that you just uploaded to Amazon S3.</p>
<p>
	Next, define and optimize a schema for the data.</p>
<p>
	<img alt="" height="473" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Image_5.png" width="588" /></p>
<p>
	&nbsp;Fix the <em>season</em> variable, which is represented by a number (1 - Spring, 2 - Summer,...), to be a category instead of a numeric data type. Numeric variables have values that describe a measurable quantity as a number, such as 'how many' or 'how much'. If you know that a specific number is not representing a quantity, it is better to label it as a category instead of a numeric value.</p>
<p>
	Next, choose the target that the machine learning model should predict.</p>
<p>
	<img alt="" height="472" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Image_6a.png" width="616" /></p>
<p>
	Choose the <em>casual</em> variable as the target for this model. The service identifies it as a number and notifies you that it will use numerical regression. Continue with the defaults for the next screens and start the model building process. The process should take a few minutes, depending on the size (length and width) of the data. In later cycles, you can see more advanced ways to build the models, but it is best to start with the simple and default options first.</p>
<h2>
	Evaluating the machine learning model</h2>
<p>
	After the model has been created, you can evaluate how good it is; use the simple default machine learning model creation to create the evaluation automatically. It is important to test the evaluation of the model with data that the model was not trained on, data that it didn't &quot;see&quot;. Amazon ML does this by <a href="http://docs.aws.amazon.com/machine-learning/latest/mlconcepts/mlconcepts.html#splitting-the-data-into-training-and-evaluation-data">splitting</a> the data randomly into two sets of records: 70% of the records are used to train the model and the rest is held out for the evaluation of the model. You can choose to use your own scheme for training and evaluation and cut the training data differently, but for now use the default.</p>
<p>
	<img alt="" height="329" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Image_7.png" width="588" /></p>
<p>
	The evaluation produces both a numeric value and a visualization. For numeric <a href="http://docs.aws.amazon.com/machine-learning/latest/mlconcepts/mlconcepts.html#regression" target="_blank">regression</a>, the numeric value is called the root-mean-square error (RMSE). The lower the RMSE number, the smaller the error of the prediction, and the better the model. In this example, the RMSE of the model is 39 compared to a na&iuml;ve model that guesses the average with a RMSE of 49.</p>
<p>
	You can now also evaluate how each of the variables provided (<em>temp</em>, <em>windspeed</em>, <em>working</em> <em>day</em>, etc.) correlated to the prediction target: in this case, casual or registered user rentals.</p>
<p>
	<img alt="" height="235" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Image_8.png" width="608" /><br />
	Variables with a higher value have better prediction power. In this example, <em>atemp</em> (“feels like” temperature) has a value of 0.32 with the casual user number, compared to 0.01 for <em>windspeed</em>. It is also interesting to see that the <em>datetime</em> variable is a strong predictor at 0.21.</p>
<p>
	<img alt="" height="179" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Image_9.png" width="622" /><br />
	Amazon ML is able to parse the text fields and extract tokens such as 01, 02, 03, etc. as predictors for the model.</p>
<p>
	Now, you can decide if you want to use the model as-is or improve it further by lowering the RMSE. You could try to extract the hour from the <em>datetime</em> variable (known as feature processing), but you saw earlier that the service is doing a decent job in parsing this text field for you. Instead, you can extract the day of the week or the month. Here is an example script to add the day of the week to the variables and copy it into the casual training set:</p>
<pre>
awk 'NR&gt;1{system(&quot;date -j -f \&quot;%Y-%m-%d\&quot; &quot; $1 &quot; +%A&quot;)}' BikeShareTrain.csv &gt;
BikeShareTrainDoW.csv
paste -d &quot;,&quot; BikeShareTrainDoW.csv BikeShareCasualTrain.csv &gt; BikeShareCasualDoW.csv
</pre>
<p>
	Each one of these feature transformations can potentially improve the prediction accuracy of the model. Being a domain expert in the problem field can be helpful in identifying variables to add (<em>is raining</em>, for example).</p>
<h2>
	Using the ML model for batch prediction</h2>
<p>
	After the model is satisfactory, you can start using it to make predictions. The model is ready to use immediately, even at great scale or in real time. In this example, you run a batch prediction of the model on the test data from the Kaggle competition site.</p>
<p>
	<img alt="" height="187" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/ML_Image_10.png" width="479" /></p>
<p>
	After the job ends, usually in a few minutes, download the files containing the batch prediction results. Before submitting the results, sum the values of both predictions (casual + registered) to get the total number of expected rentals for each hour in the test file.</p>
<p>
	You can now combine the results, for example, by printing out the sum of the numbers:</p>
<pre>
paste casual_batch.out registered_batch.out | awk '$1+$2&gt;0 {print int($1 + $2); next} {print &quot;0&quot;}' &gt; bike_share_sub_test.csv
</pre>
<p>
	That’s it! In just a few minutes, you have built a simple machine learning model that is a significant improvement over a heuristic model or an average. If you'd like to learn more about Amazon ML or machine learning, see the <a href="http://docs.aws.amazon.com/machine-learning/latest/dg/" target="_blank">Amazon ML Developer Guide</a>.</p>
<p>
	If you have any questions or suggestions, please leave a comment below.</p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p></div>
  </div>

  <div class="post-footer-block">
  <div class="fl-r">
    <ul class="share-buttons navigation inline-navigation">
      <li class="quiet">
        Share</li>
      <li>
        <span class="social-media email share-post">
<a href="/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx2OZ63RJ6Z41A0" class="share-url icon email-icon" onclick="window.open('/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx2OZ63RJ6Z41A0', '_blank', 'location=yes,width=700,height=600');return false;" title="Share via Email"></a>
</span>
</li>
      <li>            
        <span class="social-media facebook share-post">
<a href="https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx2OZ63RJ6Z41A0%2FBuilding-a-Numeric-Regression-Model-with-Amazon-Machine-Learning&t=Building%20a%20Numeric%20Regression%20Model%20with%20Amazon%20Machine%20Learning" class="share-url icon facebook-icon" onclick="window.open('https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx2OZ63RJ6Z41A0%2FBuilding-a-Numeric-Regression-Model-with-Amazon-Machine-Learning&t=Building%20a%20Numeric%20Regression%20Model%20with%20Amazon%20Machine%20Learning', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Facebook"></a>
</span>
</li>
      <li>
        <span class="social-media twitter share-post">
<a href="https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx2OZ63RJ6Z41A0%2FBuilding-a-Numeric-Regression-Model-with-Amazon-Machine-Learning&via=awscloud&text=Building+a+Numeric+Regression+Model+with+Amazon+Machine+Learning&count=none" class="share-url icon twitter-icon" onclick="window.open('https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx2OZ63RJ6Z41A0%2FBuilding-a-Numeric-Regression-Model-with-Amazon-Machine-Learning&via=awscloud&text=Building+a+Numeric+Regression+Model+with+Amazon+Machine+Learning&count=none', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Twitter"></a>
</span></li>
      </ul>
  </div>
<div>
  <ul class="post-attributes navigation inline-navigation blog-ui-piped push-bottom-1">
    <li>
      <a href="/bigdata/post/Tx2OZ63RJ6Z41A0/Building-a-Numeric-Regression-Model-with-Amazon-Machine-Learning#" rel="bookmark">
        <time datetime="2015-04-28T21:18:53.810Z" title="2015-04-28T21:18:53.810Z" class="post-creation-date">April 28, 2015</time></a>
    </li>
    <li>
          <a href="/bigdata/post/Tx2OZ63RJ6Z41A0/Building-a-Numeric-Regression-Model-with-Amazon-Machine-Learning#" rel="bookmark">Permalink</a>
        </li>
    <li>
          <a href="/bigdata/post/Tx2OZ63RJ6Z41A0/Building-a-Numeric-Regression-Model-with-Amazon-Machine-Learning#postCommentsTx2OZ63RJ6Z41A0">Comments (<span id="commentNumber">5</span>)</a>
        </li>
    </ul>
</div></div></article>
</li>
          <li class="blog-post">
              <article
  class="post display published no-comments " 
  data-postid="Tx1DSZMORS09PQE">
  
  <div class="post-heading-block clearfix">

  <div class="post-author">
      
<div class="author avatar author-avatar blog-ui-avatar blog-ui-author-avatar" data-content-id="neelamd">
  <a href="/bigdata/blog/author/Chris+Keyser" title="Chris Keyser"><img class="author-image" alt="Chris Keyser" src="//cdn.amazonblogs.com/bigdata_awsblog/authors/Author pic resized.fw_1429806863446.png"></a>
</div></div>
  <div class="post-heading">

    <h1 class="post-title navigation">
      <a href="/bigdata/post/Tx1DSZMORS09PQE/Running-a-High-Performance-SAS-Grid-Manager-Cluster-on-AWS-with-Intel-Cloud-Edit" rel="bookmark">
        Running a High Performance SAS Grid Manager Cluster on AWS with Intel Cloud Edition for Lustre</a>
    </h1>

    <ul class="post-attributes navigation inline-navigation blog-ui-piped">
      <li>
        <a href="/bigdata/post/Tx1DSZMORS09PQE/Running-a-High-Performance-SAS-Grid-Manager-Cluster-on-AWS-with-Intel-Cloud-Edit#" rel="bookmark">
          <time datetime="2015-04-23T16:44:53.072Z" title="2015-04-23T16:44:53.072Z" class="post-creation-date">April 23, 2015</time></a>
      </li>
      <li class="last">      
        <address class="by-line">
          <a rel="nofollow" data-contentid="neelamd" class="author-name" itemprop="author" href="/bigdata/blog/author/Chris+Keyser">Chris Keyser</a>
</address>
      </li>    
    </ul>
    
    <ul class="category-list post-categories navigation inline-navigation blog-ui-piped">
  </ul><ul class="tag-list post-tags navigation inline-navigation blog-ui-piped">
  </ul></div>      
</div><div class="post-content-block">
    <div class="post-text"><p>
	<em>Chris Keyser is a Solutions Architect for Amazon Web Services</em></p>
<p>
	<em>This post was co-authored by Margaret Crevar, Sr. Manager, Performance Validation at SAS. SAS is an <a href="http://www.aws-partner-directory.com/PartnerDirectory/PartnerSearch?type=ISV" target="_blank">AWS Technology Partner</a>.</em></p>
<p>
	SAS (<a href="http://www.sas.com" target="_blank">www.sas.com</a>) is an integrated environment designed for business and advanced data analytics by enterprise and government organizations. SAS and AWS recently performed testing using the <a href="https://aws.amazon.com/marketplace/pp/B00GK6D19A/ref=srh_res_product_title?ie=UTF8&amp;sr=0-4&amp;qid=1428679244190" target="_blank">Intel Cloud Edition for Lustre* Software - Global Support (HVM)</a>, available on AWS Marketplace, to determine how well a standard workload performs on AWS using SAS Grid Manager. You can find the detailed results in the <a href="http://support.sas.com/rnd/scalability/grid/SGMonAWS.pdf" target="_blank">SAS&reg; Grid&reg; Manager 9.4 Testing on AWS using Intel&reg; Lustre</a> whitepaper. In this post, we’ll take a look at an approach to scaling the underlying AWS infrastructure to run SAS Grid Manager that can also be applied to similar applications with demanding I/O requirements.</p>
<h2>
	System Design Overview</h2>
<p>
	Running high-performance workloads that use throughput heavily, with sensitivity to network latency, requires approaches outside of typical applications. AWS generally recommends that applications span multiple Availability Zones for high availability. In the case of latency sensitivity, high throughput applications traffic should be local for optimal performance. To maximize throughput:</p>
<ul>
	<li>
		Run in a virtual private cloud (VPC), using instance types that support enhanced networking</li>
	<li>
		Run instances in the same Availability Zone (they can be in multiple subnets)</li>
	<li>
		Run instances within a placement group (see <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html" target="_blank">Placement Groups</a>)</li>
</ul>
<p>
	<img alt="" height="493" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Image_Luster.png" width="599" /></p>
<p>
	The SAS GRID nodes in the cluster are i2.8xlarge instances. The 8xlarge instance size proportionally provides the best network performance to shared storage of any instance size, assuming minimal <a href="https://aws.amazon.com/ebs" target="_blank">Amazon EBS</a> traffic. In the case of the 8xlarge, both EBS and other network traffic goes onto the 10 gigabit network, whereas the capacity is split for smaller sizes. This lets you achieve disproportionately higher throughput on an 8xlarge when traffic is skewed since the full 10 gigabits of capacity can be used to access the shared Lustre file system. The i2 instance also provides high performance local storage, which is covered in more detail in the following section.</p>
<p>
	The choice of an 8xlarge size for the Lustre cluster has less of an impact than choosing 8xlarge for the SAS nodes because there is significant traffic to both EBS and the file system clients, although an 8xlarge is still more optimal. The Lustre file system has a caching strategy and you will see higher throughput to clients in the case of frequent cache hits, which effectively reduces the network traffic to EBS.&nbsp;</p>
<p>
	Intel provides several <a href="https://aws.amazon.com/cloudformation" target="_blank">AWS CloudFormation</a> templates for launching a Lustre cluster. It’s pretty amazing that you can have a file system as complex as Lustre running and available in 10 or 15 minutes using the template. The template creates a Lustre metadata server instance, Lustre management server instance, NAT instance, and one or more object storage service (OSS) instances. The data is served off the OSS nodes, making them the component that has the most influence over throughput.</p>
<p>
	The Intel Cloud Edition for Lustre solution adds specific features for AWS, such as automatically replacing failed instances. The Intel <a href="https://wiki.hpdd.intel.com/display/PUB/Why+Use+Lustre" target="_blank">wiki</a> has a lot of detail on the design of Lustre. We made a few adjustments for testing to the template available at <a href="https://wiki.hpdd.intel.com/display/PUB/Intel+Cloud+Edition+for+Lustre*+-+Global+Support+HVM" target="_blank">ICEL - Global Support HVM</a>: adding a placement group, and changing the type of instance for the OSS (storage server) to c3.8xlarge. The <a href="http://support.sas.com/rnd/scalability/grid/SGMonAWS.pdf" target="_blank">SAS&reg; Grid&reg; Manager 9.4 Testing on AWS using Intel&reg; Lustre</a> whitepaper outlines how to make the specific changes required.</p>
<h2>
	Steps to Maximize Storage I/O Performance</h2>
<p>
	SAS applications need high-speed, temporary storage. Typically, temporary storage has the most demanding load. The high I/O instance family I2, and the recently released dense storage instance D2, provide high aggregate throughput to ephemeral (local) storage. The i2.8xlarge has 6.4 TB of local SSD storage, while the D2 has 48 TB of HDD.</p>
<p>
	The SAS workload tested used the I2, although you should be able to achieve similar performance with the D2. Ephemeral storage is perfect for temporary space, or for use in systems that replicate data (like Hadoop’s file system, HDFS). SAS applications use two temporary storage spaces, SASWORK and UTILLOC. We created RAID 0 volumes for these two locations. One of the considerations when dealing with ephemeral storage is that you will lose it whenever you stop, then start an instance. We wanted to be able to shut down for cost savings, so we created a startup script that creates and mounts ephemeral RAIDs automatically on restart. For more information about the example scripts initdisk.sh and initeph.sh, see the <a href="http://support.sas.com/rnd/scalability/grid/SGMonAWS.pdf" target="_blank">SAS&reg; Grid&reg; Manager 9.4 Testing on AWS using Intel&reg; Lustre</a> whitepaper.</p>
<p>
	For permanent storage on the Lustre cluster, you need to decide what type of EBS volume to use. For this type of workload, the choice is between the General Purpose (SSD) and Provisioned IOPS (SSD) volume types. SAS workloads tend to be large block sequential operations; General Purpose SSD volumes work well at a lower cost than Provisioned IOPS volumes for this type of workload. The number of IOPs for General Purpose (SSD) is determined by the volume size (3 IOPs per GB) and it has a bursting capability over the allocated IOP limit. You can overprovision storage to achieve better performance and, for sequential large block access, it will likely be cheaper to provision more storage than to use smaller volumes and pay for IOPS using Provisioned IOPS (SSD).</p>
<p>
	A second consideration with EBS is the throughput of each volume. There is a limit of 160 megabytes per second for one volume. You need to create an adequate mix of storage size and volumes to achieve the total throughput needed. There is great documentation on describing bursting and performance at <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSPerformance.html" target="_blank">Amazon EBS Volume Performance on Linux Instances</a> and <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#EBSVolumeTypes_gp2" target="_blank">Amazon EBS Volume Types</a>.&nbsp; We chose (8) 750 GB General Purpose (SSD) volumes per OSS for the testing.</p>
<h2>
	Throughput Testing and Results</h2>
<p>
	We want to achieve a throughput of least 100 MB/sec/core to temporary storage, and 50-75 MB/sec/core to shared storage. The i2.8xlarge has 16 cores (32 virtual CPUs, each virtual CPU is a hyperthread on a core, and a core has two hyperthreads). Doing the math, we needed to get per instance at least 1.6 gigabytes per second to temporary storage, and 800 megabytes per second to shared storage in order to use the compute power on the node fully, and not be I/O bound.</p>
<p>
	Testing results showed good processing efficiency and that the workload was not I/O bound. This means that the SAS workload was able to use the compute power fully for the 64 cores (4 i2.8xlarge instances x 16 cores per instance) under test.</p>
<p>
	Testing done with lower level testing tool (a SAS tool, <a href="http://support.sas.com/kb/51/660.html" target="_blank">iotest.sh</a>) &nbsp;showed a throughput of about 3 GB/sec to temporary storage, or almost 200 MB/sec/core, and about 1.5 GB/sec to shared storage, or about 25 MB/sec/core (because the shared file system is used by all 64 cores) in this configuration. The shared storage performance does not take into account file system caching, which Lustre does well. We started with the 1.5 GB/sec configuration for our functional setup, and then found that we did not need to increase the throughput of the Lustre cluster beyond that to achieve good overall performance.</p>
<p>
	Lustre could have been scaled up to achieve an uncached throughput performance of over 50 MB/sec/core by adding OSS nodes. In independent testing, Lustre scaled linearly using iotest.sh, doubling file system throughput to 3 GB/sec when increasing the number of OSS nodes to 6. For more information, see the Intel whitepaper <a href="http://www.intel.com/content/dam/www/public/us/en/documents/reference-architectures/ICEL-RA.pdf" target="_blank">Developing High-Performance, Scalable, cost effective storage solutions with Intel&reg;Cloud Edition Lustre* and Amazon Web Services</a>.</p>
<p>
	This testing demonstrates that, with the right design choices, you can run demanding compute and I/O applications on AWS. For full details of the testing configuration and results, see the <a href="http://support.sas.com/rnd/scalability/grid/SGMonAWS.pdf" target="_blank">SAS&reg; Grid&reg; Manager 9.4 Testing on AWS using Intel&reg; Lustre</a> whitepaper.</p>
<p>
	If you have questions or sugggestions, please leave a comment below.</p>
<p>
	-------------------------------------------------------</p>
<p>
	<span style="font-size:16px;"><strong>Do more on AWS</strong></span><strong>:</strong></p>
<p>
	<strong><a href="http://blogs.aws.amazon.com/bigdata/post/Tx14P056OLCWDOP/Launching-and-Running-an-Amazon-EMR-Cluster-inside-a-VPC" target="_blank">Launching and Running an Amazon EMR Cluster inside a VPC</a></strong></p>
<p>
	<img alt="" height="130" src="http://cdn.amazonblogs.com/bigdata_awsblog/images/Social%20media%20image_VPC.PNG" width="235" /></p>
<p>
	&nbsp;</p>
<p>
	&nbsp;</p></div>
  </div>

  <div class="post-footer-block">
  <div class="fl-r">
    <ul class="share-buttons navigation inline-navigation">
      <li class="quiet">
        Share</li>
      <li>
        <span class="social-media email share-post">
<a href="/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx1DSZMORS09PQE" class="share-url icon email-icon" onclick="window.open('/blog/amazon/signin?continueURL=/bigdata/blog/email/share%3FpostID=Tx1DSZMORS09PQE', '_blank', 'location=yes,width=700,height=600');return false;" title="Share via Email"></a>
</span>
</li>
      <li>            
        <span class="social-media facebook share-post">
<a href="https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx1DSZMORS09PQE%2FRunning-a-High-Performance-SAS-Grid-Manager-Cluster-on-AWS-with-Intel-Cloud-Edit&t=Running%20a%20High%20Performance%20SAS%20Grid%20Manager%20Cluster%20on%20AWS%20with%20Intel%20Cloud%20Edition%20for%20Lustre" class="share-url icon facebook-icon" onclick="window.open('https://www.facebook.com/share.php?u=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx1DSZMORS09PQE%2FRunning-a-High-Performance-SAS-Grid-Manager-Cluster-on-AWS-with-Intel-Cloud-Edit&t=Running%20a%20High%20Performance%20SAS%20Grid%20Manager%20Cluster%20on%20AWS%20with%20Intel%20Cloud%20Edition%20for%20Lustre', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Facebook"></a>
</span>
</li>
      <li>
        <span class="social-media twitter share-post">
<a href="https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx1DSZMORS09PQE%2FRunning-a-High-Performance-SAS-Grid-Manager-Cluster-on-AWS-with-Intel-Cloud-Edit&via=awscloud&text=Running+a+High+Performance+SAS+Grid+Manager+Cluster+on+AWS+with+Intel+Cloud+Edition+for+Lustre&count=none" class="share-url icon twitter-icon" onclick="window.open('https://twitter.com/share?url=http%3A%2F%2Fblogs.aws.amazon.com%2Fbigdata%2Fpost%2FTx1DSZMORS09PQE%2FRunning-a-High-Performance-SAS-Grid-Manager-Cluster-on-AWS-with-Intel-Cloud-Edit&via=awscloud&text=Running+a+High+Performance+SAS+Grid+Manager+Cluster+on+AWS+with+Intel+Cloud+Edition+for+Lustre&count=none', '_blank', 'location=yes,width=700,height=400');return false;" title="Share on Twitter"></a>
</span></li>
      </ul>
  </div>
<div>
  <ul class="post-attributes navigation inline-navigation blog-ui-piped push-bottom-1">
    <li>
      <a href="/bigdata/post/Tx1DSZMORS09PQE/Running-a-High-Performance-SAS-Grid-Manager-Cluster-on-AWS-with-Intel-Cloud-Edit#" rel="bookmark">
        <time datetime="2015-04-23T16:44:53.072Z" title="2015-04-23T16:44:53.072Z" class="post-creation-date">April 23, 2015</time></a>
    </li>
    <li>
          <a href="/bigdata/post/Tx1DSZMORS09PQE/Running-a-High-Performance-SAS-Grid-Manager-Cluster-on-AWS-with-Intel-Cloud-Edit#" rel="bookmark">Permalink</a>
        </li>
    <li>
          <a href="/bigdata/post/Tx1DSZMORS09PQE/Running-a-High-Performance-SAS-Grid-Manager-Cluster-on-AWS-with-Intel-Cloud-Edit#postCommentsTx1DSZMORS09PQE">Comments (<span id="commentNumber">0</span>)</a>
        </li>
    </ul>
</div></div></article>
</li>
          </ul>
      <nav class="paging" data-page-size="10" data-current-page="1" data-total-pages="6" data-total-items="54" data-root-dir="bigdata">
  <ul class="navigation inline-navigation">
    <li class="nav-link">
      <span><span class="arrow-back-icon icon">&nbsp;</span>Previous</span>
        </li>
    
    <li>
        <a data-page="1" class="current" href="/bigdata/blog?number=1&size=10">1</a>
      </li>
    <li>
        <a data-page="2" class="" href="/bigdata/blog?number=2&size=10">2</a>
      </li>
    <li>
        <a data-page="3" class="" href="/bigdata/blog?number=3&size=10">3</a>
      </li>
    <li>
        <a data-page="4" class="" href="/bigdata/blog?number=4&size=10">4</a>
      </li>
    <li>
        <a data-page="5" class="" href="/bigdata/blog?number=5&size=10">5</a>
      </li>
    <li>
        <a data-page="6" class="" href="/bigdata/blog?number=6&size=10">6</a>
      </li>
    <li class="nav-link">
      <a data-page="2" href="/bigdata/blog?number=2&size=10"><span>Next<span class="arrow-forward-icon icon">&nbsp;</span></span></a>
        </li>
  </ul>
  
  </nav></div>
            <div id="sidebar" class="push-1 span-8 last">
  <aside id="searchModule" class="module clearfix">
  <form id="searchForm" action="/bigdata/blog/search" method="get">
    <div class="blog-ui-search-box aws-sprite">
      <div class="aws-sprite">
        <div class="search-module-input-wrapper">
          <input type="text" name="q" class="query">
        </div>
      </div>
    </div>
    <span class="aws-sprite blog-ui-search-selector">
        <span id="selectedCategory">Entire Blog</span>
        <span class="aws-sprite nav-chevron"></span>
        <select name="category" class="category">
          <option  value="">Entire Blog</option>
          <option  value="Announcements">Announcements</option>
          <option  value="Best Practices">Best Practices</option>
          <option  value="Compliance">Compliance</option>
          <option  value="Compute">Compute</option>
          <option  value="Customer stories">Customer stories</option>
          <option  value="Database">Database</option>
          <option  value="Encryption">Encryption</option>
          <option  value="Enterprise">Enterprise</option>
          <option  value="Federation">Federation</option>
          <option  value="Government">Government</option>
          <option  value="How-to guides">How-to guides</option>
          <option  value="Networking">Networking</option>
          <option  value="Storage">Storage</option>
          </select>
      </span>
    <button class="aws-sprite" type="submit">Search</button>
  </form>
    <div class="search-query-error" id="searchQueryError" style="display:none;">
      <p>The search query cannot be empty or single character.</p>
    </div>
  <script type="text/javascript">
  (function($) {
    $(document).ready(function() {
      var mod = new blogs.searchModule();

      $('#searchForm').submit(function(e) {
        var keywords = $.trim($('input[name="q"]').val());
        if (keywords.length < 2) {
          $('#searchQueryError').show();
          return false;
        }
        return true;
      });
    
      $('input[name="q"]').keyup(function(e) {
        var keycode = (e.keyCode ? e.keyCode : e.which);
        if(keycode != '13') {
          $('#searchQueryError:visible').hide();
        }
      });
    });
  })(jQuery);
  </script>  
</aside><aside class="module navigation">
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://cdn.amazonblogs.com/awsblog/assets/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
    &nbsp;&nbsp;&nbsp;
  <a title="Recent Posts (RSS)" href="/bigdata/blog/feed/recentPosts.rss"><img src="http://cdn.amazonblogs.com/awsblog/assets/feed-icon.png"></a>
</aside>

<aside id="recentPostsModule" class="module">
  <h2>Latest Blog Entries</h2>

  <div class="module-content">
    <ul class="module-list navigation">
      <li>
          <div class="module-item module-link-item">
            <a href="/bigdata/post/Tx22THFQ9MI86F9/Applying-Machine-Learning-to-Text-Mining-with-Amazon-S3-and-RapidMiner">
              Applying Machine Learning to Text Mining with Amazon S3 and RapidMiner</a>
          </div>
        </li>
      <li>
          <div class="module-item module-link-item">
            <a href="/bigdata/post/Tx21LOP0UQ2ZA9N/Large-Scale-Machine-Learning-with-Spark-on-Amazon-EMR">
              Large-Scale Machine Learning with Spark on Amazon EMR</a>
          </div>
        </li>
      <li>
          <div class="module-item module-link-item">
            <a href="/bigdata/post/TxGVITXN9DT5V6/Building-a-Binary-Classification-Model-with-Amazon-Machine-Learning-and-Amazon-R">
              Building a Binary Classification Model with Amazon Machine Learning and Amazon Redshift</a>
          </div>
        </li>
      <li>
          <div class="module-item module-link-item">
            <a href="/bigdata/post/Tx11LQAXCMNYZOA/Test-drive-two-big-data-scenarios-from-the-Building-a-Big-Data-Platform-on-AWS-b">
              Test drive two big data scenarios from the 'Building a Big Data Platform on AWS' bootcamp</a>
          </div>
        </li>
      <li>
          <div class="module-item module-link-item">
            <a href="/bigdata/post/TxC0CXZ3RPPK7O/Indexing-Common-Crawl-Metadata-on-Amazon-EMR-Using-Cascading-and-Elasticsearch">
              Indexing Common Crawl Metadata on Amazon EMR Using Cascading and Elasticsearch</a>
          </div>
        </li>
      <li>
          <div class="module-item module-link-item">
            <a href="/bigdata/post/Tx2LQ4WAWOP80EG/Building-a-Multi-Class-ML-Model-with-Amazon-Machine-Learning">
              Building a Multi-Class ML Model with Amazon Machine Learning</a>
          </div>
        </li>
      <li>
          <div class="module-item module-link-item">
            <a href="/bigdata/post/Tx1WZP38ERPGK5K/Optimizing-for-Star-Schemas-and-Interleaved-Sorting-on-Amazon-Redshift">
              Optimizing for Star Schemas and Interleaved Sorting on Amazon Redshift</a>
          </div>
        </li>
      <li>
          <div class="module-item module-link-item">
            <a href="/bigdata/post/Tx2CD670NRJ8XXK/Using-AWS-Data-Pipeline-s-Parameterized-Templates-to-Build-Your-Own-Library-of-E">
              Using AWS Data Pipeline's Parameterized Templates to Build Your Own Library of ETL Use-case Definitions</a>
          </div>
        </li>
      <li>
          <div class="module-item module-link-item">
            <a href="/bigdata/post/Tx2OZ63RJ6Z41A0/Building-a-Numeric-Regression-Model-with-Amazon-Machine-Learning">
              Building a Numeric Regression Model with Amazon Machine Learning</a>
          </div>
        </li>
      <li>
          <div class="module-item module-link-item">
            <a href="/bigdata/post/Tx1DSZMORS09PQE/Running-a-High-Performance-SAS-Grid-Manager-Cluster-on-AWS-with-Intel-Cloud-Edit">
              Running a High Performance SAS Grid Manager Cluster on AWS with Intel Cloud Edition for Lustre</a>
          </div>
        </li>
      </ul>

  </div>

</aside><div class="module static-content" id="static-browse-blog-list-awsblog">
  <h2>AWS Blogs</h2>
  <div class="module-content">
    <ul class="module-list navigation">
      <li>
        <a href="http://aws.amazon.com/blogs/aws/">The AWS Blog</a></li>
      <li>
        <a href="http://blogs.aws.amazon.com/security/">Security</a></li>
      <li>
        <a href="http://blogs.aws.amazon.com/cli/">AWS CLI</a></li>
      <li>
        <a href="http://blogs.aws.amazon.com/bigdata/">Big Data</a></li>
      <li>
        <a href="http://sesblog.amazon.com/">Amazon SES</a></li>
      <li>
        <a href="http://java.awsblog.com">Java Development</a></li>
      <li>
        <a href="http://blogs.aws.amazon.com/javascript/">JavaScript Development</a></li>
      <li>
        <a href="http://mobile.awsblog.com">Mobile Development</a></li>
      <li>
        <a href="http://blogs.aws.amazon.com/net/">.NET Development</a></li>
      <li>
        <a href="http://blogs.aws.amazon.com/php/">PHP Development</a></li>
      <li>
        <a href="http://ruby.awsblog.com">Ruby Development</a></li>
      <li>
        <a href="http://blogs.aws.amazon.com/application-management">Application Management</a></li>
      <li>
        <a href="https://medium.com/aws-activate-startup-blog">AWS Activate</a></li>
      <li>
        <a href="http://www.awsarchitectureblog.com/">AWS Architecture</a></li>
      <li>
        <a href="http://aws.amazon.com/podcasts/aws-podcast/">Podcast</a></li>
    </ul>
  </div>
</div>
<div class="module static-content" id="static-browse-useful-link-security-awsblog">
  <h2>Useful Links</h2>
  <div class="module-content">
    <ul class="module-list navigation">
      <li><a href="http://aws-website-staging.aka.amazon.com/training/course-descriptions/bigdata-fundamentals/">Big Data Technology Fundamentals</a></li>
      <li><a href="http://aws.amazon.com/training/course-descriptions/bigdata/">Big Data on AWS: Instructor-led Training</a></li>
      <li><a href="http://aws.amazon.com/partners/competencies/big-data/">Big Data Competency Partners</a></li>
      <li><a href="http://aws.amazon.com/elasticmapreduce/">Amazon Elastic MapReduce</a></li>
      <li><a href="http://aws.amazon.com/redshift/?nc1=h_l2_al">Amazon Redshift</a></li>
      <li><a href="http://aws.amazon.com/kinesis/?nc1=h_l2_al">Amazon Kinesis</a></li>
      <li><a href="http://aws.amazon.com/datapipeline/?nc1=h_l2_al">AWS Data Pipeline</a></li>
      <li><a href="http://aws.amazon.com/dynamodb/?nc1=h_l2_db">Amazon Dynamo DB</a></li>
      <li><a href="http://aws.amazon.com/elasticache/?nc1=h_l2_db">Amazon ElastiCache</a></li>
    </ul>
  </div>
</div><aside id="archiveModule" class="module">
  <h2>Archive</h2>

  <div class="module-content">
    <select class="year">
      <option class="default" value="0">Year</option>
      <option value="2015">2015</option>
      <option value="2014">2014</option>
      <option value="2013">2013</option>
      <option value="2012">2012</option>
      <option value="2011">2011</option>
      <option value="2010">2010</option>
      <option value="2009">2009</option>
      <option value="2008">2008</option>
      <option value="2007">2007</option>
      <option value="2006">2006</option>
      <option value="2005">2005</option>
      </select> 
    
    <select class="month">
      <option class="default" value="0">Month</option>
      <option value="1">January</option>
      <option value="2">February</option>
      <option value="3">March</option>
      <option value="4">April</option>
      <option value="5">May</option>
      <option value="6">June</option>
      <option value="7">July</option>
      <option value="8">August</option>
      <option value="9">September</option>
      <option value="10">October</option>
      <option value="11">November</option>
      <option value="12">December</option>
      </select> 
    
    <select class="day">
      <option class="default" value="0">Day</option>
      <option value="1">1</option>
      <option value="2">2</option>
      <option value="3">3</option>
      <option value="4">4</option>
      <option value="5">5</option>
      <option value="6">6</option>
      <option value="7">7</option>
      <option value="8">8</option>
      <option value="9">9</option>
      <option value="10">10</option>
      <option value="11">11</option>
      <option value="12">12</option>
      <option value="13">13</option>
      <option value="14">14</option>
      <option value="15">15</option>
      <option value="16">16</option>
      <option value="17">17</option>
      <option value="18">18</option>
      <option value="19">19</option>
      <option value="20">20</option>
      <option value="21">21</option>
      <option value="22">22</option>
      <option value="23">23</option>
      <option value="24">24</option>
      <option value="25">25</option>
      <option value="26">26</option>
      <option value="27">27</option>
      <option value="28">28</option>
      <option value="29">29</option>
      <option value="30">30</option>
      <option value="31">31</option>
      </select>

    <button class="go-button blog-ui-button" type="button">
      <span>Go</span>
    </button>
  </div>
  <script type="text/javascript">
  (function($) {
    $(document).ready(function() {
      var mod = new blogs.archiveModule();
    });
  })(jQuery);
  </script>
</aside></div>
</section>
          <footer class="clearfix">
  <hr class ="aws-common-footer-separate-bar">
  <div class="aws-common-footer-message-style aws-common-footer-message-color">&copy;2015, Amazon Web Services, Inc. or its affiliates. All rights reserved.</div>
  <div  class="aws-common-footer-message-style"> 
    <ul class="inline-navigation navigation">
      <li>
        <a href="http://aws.amazon.com/terms/">Terms of Use</a>
      </li>
      <li>
        <a href="http://aws.amazon.com/privacy/">Privacy Policy</a>
      </li>
    </ul>
  </div>
  <script type="text/javascript">
    SyntaxHighlighter.all()
  </script>
</footer></div>
      </div>
  <!-- Send to Kindle -->
    <script type="text/javascript" src="https://d1xnn692s7u6t6.cloudfront.net/widget.js"></script>
    <script type="text/javascript">(function k(){window.$SendToKindle&&window.$SendToKindle.Widget?$SendToKindle.Widget.init({}):setTimeout(k,500);})();</script>
    <!-- SiteCatalyst code version: H.25.1.
Copyright 1996-2012 Adobe, Inc. All Rights Reserved
More info available at http://www.omniture.com -->
<script language="JavaScript" type="text/javascript" src="https://d36cz9buwru1tt.cloudfront.net/js/sitecatalyst/s_code.min.js"></script>

<script language="JavaScript" type="text/javascript"><!--
/************* DO NOT ALTER ANYTHING BELOW THIS LINE ! **************/
var s_code=s.t();if(s_code)document.write(s_code)//--></script>
<script language="JavaScript" type="text/javascript"><!--
if(navigator.appVersion.indexOf('MSIE')>=0)document.write(unescape('%3C')+'\!-'+'-')
//--></script><noscript><img src="//amazonwebservices.d2.sc.omtrdc.net/b/ss/awsamazondev/1/H.25.1--NS/0"
height="1" width="1" border="0" alt="" /></noscript><!--/DO NOT REMOVE/-->
<!-- End SiteCatalyst code version: H.25.1. -->  
  </body>
</html>
